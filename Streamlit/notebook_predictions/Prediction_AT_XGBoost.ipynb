{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b75736b-db15-4c25-b523-42ee90ad6db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b6b26a-f1f1-489b-a90d-859edc487185",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model: XGBoost"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Imports\n",
    "# ============================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd59eb13-2c69-4f47-a36e-b18bb5771673",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Austria (AT)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Load base table and filter to Austria (AT)\n",
    "# ============================================\n",
    "df = spark.table(\"workspace.default.train_set_timebins_lags\")\n",
    "\n",
    "print(\"Total rows (all countries):\", df.count())\n",
    "print(\"Total columns:\", len(df.columns))\n",
    "\n",
    "# df_de = df.filter(F.col(\"country\") == \"AT\")\n",
    "# print(\"Rows for AT:\", df_de.count())\n",
    "\n",
    "# display(df_de.limit(5))\n",
    "# workspace.default.electricity_and_weather_europe_with_target\n",
    "\n",
    "df_de = df.filter(F.col(\"country\") == \"AT\")\n",
    "print(\"Rows for AT:\", df_de.count())\n",
    "\n",
    "display(df_de.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2515ae7e-51ba-43dd-8252-fdb86ddd244a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Create future target columns (1‚Äì6 hours ahead)\n",
    "# ============================================\n",
    "\n",
    "# Window by country + time (even though we only have DE, it's safer)\n",
    "w = Window.partitionBy(\"country\").orderBy(\"timestamp\")\n",
    "\n",
    "horizons = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "for h in horizons:\n",
    "    df_de = df_de.withColumn(f\"stress_plus_{h}h\", F.lag(\"grid_stress_score\", -h).over(w))\n",
    "\n",
    "# Drop rows where any target is null (end of series)\n",
    "target_cols = [f\"stress_plus_{h}h\" for h in horizons]\n",
    "df_de = df_de.dropna(subset=target_cols)\n",
    "\n",
    "print(\"Rows for AT after adding targets and dropping tail:\", df_de.count())\n",
    "display(df_de.select(\"timestamp\", \"grid_stress_score\", *target_cols).orderBy(\"timestamp\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e682f80-072b-42e7-bbf6-250a9b5a33eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Convert to Pandas and basic cleaning\n",
    "# ============================================\n",
    "\n",
    "pdf = (\n",
    "    df_de\n",
    "    .orderBy(\"timestamp\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Ensure timestamp is datetime and set as index for reference (not in features)\n",
    "pdf[\"timestamp\"] = pd.to_datetime(pdf[\"timestamp\"])\n",
    "pdf = pdf.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(\"Pandas shape:\", pdf.shape)\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2432c7-775f-4761-b8da-003a29251eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. Drop high-null columns & define features/targets\n",
    "# ============================================\n",
    "\n",
    "# 5.1 Drop columns with > 50% missing\n",
    "null_frac = pdf.isna().mean()\n",
    "high_null_cols = null_frac[null_frac > 0.5].index.tolist()\n",
    "print(\"Columns with >50% nulls (will drop):\")\n",
    "print(high_null_cols)\n",
    "\n",
    "pdf_clean = pdf.drop(columns=high_null_cols)\n",
    "\n",
    "# ============================================\n",
    "# üëâ ADD THIS HERE ‚Äî encode daytime_bin\n",
    "# ============================================\n",
    "\n",
    "if \"daytime_bin\" in pdf_clean.columns:\n",
    "    pdf_clean[\"daytime_bin\"] = pdf_clean[\"daytime_bin\"].map({\n",
    "        \"night\": 0,\n",
    "        \"early_morning\": 1,\n",
    "        \"morning\": 2,\n",
    "        \"afternoon\": 3,\n",
    "        \"evening\": 4\n",
    "    })\n",
    "\n",
    "# ============================================\n",
    "# 5.2 Define targets\n",
    "# ============================================\n",
    "target_cols = [f\"stress_plus_{h}h\" for h in horizons]\n",
    "\n",
    "# 5.3 Columns to exclude from features\n",
    "exclude_cols = (\n",
    "    [\"index\", \"country\", \"timestamp\", \"grid_stress_score\"]\n",
    "    + target_cols\n",
    ")\n",
    "\n",
    "# 5.4 Build feature list\n",
    "feature_cols = [c for c in pdf_clean.columns if c not in exclude_cols]\n",
    "\n",
    "print(\"Number of feature columns:\", len(feature_cols))\n",
    "print(\"Example features:\", feature_cols[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7db700-3180-496e-aceb-6e9e31f1b882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. Simple imputation for remaining nulls\n",
    "# ============================================\n",
    "\n",
    "# For time series, a simple forward-fill then back-fill is reasonable for a first model\n",
    "pdf_model = pdf_clean.copy()\n",
    "\n",
    "pdf_model[feature_cols] = (\n",
    "    pdf_model[feature_cols]\n",
    "        .ffill()\n",
    "        .bfill()\n",
    ")\n",
    "\n",
    "# If still any null remains (edge cases), fill with column median\n",
    "for c in feature_cols:\n",
    "    if pdf_model[c].isna().any():\n",
    "        pdf_model[c] = pdf_model[c].fillna(pdf_model[c].median())\n",
    "\n",
    "# Sanity check\n",
    "print(\"Any nulls left in features?\", pdf_model[feature_cols].isna().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260cfd91-fbc3-4f61-b44f-3dae6a65e148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Time-based train/validation split\n",
    "# ============================================\n",
    "\n",
    "n = len(pdf_model)\n",
    "split_idx = int(n * 0.8)\n",
    "\n",
    "X = pdf_model[feature_cols].values\n",
    "y = pdf_model[target_cols]   # this is a DataFrame with 6 target columns\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(\"Train size:\", X_train.shape, y_train.shape)\n",
    "print(\"Val size  :\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccec7496-eb29-40c8-995a-94e39c9ab03c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_de.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9da8989-63a6-42a3-bd72-1847c4ffb822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0604da-9443-497e-9912-13a44bdbaf02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. Train one XGBoost model per horizon\n",
    "# ============================================\n",
    "\n",
    "horizons = [1, 2, 3, 4, 5, 6]  # ensure defined\n",
    "\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "for h in horizons:\n",
    "    target_name = f\"stress_plus_{h}h\"\n",
    "    print(f\"\\n=== Training XGBoost model for {target_name} ===\")\n",
    "    \n",
    "    y_tr = y_train[target_name].values\n",
    "    y_va = y_val[target_name].values\n",
    "\n",
    "    # XGBoost regressor (you can tune these later)\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"   # fast & works well on CPUs\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train, y_tr)\n",
    "\n",
    "    y_pred = xgb.predict(X_val)\n",
    "\n",
    "    mae = mean_absolute_error(y_va, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "\n",
    "    models[target_name] = xgb\n",
    "    metrics[target_name] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "    print(f\"{target_name} -> MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22afec9-d1e7-4fab-a4eb-74ceaeaf4ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. Show metrics nicely\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083bd926-339b-4ebf-825b-84d9a08de914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. Plot actual vs predicted for 1h horizon\n",
    "# ============================================\n",
    "h = 1\n",
    "target_name = f\"stress_plus_{h}h\"\n",
    "\n",
    "y_va = y_val[target_name].values\n",
    "y_pred = models[target_name].predict(X_val)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y_va, label=\"Actual future stress (+1h)\", alpha=0.8)\n",
    "plt.plot(y_pred, label=\"Predicted future stress (+1h)\", alpha=0.8)\n",
    "plt.title(\"AT - Grid Stress Prediction (+1h)\")\n",
    "plt.xlabel(\"Time steps (validation)\")\n",
    "plt.ylabel(\"Stress Score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8773f7c6-665d-4730-b6be-389438bfd9db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Zoom into a smaller window"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Better Plot: Zoom into first 300 validation points\n",
    "# ============================================\n",
    "h = 1\n",
    "target_name = f\"stress_plus_{h}h\"\n",
    "\n",
    "y_va = y_val[target_name].values\n",
    "y_pred = models[target_name].predict(X_val)\n",
    "\n",
    "N = 300   # zoom window\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y_va[:N], label=\"Actual (+1h)\", alpha=0.9)\n",
    "plt.plot(y_pred[:N], label=\"Predicted (+1h)\", alpha=0.9)\n",
    "plt.title(\"AT ‚Äî Grid Stress Prediction (+1h) ‚Äì Zoomed (first 300 samples)\")\n",
    "plt.xlabel(\"Time steps (validation)\")\n",
    "plt.ylabel(\"Stress Score\")\n",
    "plt.legend()\n",
    "plt.show()  \n",
    "\n",
    "# üìå Now the plot will be readable and show if the model tracks the signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9af8bc0-63ac-42c2-9ede-c302207865e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Scatter Plot: Actual vs Predicted\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_va, y_pred, alpha=0.3, s=10)\n",
    "plt.plot([0,80], [0,80], \"r--\", label=\"Perfect prediction\")\n",
    "plt.xlabel(\"Actual stress (+1h)\")\n",
    "plt.ylabel(\"Predicted stress (+1h)\")\n",
    "plt.title(\"Actual vs Predicted Stress (+1h)\")\n",
    "plt.legend()\n",
    "plt.show()  \n",
    "\n",
    "# üìå If points are close to the red line ‚Üí strong model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2083b659-d7d8-4b73-83d4-fcc2bfa8d1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Residual Plot\n",
    "# ============================================\n",
    "\n",
    "residuals = y_va - y_pred\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(residuals, alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Residuals ‚Äî Stress Prediction (+1h)\")\n",
    "plt.xlabel(\"Time steps (validation)\")\n",
    "plt.ylabel(\"Error (Actual - Predicted)\")\n",
    "plt.show()\n",
    "\n",
    "# üìå Good models ‚Üí residuals centered around zero, no obvious structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7523277-73ff-498f-bde2-696b98e8cb1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Histogram of Residuals\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residuals, bins=40, alpha=0.7)\n",
    "plt.title(\"Residual Distribution (+1h)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# üìå Ideal ‚Üí symmetric around zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a99642-621f-468c-a2d9-4b5bc686f555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-Panel Comparison: Actual vs Predicted for 1‚Äì6 hours\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "for i, h in enumerate(horizons):\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    \n",
    "    # actual & predicted\n",
    "    y_va = y_val[target].values\n",
    "    y_pred = models[target].predict(X_val)\n",
    "    \n",
    "    # zoom window for readability\n",
    "    N = 300\n",
    "    \n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.plot(y_va[:N], label=f\"Actual (+{h}h)\", alpha=0.9)\n",
    "    plt.plot(y_pred[:N], label=f\"Predicted (+{h}h)\", alpha=0.9)\n",
    "    plt.title(f\"Grid Stress Prediction (+{h}h)\")\n",
    "    plt.xlabel(\"Validation time steps (zoom 0‚Äì300)\")\n",
    "    plt.ylabel(\"Stress Score\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4194572c-a30d-438e-b15e-1256434acb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-Panel Scatter Dashboard: Actual vs Predicted\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "for i, h in enumerate(horizons):\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    \n",
    "    # Actual & predicted\n",
    "    y_va = y_val[target].values\n",
    "    y_pred = models[target].predict(X_val)\n",
    "\n",
    "    # Scatter plot (Actual vs Predicted)\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.scatter(y_va, y_pred, s=8, alpha=0.5)\n",
    "    plt.plot([0, 100], [0, 100], 'r--', linewidth=1)  # perfect line\n",
    "\n",
    "    plt.title(f\"Actual vs Predicted (+{h}h)\")\n",
    "    plt.xlabel(\"Actual Stress Score\")\n",
    "    plt.ylabel(\"Predicted Stress Score\")\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter of actual vs predicted values\n",
    "# Red dashed line = perfect predictions\n",
    "# Tight axis limits (0‚Äì100 stress score) for consistency\n",
    "# Helps detect:\n",
    "#   over-prediction\n",
    "#   under-prediction\n",
    "#   heteroskedasticity\n",
    "#   horizon degradation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab3b11b1-b956-4375-98bf-11d81c497a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1. Residual Distribution Dashboard (6 Horizons)\n",
    "# =====================================================\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "for i, h in enumerate(horizons):\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    y_va = y_val[target].values\n",
    "    y_pred = models[target].predict(X_val)\n",
    "\n",
    "    residuals = y_va - y_pred\n",
    "\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.hist(residuals, bins=40, color=\"steelblue\", alpha=0.75)\n",
    "    plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"Residual Distribution (+{h}h)\")\n",
    "    plt.xlabel(\"Residual (Actual ‚Äì Predicted)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791d9fac-8614-46bf-90ef-055a8de302f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Residual vs Predicted (6 Horizons)\n",
    "# =====================================================\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "for i, h in enumerate(horizons):\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    y_va = y_val[target].values\n",
    "    y_pred = models[target].predict(X_val)\n",
    "\n",
    "    residuals = y_va - y_pred\n",
    "\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.scatter(y_pred, residuals, s=8, alpha=0.5)\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    plt.title(f\"Residual vs Predicted (+{h}h)\")\n",
    "    plt.xlabel(\"Predicted Stress Score\")\n",
    "    plt.ylabel(\"Residual (Actual ‚Äì Predicted)\")\n",
    "    plt.ylim(-40, 40)  # adjust if needed\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26089b08-b1bf-46bc-bc9b-075d84cb76b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####‚úÖ Why the diagonal clusters + stripes are OK\n",
    "\n",
    "Your target variable grid_stress_score has discrete plateaus (e.g., 0, 12.5, 25, 37.5, 50, 62.5, 75, 87.5).  \n",
    "\n",
    "This means:  \n",
    "Models predict continuous values ‚Üí  \n",
    "When plotted against discrete actual values ‚Üí  \n",
    "You naturally see vertical bands (Actual fixed)  \n",
    "\n",
    "And in residual plots, you see diagonal stripes  \n",
    "üëâ This is NOT a modeling error.  \n",
    "üëâ Both RF and XGB behave the same because the target itself is discrete.  \n",
    "\n",
    "This pattern will always appear unless the scoring system changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67934cc4-17e7-45bb-a1a6-6816d1652816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3. RMSE Trend Plot\n",
    "# =====================================================\n",
    "\n",
    "rmses = []\n",
    "for h in horizons:\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    y_va = y_val[target].values\n",
    "    y_pred = models[target].predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_va, y_pred))\n",
    "    rmses.append(rmse)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(horizons, rmses, marker=\"o\", linewidth=2)\n",
    "plt.title(\"RMSE Trend Across Prediction Horizons\")\n",
    "plt.xlabel(\"Prediction Horizon (hours ahead)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8079256c-2263-4f82-a4e3-f9a63b87a701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Bar Chart: Top 10 Features per Horizon\n",
    "# =====================================================\n",
    "\n",
    "for h in horizons:\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    model = models[target]\n",
    "\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False).head(10)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # sns.barplot(data=imp, x=\"importance\", y=\"feature\", palette=\"magma\") -> for old seaborn version\n",
    "    \n",
    "    sns.barplot(\n",
    "    data=imp,\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    hue=\"feature\",         # assign hue so palette is valid\n",
    "    palette=\"magma\",\n",
    "    dodge=False,\n",
    "    legend=False           # hide legend, not useful here\n",
    ")\n",
    "\n",
    "    plt.title(f\"Top 10 Features for +{h}h Prediction\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77434bc-d5cd-48a2-9fc0-9aea97c5a5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 11. Feature importance for +1h model\n",
    "# ============================================\n",
    "\n",
    "h = 1\n",
    "target_name = f\"stress_plus_{h}h\"\n",
    "rf = models[target_name]\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "feat_imp.head(20).plot(kind=\"barh\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 20 Feature Importances ‚Äì Stress +1h\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "feat_imp.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f1a6d7-0ed6-4906-970b-fe9ecaa26456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Save trained XGBoost models\n",
    "# =====================================================\n",
    "\n",
    "import pickle\n",
    "import base64\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rows = []\n",
    "\n",
    "for h in horizons:\n",
    "    target = f\"stress_plus_{h}h\"\n",
    "    \n",
    "    if target not in models:\n",
    "        print(\"‚ùå Model missing:\", target)\n",
    "        continue\n",
    "\n",
    "    model = models[target]\n",
    "    \n",
    "    model_bytes = pickle.dumps(model)\n",
    "    model_b64 = base64.b64encode(model_bytes).decode(\"utf-8\")\n",
    "    \n",
    "    rows.append(Row(\n",
    "        horizon=target,\n",
    "        model_name=f\"XGBoost_{target}\",\n",
    "        model_binary=model_b64\n",
    "    ))\n",
    "\n",
    "spark_df = spark.createDataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66a26c9-4d25-4ffb-9da5-a56c75153295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"workspace.default.AT_XGBoost_grid_stress_models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f558cafd-9bf5-4f0b-a4ba-ace757f4c556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚úÖ What you have now\n",
    "\n",
    "- A **time-based modeling dataset** with:  \n",
    "  - lag features  \n",
    "  - rolling features  \n",
    "  - time bins  \n",
    "  - weather  \n",
    "  - imports/exports  \n",
    "- Targets: stress_plus_1h ‚Ä¶ stress_plus_6h  \n",
    "- A **XGBoost model per horizon**  \n",
    "- MAE/RMSE for each horizon  \n",
    "- Example prediction plot  \n",
    "- Feature importance for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605e0f99-ac15-490d-88c0-efd065e064bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3111c9de-7a01-406b-aa39-337650994833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ac8976-a379-497b-a5bf-0b6a5afe7615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_saved = spark.table(\"workspace.default.AT_XGBoost_grid_stress_models\")\n",
    "df_saved.printSchema()\n",
    "display(df_saved.limit(5))\n",
    "\n",
    "# ‚úî Each row = one XGBoost model\n",
    "# ‚úî model_binary = pickled model stored as bytes\n",
    "# ‚úî You do NOT need .pkl files anymore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26941388-357b-459b-af76-d24590f13478",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check horizons contains 6 numbers"
    }
   },
   "outputs": [],
   "source": [
    "print(horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a609ce03-5d09-4e5f-8df8-e23bd61d138f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c0a1c1-321f-422c-b8b7-12068ad48794",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop tables"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS workspace.default.DE_XGBoost_grid_stress_models\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prediction_AT_XGBoost",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
