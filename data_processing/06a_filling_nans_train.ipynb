{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff03a592-9ef9-4ef2-9f55-e070f981f3ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This notebook is used to fill NULLS values in the train set.\n",
    "# It creates a mask of known values and substitutes them with the mean, median and fbfill of that type of generation for the country. If all values per type are NULL the values will keep as NULL\n",
    "# Computes the MAE of the filled values and the known, and selectsselect per column the best filling method for each column (smallest MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571b5540-5972-49fc-b277-6cbad38466b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load train dataset"
    }
   },
   "outputs": [],
   "source": [
    "train = spark.table(\"workspace.default.train_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcf5de7-1f71-4403-b4c8-fc223e0082a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "See first 5 rows"
    }
   },
   "outputs": [],
   "source": [
    "display(train.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6c93b9-283b-4df1-97a1-b94dd511b30c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove all rows corresponding o countries that don't have information about generation"
    }
   },
   "outputs": [],
   "source": [
    "missing_countries = [\"DK\", \"FI\", \"LV\", \"SE\", \"EE\", \"GR\", \"RO\", \"SI\", \"NO\", \"CH\", \"BG\"]\n",
    "train = train.filter(~train[\"country\"].isin(missing_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f0b5ae-3426-4589-891e-4d6b6d152b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.select(\"country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483a15ac-51cf-461d-b7e0-916cc06acf29",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select only columns that should be imputed (we know from EDA process)"
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    c for c, t in train.dtypes\n",
    "    if t in (\"double\", \"float\", \"int\", \"bigint\")\n",
    "    and (\n",
    "        c.endswith(\"__Actual_Aggregated\")\n",
    "        or c.endswith(\"__Actual_Consumption\")\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd311f60-a734-4525-9227-c485710f0581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove columns that are fully empty"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Count nulls per column\n",
    "null_counts = train.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "\n",
    "# Identify columns where all values are null\n",
    "all_null_cols = [c for c, cnt in null_counts.items() if cnt == train.count()]\n",
    "\n",
    "print(\"Columns with all null values:\", all_null_cols)\n",
    "\n",
    "# Drop these columns\n",
    "if all_null_cols:\n",
    "    train = train.drop(*all_null_cols)\n",
    "\n",
    "# Update numeric_cols list\n",
    "numeric_cols = [c for c in numeric_cols if c not in all_null_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda3250c-fe3c-4412-8fe3-5fb390965abd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create artificial mask (10% NULLS per country per column)"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_all = train\n",
    "\n",
    "# Preserve original values\n",
    "for c in numeric_cols:\n",
    "    df_all = df_all.withColumn(f\"{c}_original\", F.col(c))\n",
    "\n",
    "# Create deliberate 10% mask per country\n",
    "for c in numeric_cols:\n",
    "    df_all = df_all.withColumn(\n",
    "        f\"{c}_was_masked\",\n",
    "        ((F.rand(seed=42) < 0.1) & F.col(c).isNotNull()).cast(\"int\")\n",
    "    )\n",
    "    df_all = df_all.withColumn(\n",
    "        f\"{c}_mask\",\n",
    "        F.when(F.col(f\"{c}_was_masked\") == 1, None).otherwise(F.col(c))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3457abee-2016-4885-bcfa-02c843714812",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Trying different methods to fill NULLS"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregation-based per group\n",
    "# Substitute nulls with the mean of that type of generation for the country. If all values per type are NULL the values will keep as NULL\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Use df_all from previous cell, which contains mask columns\n",
    "w = Window.partitionBy(\"country\")\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df_all = df_all.withColumn(\n",
    "        f\"{c}_mean\",\n",
    "        F.when(F.col(f\"{c}_mask\").isNull(), F.avg(f\"{c}_mask\").over(w))\n",
    "         .otherwise(F.col(f\"{c}_mask\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0b2e25-d9dd-4922-a16f-2d195d218d4a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764002812234}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23623442-4a4d-4580-8bf8-c9b6961bec86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forward + Backward Fill: fills nulls with the last known value. Then, fill with next known value (reverse order) for filling the remaining nulls.\n",
    "\n",
    "w_ff = Window.partitionBy(\"country\").orderBy(\"index\") \\\n",
    "          .rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "w_bf = Window.partitionBy(\"country\").orderBy(F.col(\"index\").desc()).rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df_all = df_all.withColumn(f\"{c}_ffill\", F.last(f\"{c}_mask\", ignorenulls=True).over(w_ff))\n",
    "    df_all = df_all.withColumn(f\"{c}_fbfill\",\n",
    "                               F.coalesce(F.col(f\"{c}_ffill\"), F.last(f\"{c}_mask\", ignorenulls=True).over(w_bf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35731957-4d96-4953-9c9b-65dcae0df08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Median\n",
    "\n",
    "for c in numeric_cols:\n",
    "    df_all = df_all.withColumn(\n",
    "        f\"{c}_median\",\n",
    "        F.when(F.col(f\"{c}_mask\").isNull(), F.expr(f\"percentile_approx({c}_mask, 0.5)\").over(w))\n",
    "         .otherwise(F.col(f\"{c}_mask\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28889f6-7390-4689-8e91-01785f1763ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluate all methods (per method per country)"
    }
   },
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"mean\": \"_mean\",\n",
    "    \"ffill_bfill\": \"_fbfill\",\n",
    "    \"median\": \"_median\"\n",
    "}\n",
    "\n",
    "mae_results = []\n",
    "\n",
    "for c in numeric_cols:\n",
    "    mask_condition = F.col(f\"{c}_was_masked\") == 1  # only rows deliberately masked\n",
    "\n",
    "    for method_name, suffix in methods.items():\n",
    "        mae_df = (\n",
    "            df_all.filter(mask_condition)\n",
    "                  .groupBy(\"country\")\n",
    "                  .agg(F.mean(F.abs(F.col(f\"{c}{suffix}\") - F.col(f\"{c}_original\"))).alias(\"mae\"))\n",
    "                  .withColumn(\"column\", F.lit(c))\n",
    "                  .withColumn(\"method\", F.lit(method_name))\n",
    "        )\n",
    "        mae_results.append(mae_df)\n",
    "\n",
    "# Combine all results\n",
    "mae_all_spark = mae_results[0]\n",
    "for m in mae_results[1:]:\n",
    "    mae_all_spark = mae_all_spark.unionByName(m)\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "mae_pdf = mae_all_spark.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4cae763-f123-44b2-a452-115193a1976b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mae_pdf.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63507ff-f052-42d9-880d-cd58a3288c8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select best imputation method per column"
    }
   },
   "outputs": [],
   "source": [
    "best_method_per_col = (\n",
    "    mae_pdf.groupby(\"column\")\n",
    "           .apply(lambda x: x.loc[x[\"mae\"].idxmin()])\n",
    "           .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07aa0c5-5941-46a4-b8ff-bd51ba6352c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Turn it into dictionary"
    }
   },
   "outputs": [],
   "source": [
    "best_method_map = {\n",
    "    (row.country, row.column): row.method\n",
    "    for _, row in best_method_per_col.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0634783-d143-4d25-aedc-0f4583353fe2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display best method per column"
    }
   },
   "outputs": [],
   "source": [
    "best_method_per_col[[\"column\", \"method\", \"mae\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73702642-3b2b-4fbd-8e6c-12e00ffec63b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Apply best method to fill NULLS in every column"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best methods to impute nulls are: n\" + best_method_per_col[\"method\"].unique()) \n",
    "# Compute the values for mean and median columns \n",
    "mean_values = {}\n",
    "\n",
    "for (country, column), method in best_method_map.items():\n",
    "    if method == \"mean\":\n",
    "        val = (\n",
    "            df_all.filter(F.col(\"country\") == country)\n",
    "                  .agg(F.mean(column).alias(column))\n",
    "                  .collect()[0][column]\n",
    "        )\n",
    "        mean_values[(country, column)] = val\n",
    "\n",
    "median_values = {}\n",
    "\n",
    "for (country, column), method in best_method_map.items():\n",
    "    if method == \"median\":\n",
    "        val = (\n",
    "            df_all.filter(F.col(\"country\") == country)\n",
    "                  .approxQuantile(column, [0.5], 0.001)[0]\n",
    "        )\n",
    "        median_values[(country, column)] = val\n",
    "\n",
    "df_imputed = df_all\n",
    "\n",
    "for (country, column), value in mean_values.items():\n",
    "    df_imputed = df_imputed.withColumn(\n",
    "        column,\n",
    "        F.when(\n",
    "            (F.col(\"country\") == country) & F.col(column).isNull(),\n",
    "            F.lit(value)\n",
    "        ).otherwise(F.col(column))\n",
    "    )\n",
    "\n",
    "for (country, column), value in median_values.items():\n",
    "    df_imputed = df_imputed.withColumn(\n",
    "        column,\n",
    "        F.when(\n",
    "            (F.col(\"country\") == country) & F.col(column).isNull(),\n",
    "            F.lit(value)\n",
    "        ).otherwise(F.col(column))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c9bc59-abec-46ce-97ec-2d9de08bb5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify columns that need forward+backward fill\n",
    "# Define windows for forward and backward fills \n",
    "w_ff = Window.partitionBy(\"country\").orderBy(\"index\").rowsBetween(Window.unboundedPreceding, 0) \n",
    "w_bf = Window.partitionBy(\"country\").orderBy(F.col(\"index\").desc()).rowsBetween(Window.unboundedPreceding, 0) \n",
    "\n",
    "# Apply forward + backward fill per column\n",
    "for col in numeric_cols:\n",
    "    df_imputed = df_imputed.withColumn(\n",
    "        f\"{col}_ffill\", F.last(F.col(col), ignorenulls=True).over(w_ff)\n",
    "    )\n",
    "    df_imputed = df_imputed.withColumn(\n",
    "        f\"{col}_fbfill\", F.coalesce(F.col(f\"{col}_ffill\"), F.last(F.col(col), ignorenulls=True).over(w_bf))\n",
    "    )\n",
    "    # Replace the original column with the filled one\n",
    "    df_imputed = df_imputed.drop(col).withColumnRenamed(f\"{col}_fbfill\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e1dd11-e741-4707-bbe4-4273c8e97b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for remaining nulls\n",
    "remaining_nans = df_imputed.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in numeric_cols])\n",
    "display(remaining_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5004bb3-bf53-4154-9b68-b837776560b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fill with zero the remaining NULLS"
    }
   },
   "outputs": [],
   "source": [
    "# some countries don't provide information about certain kind of energy sources, so we can fill them with zeros, like nuclear energy\n",
    "\n",
    "df_imputed = df_imputed.fillna(0, subset=numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255d4f7e-6c60-44a2-9257-0e185d8be250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for remaining nulls\n",
    "remaining_nans = df_imputed.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in numeric_cols])\n",
    "display(remaining_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f407756d-05a5-46c8-a76e-cfe3dc1d3302",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764021764482}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_cols =display(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be8b6e1e-5717-45eb-bd59-133fb719f2dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Eliminate not necessary columns with intermediate steps"
    }
   },
   "outputs": [],
   "source": [
    "original_cols = train.columns\n",
    "\n",
    "helper_cols = [c for c in df_imputed.columns if c not in original_cols]\n",
    "\n",
    "# 3. Remove them\n",
    "df_final = df_imputed.drop(*helper_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5b2a1d-2a7a-429f-99c1-778840fb4080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .saveAsTable(\"train_set_imputed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06a_filling_nans_train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
