{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571b5540-5972-49fc-b277-6cbad38466b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filling_nan(df):\n",
    "    # should eneter path to the table\n",
    "\n",
    "    # import libraries\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql import Window\n",
    "\n",
    "    # load data\n",
    "    data_set = spark.table(str(df))\n",
    "\n",
    "    # Remove all rows corresponding o countries that don't have information about generation\n",
    "    missing_countries = [\"DK\", \"FI\", \"LV\", \"SE\", \"EE\", \"GR\", \"RO\", \"SI\", \"NO\", \"CH\", \"BG\"]\n",
    "    data_set = data_set.filter(~data_set[\"country\"].isin(missing_countries))\n",
    "\n",
    "    # -----  Select only columns that should be imputed (we know from EDA process) --\n",
    "    numeric_cols = [\n",
    "    c for c, t in data_set.dtypes\n",
    "    if t in (\"double\", \"float\", \"int\", \"bigint\")\n",
    "    and (\n",
    "        c.endswith(\"__Actual_Aggregated\")\n",
    "        or c.endswith(\"__Actual_Consumption\")\n",
    "    )\n",
    "    ]\n",
    "\n",
    "    # ----- Remove columns that are fully empty-------\n",
    "    # Count nulls per column\n",
    "    null_counts = data_set.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in numeric_cols]).collect()[0].asDict()\n",
    "\n",
    "    # Identify columns where all values are null\n",
    "    all_null_cols = [c for c, cnt in null_counts.items() if cnt == data_set.count()]\n",
    "\n",
    "    # Drop these columns\n",
    "    if all_null_cols:\n",
    "        data_set = data_set.drop(*all_null_cols)\n",
    "\n",
    "    # Update numeric_cols list\n",
    "    numeric_cols = [c for c in numeric_cols if c not in all_null_cols]\n",
    "\n",
    "    # ------- Create artificial mask (10% NULLS per country -------\n",
    "\n",
    "    df_all = data_set\n",
    "\n",
    "    # Preserve original values\n",
    "    for c in numeric_cols:\n",
    "        df_all = df_all.withColumn(f\"{c}_original\", F.col(c))\n",
    "\n",
    "    # Create deliberate 10% mask per country\n",
    "    for c in numeric_cols:\n",
    "        df_all = df_all.withColumn(\n",
    "            f\"{c}_was_masked\",\n",
    "            ((F.rand(seed=42) < 0.1) & F.col(c).isNotNull()).cast(\"int\")\n",
    "        )\n",
    "        df_all = df_all.withColumn(\n",
    "            f\"{c}_mask\",\n",
    "            F.when(F.col(f\"{c}_was_masked\") == 1, None).otherwise(F.col(c))\n",
    "        )\n",
    "    \n",
    "    # --------Trying different methods to fill NULLS ---------\n",
    "    # Aggregation-based per group\n",
    "    # Substitute nulls with the mean of that type of generation for the country. If all values per type are NULL the values will keep as NULL\n",
    "\n",
    "    # Use df_all from previous cell, which contains mask columns\n",
    "    w = Window.partitionBy(\"country\")\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        df_all = df_all.withColumn(\n",
    "            f\"{c}_mean\",\n",
    "            F.when(F.col(f\"{c}_mask\").isNull(), F.avg(f\"{c}_mask\").over(w))\n",
    "            .otherwise(F.col(f\"{c}_mask\"))\n",
    "        )\n",
    "\n",
    "    # Forward + Backward Fill: fills nulls with the last known value. Then, fill with next known value (reverse order) for filling the remaining nulls.\n",
    "\n",
    "    w_ff = Window.partitionBy(\"country\").orderBy(\"index\") \\\n",
    "          .rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    w_bf = Window.partitionBy(\"country\").orderBy(F.col(\"index\").desc()).rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        df_all = df_all.withColumn(f\"{c}_ffill\", F.last(f\"{c}_mask\", ignorenulls=True).over(w_ff))\n",
    "        df_all = df_all.withColumn(f\"{c}_fbfill\",\n",
    "                               F.coalesce(F.col(f\"{c}_ffill\"), F.last(f\"{c}_mask\", ignorenulls=True).over(w_bf)))\n",
    "        \n",
    "    # Median\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        df_all = df_all.withColumn(\n",
    "            f\"{c}_median\",\n",
    "            F.when(F.col(f\"{c}_mask\").isNull(), F.expr(f\"percentile_approx({c}_mask, 0.5)\").over(w))\n",
    "            .otherwise(F.col(f\"{c}_mask\"))\n",
    "        )\n",
    "\n",
    "    # ------Evaluate all methods (per method per country)-------\n",
    "    methods = {\n",
    "    \"mean\": \"_mean\",\n",
    "    \"ffill_bfill\": \"_fbfill\",\n",
    "    \"median\": \"_median\"\n",
    "    }\n",
    "\n",
    "    mae_results = []\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        mask_condition = F.col(f\"{c}_was_masked\") == 1  # only rows deliberately masked\n",
    "\n",
    "        for method_name, suffix in methods.items():\n",
    "            mae_df = (\n",
    "                df_all.filter(mask_condition)\n",
    "                  .groupBy(\"country\")\n",
    "                  .agg(F.mean(F.abs(F.col(f\"{c}{suffix}\") - F.col(f\"{c}_original\"))).alias(\"mae\"))\n",
    "                  .withColumn(\"column\", F.lit(c))\n",
    "                  .withColumn(\"method\", F.lit(method_name))\n",
    "            )\n",
    "            mae_results.append(mae_df)\n",
    "\n",
    "    # Combine all results\n",
    "    mae_all_spark = mae_results[0]\n",
    "    for m in mae_results[1:]:\n",
    "        mae_all_spark = mae_all_spark.unionByName(m)\n",
    "\n",
    "    # Convert to pandas for plotting\n",
    "    mae_pdf = mae_all_spark.toPandas()\n",
    "\n",
    "    # Select best imputation method per column\n",
    "    best_method_per_col = (\n",
    "    mae_pdf.groupby(\"column\")\n",
    "           .apply(lambda x: x.loc[x[\"mae\"].idxmin()])\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Turn it into dictionary\n",
    "    best_method_map = {\n",
    "    (row.country, row.column): row.method\n",
    "    for _, row in best_method_per_col.iterrows()\n",
    "    }\n",
    "\n",
    "    # ------Apply best method to fill NULLS in every column--------\n",
    "    # Compute the values for mean and median columns \n",
    "    mean_values = {}\n",
    "\n",
    "    for (country, column), method in best_method_map.items():\n",
    "        if method == \"mean\":\n",
    "            val = (\n",
    "                df_all.filter(F.col(\"country\") == country)\n",
    "                    .agg(F.mean(column).alias(column))\n",
    "                    .collect()[0][column]\n",
    "            )\n",
    "            mean_values[(country, column)] = val\n",
    "\n",
    "    median_values = {}\n",
    "\n",
    "    for (country, column), method in best_method_map.items():\n",
    "        if method == \"median\":\n",
    "            val = (\n",
    "                df_all.filter(F.col(\"country\") == country)\n",
    "                    .approxQuantile(column, [0.5], 0.001)[0]\n",
    "            )\n",
    "            median_values[(country, column)] = val\n",
    "\n",
    "    df_imputed = df_all\n",
    "\n",
    "    for (country, column), value in mean_values.items():\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            column,\n",
    "            F.when(\n",
    "                (F.col(\"country\") == country) & F.col(column).isNull(),\n",
    "                F.lit(value)\n",
    "            ).otherwise(F.col(column))\n",
    "        )\n",
    "\n",
    "    for (country, column), value in median_values.items():\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            column,\n",
    "            F.when(\n",
    "                (F.col(\"country\") == country) & F.col(column).isNull(),\n",
    "                F.lit(value)\n",
    "            ).otherwise(F.col(column))\n",
    "        )\n",
    "\n",
    "    # Identify columns that need forward+backward fill\n",
    "    # Define windows for forward and backward fills \n",
    "    w_ff = Window.partitionBy(\"country\").orderBy(\"index\").rowsBetween(Window.unboundedPreceding, 0) \n",
    "    w_bf = Window.partitionBy(\"country\").orderBy(F.col(\"index\").desc()).rowsBetween(Window.unboundedPreceding, 0) \n",
    "\n",
    "    # Apply forward + backward fill per column\n",
    "    for col in numeric_cols:\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            f\"{col}_ffill\", F.last(F.col(col), ignorenulls=True).over(w_ff)\n",
    "        )\n",
    "        df_imputed = df_imputed.withColumn(\n",
    "            f\"{col}_fbfill\", F.coalesce(F.col(f\"{col}_ffill\"), F.last(F.col(col), ignorenulls=True).over(w_bf))\n",
    "        )\n",
    "    # Replace the original column with the filled one\n",
    "    df_imputed = df_imputed.drop(col).withColumnRenamed(f\"{col}_fbfill\", col)\n",
    "\n",
    "    # -------- Fill with zero the remaining NULLS-------\n",
    "    # some countries don't provide information about certain kind of energy sources, so we can fill them with zeros, like nuclear energy\n",
    "\n",
    "    df_imputed = df_imputed.fillna(0, subset=numeric_cols)\n",
    "\n",
    "    # -----Eliminate not necessary columns with intermediate steps-------\n",
    "    original_cols = train.columns\n",
    "\n",
    "    helper_cols = [c for c in df_imputed.columns if c not in original_cols]\n",
    "\n",
    "    # 3. Remove them\n",
    "    df_final = df_imputed.drop(*helper_cols)\n",
    "\n",
    "    # save to table\n",
    "    df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(str(df)+ \"_imputed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d772d8-7751-4e39-ac75-eefeb2b7548d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filling_nan(\"workspace.default.validation_set\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_filling_nans",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
