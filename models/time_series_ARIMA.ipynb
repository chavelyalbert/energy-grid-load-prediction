{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba242878-32df-4465-9d99-aca689fa2244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 0 — Install 2 Python TS forecasting libraries\n",
    "# --------------------------------------------\n",
    "\n",
    "%pip install pmdarima statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94622edc-1db6-4ee8-a4b8-200952d4e203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 1 — Load Data from Databricks\n",
    "# --------------------------------------------\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 1 — LOAD DATA FROM DATABRICKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Load from your existing table\n",
    "df_spark = spark.table(\"workspace.default.train_set_imputed\")\n",
    "\n",
    "# Rename index → timestamp\n",
    "df_spark = df_spark.withColumnRenamed(\"index\", \"timestamp\")\n",
    "\n",
    "df_spark = df_spark.select(\n",
    "    \"timestamp\", \"country\", \"grid_stress_score\",\n",
    "    \"mean_temperature_c\", \"Actual_Load\"\n",
    ")\n",
    "\n",
    "df_spark = df_spark.orderBy(\"timestamp\")\n",
    "\n",
    "df = df_spark.toPandas()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b7d641-ce77-4c10-b6b1-753783795382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 2 — Prepare TS & Fit ARIMA per country\n",
    "# --------------------------------------------\n",
    "\n",
    "# It loops through each country in the dataset, prepares its time series data (hourly stress scores), \n",
    "# and fits an ARIMA model using auto_arima. \n",
    "# The result is a dictionary of fitted models — one per country\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 2 — PREPARE TIME SERIES (PER COUNTRY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "countries = sorted(df[\"country\"].unique())\n",
    "print(\"Countries found:\", countries)\n",
    "\n",
    "arima_models = {}   # to store fitted models\n",
    "series_info = {}    \n",
    "\n",
    "for country in countries:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"▶ Training ARIMA for country: {country}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Filter this country's data\n",
    "    df_country = df[df[\"country\"] == country].copy()\n",
    "\n",
    "    # Sort & index by timestamp\n",
    "    df_country = df_country.sort_values(\"timestamp\")\n",
    "    df_country = df_country.set_index(\"timestamp\")\n",
    "\n",
    "    # Univariate time series (grid stress)\n",
    "    ts = df_country[\"grid_stress_score\"]\n",
    "\n",
    "    print(\"Head:\")\n",
    "    print(ts.head())\n",
    "    print(\"Length:\", len(ts))\n",
    "\n",
    "    # Skip if too few points\n",
    "    if len(ts) < 30:\n",
    "        print(\"⚠️ Not enough observations, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Fit ARIMA automatically\n",
    "    model = auto_arima(\n",
    "        ts,\n",
    "        seasonal=False,         \n",
    "        trace=False,\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True\n",
    "    )\n",
    "\n",
    "    arima_models[country] = model\n",
    "    series_info[country] = {\"n_obs\": len(ts)}\n",
    "\n",
    "    print(f\"✔ Fitted ARIMA for {country}, order={model.order}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272ed17d-1727-42dd-a91a-f63c1fb3a381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 3 — Forecast with ARIMA models\n",
    "# --------------------------------------------\n",
    "\n",
    "# It generates 6-hour-ahead forecasts from the ARIMA models for each country \n",
    "# and stores the results in a dictionary (arima_forecasts) for later use or visualization.\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 3 — FORECAST WITH ARIMA (PER COUNTRY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "arima_forecasts = {}\n",
    "\n",
    "n_periods = 6  # forecast horizon\n",
    "\n",
    "for country, model in arima_models.items():\n",
    "    print(f\"\\n▶ Forecasting for {country}\")\n",
    "    fc = model.predict(n_periods=n_periods)\n",
    "    arima_forecasts[country] = fc\n",
    "    print(fc[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c23625-8597-4870-9a78-2a99d8874e8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "(Add your E-mail Account)"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 4 — SAVE ARIMA MODELS FOR STREAMLIT - Enter your email account for databricks!\n",
    "# --------------------------------------------\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 4 — SAVE ARIMA MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Use the folder path where you want to save the models\n",
    "output_dir = \"/Workspace/Users/(Enter your email account for Databricks here)/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create subfolder for ARIMA models\n",
    "arima_dir = f\"{output_dir}/arima_models\"\n",
    "os.makedirs(arima_dir, exist_ok=True)\n",
    "\n",
    "# Save model for each country\n",
    "for country, model in arima_models.items():\n",
    "    model_path = f\"{arima_dir}/arima_{country}.pkl\"\n",
    "    \n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"✔ Saved ARIMA model for {country} → {model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b627cf-9ec7-4be5-a337-05a4380755c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# SECTION 5 — ARIMA VALIDATION\n",
    "# --------------------------------------------\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 5 — ARIMA VALIDATION (MAE & RMSE ONLY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load validation dataset\n",
    "val_spark = spark.table(\"workspace.default.validation_set_imputed\")\n",
    "\n",
    "# Rename index → timestamp\n",
    "val_spark = val_spark.withColumnRenamed(\"index\", \"timestamp\")\n",
    "\n",
    "# Select required columns\n",
    "val_spark = val_spark.select(\n",
    "    \"timestamp\", \"country\", \"grid_stress_score\"\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "# Convert to pandas\n",
    "val_df = val_spark.toPandas()\n",
    "val_df[\"timestamp\"] = pd.to_datetime(val_df[\"timestamp\"])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for country in countries:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"▶ VALIDATING ARIMA for {country}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    if country not in arima_models:\n",
    "        print(\" No ARIMA model trained for this country — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Filter validation data for this country\n",
    "    df_val_country = val_df[val_df[\"country\"] == country].copy()\n",
    "\n",
    "    if len(df_val_country) == 0:\n",
    "        print(\" No validation data available — skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_val_country = df_val_country.sort_values(\"timestamp\")\n",
    "    df_val_country = df_val_country.set_index(\"timestamp\")\n",
    "\n",
    "    val_ts = df_val_country[\"grid_stress_score\"]\n",
    "\n",
    "    # Forecast same length as validation set\n",
    "    model = arima_models[country]\n",
    "    steps = len(val_ts)\n",
    "    fc = model.predict(n_periods=steps)\n",
    "\n",
    "    # Compute MAE and RMSE\n",
    "    mae = mean_absolute_error(val_ts, fc)\n",
    "    rmse = np.sqrt(mean_squared_error(val_ts, fc))\n",
    "\n",
    "    validation_results[country] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "    print(f\"MAE  = {mae:.3f}\")\n",
    "    print(f\"RMSE = {rmse:.3f}\")\n",
    "\n",
    "# Summary of all countries\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for c, metrics in validation_results.items():\n",
    "    print(f\"{c}:  MAE={metrics['MAE']:.3f}   RMSE={metrics['RMSE']:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TS_ARIMA_13_countries",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
