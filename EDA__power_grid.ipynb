{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f339498d-1a59-42b9-8ee9-cecbbaf07328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading all tables (with FIXED crossborder - double underscore!)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "GRID_SCHEMA = \"european_grid_raw__v2\"\n",
    "WEATHER_SCHEMA = \"european_weather_raw\"\n",
    "\n",
    "# Load crossborder_flows\n",
    "crossborder_flows = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.crossborder_flows\")\n",
    "generation = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.generation\")\n",
    "installed_capacity = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.installed_capacity\")\n",
    "load_actual = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_actual\")\n",
    "load_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_forecast\")\n",
    "solar_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.solar_forecast\")\n",
    "wind_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.wind_forecast\")\n",
    "weather_hourly = spark.table(f\"{CATALOG}.{WEATHER_SCHEMA}.weather_hourly\")\n",
    "\n",
    "print(\"âœ“ All tables loaded!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Row counts:\")\n",
    "print(f\"  crossborder_flows:   {crossborder_flows.count():>12,}\")\n",
    "print(f\"  generation:          {generation.count():>12,}\")\n",
    "print(f\"  installed_capacity:  {installed_capacity.count():>12,}\")\n",
    "print(f\"  load_actual:         {load_actual.count():>12,}\")\n",
    "print(f\"  load_forecast:       {load_forecast.count():>12,}\")\n",
    "print(f\"  solar_forecast:      {solar_forecast.count():>12,}\")\n",
    "print(f\"  wind_forecast:       {wind_forecast.count():>12,}\")\n",
    "print(f\"  weather_hourly:      {weather_hourly.count():>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d44e91-7caf-48fe-b03d-92941d0b0b40",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763551892662}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763551892672}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"EXAMINING TABLE STRUCTURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CROSSBORDER_FLOWS (FIXED):\")\n",
    "print(f\"   Columns: {crossborder_flows.columns}\")\n",
    "display(crossborder_flows.limit(5))\n",
    "\n",
    "print(\"\\n2. LOAD_ACTUAL:\")\n",
    "print(f\"   Columns: {load_actual.columns}\")\n",
    "display(load_actual.limit(3))\n",
    "\n",
    "print(\"\\n3. LOAD_FORECAST:\")\n",
    "print(f\"   Columns: {load_forecast.columns}\")\n",
    "display(load_forecast.limit(3))\n",
    "\n",
    "print(\"\\n4. GENERATION:\")\n",
    "print(f\"   Columns: {generation.columns}\")\n",
    "display(generation.limit(3))\n",
    "\n",
    "print(\"\\n5. SOLAR_FORECAST:\")\n",
    "print(f\"   Columns: {solar_forecast.columns}\")\n",
    "display(solar_forecast.limit(3))\n",
    "\n",
    "print(\"\\n6. WIND_FORECAST:\")\n",
    "print(f\"   Columns: {wind_forecast.columns}\")\n",
    "display(wind_forecast.limit(3))\n",
    "\n",
    "print(\"\\n7. WEATHER_HOURLY:\")\n",
    "print(f\"   Columns: {weather_hourly.columns}\")\n",
    "display(weather_hourly.limit(3))\n",
    "\n",
    "print(\"\\n8. INSTALLED_CAPACITY:\")\n",
    "print(f\"   Columns: {installed_capacity.columns}\")\n",
    "display(installed_capacity.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9013c5-c365-44f9-bfd3-c5e0d378e237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"COLUMN NAMES FOR EACH TABLE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. crossborder_flows: {crossborder_flows.columns}\")\n",
    "print(f\"\\n2. load_actual: {load_actual.columns}\")\n",
    "print(f\"\\n3. load_forecast: {load_forecast.columns}\")\n",
    "print(f\"\\n4. generation: {generation.columns}\")\n",
    "print(f\"\\n5. solar_forecast: {solar_forecast.columns}\")\n",
    "print(f\"\\n6. wind_forecast: {wind_forecast.columns}\")\n",
    "print(f\"\\n7. weather_hourly: {weather_hourly.columns}\")\n",
    "print(f\"\\n8. installed_capacity: {installed_capacity.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9d2c4f-53bc-4b27-b084-4afb9bd76ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Creating master dataset by joining tables...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with load_actual as base, rename 'index' to 'timestamp'\n",
    "master = load_actual.withColumnRenamed('index', 'timestamp')\n",
    "print(f\"âœ“ Base: load_actual - {master.count():,} rows\")\n",
    "\n",
    "# Join load_forecast\n",
    "master = master.join(\n",
    "    load_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added load_forecast - {master.count():,} rows\")\n",
    "\n",
    "# Join generation\n",
    "master = master.join(\n",
    "    generation.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added generation - {master.count():,} rows\")\n",
    "\n",
    "# Join solar_forecast\n",
    "master = master.join(\n",
    "    solar_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added solar_forecast - {master.count():,} rows\")\n",
    "\n",
    "# Join wind_forecast\n",
    "master = master.join(\n",
    "    wind_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added wind_forecast - {master.count():,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"MASTER DATASET: {master.count():,} rows, {len(master.columns)} columns\")\n",
    "print(f\"Columns: {master.columns}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "display(master.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8109dd5b-ce1d-482f-917d-f466176eb9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check rows per country\n",
    "print(\"\\n1. ROWS PER COUNTRY:\")\n",
    "country_counts = master.groupBy('country').count().orderBy(F.desc('count'))\n",
    "display(country_counts)\n",
    "\n",
    "# 2. Check which countries have generation data\n",
    "print(\"\\n2. COUNTRIES WITH vs WITHOUT GENERATION DATA:\")\n",
    "countries_with_gen = master.filter(\n",
    "    F.col('Biomass__Actual_Aggregated').isNotNull()\n",
    ").select('country').distinct().count()\n",
    "\n",
    "total_countries = master.select('country').distinct().count()\n",
    "\n",
    "print(f\"   Total countries: {total_countries}\")\n",
    "print(f\"   With generation data: {countries_with_gen}\")\n",
    "print(f\"   Without generation data: {total_countries - countries_with_gen}\")\n",
    "\n",
    "# 3. Sample countries with generation data\n",
    "print(\"\\n3. SAMPLE COUNTRIES WITH GENERATION DATA:\")\n",
    "countries_gen_list = master.filter(\n",
    "    F.col('Biomass__Actual_Aggregated').isNotNull()\n",
    ").select('country').distinct().orderBy('country')\n",
    "display(countries_gen_list)\n",
    "\n",
    "# 4. Check missing percentages for key columns\n",
    "print(\"\\n4. MISSING VALUE PERCENTAGES:\")\n",
    "total_rows = master.count()\n",
    "\n",
    "key_cols = ['Actual_Load', 'Forecasted_Load', 'Solar', 'Wind_Onshore', \n",
    "            'Nuclear__Actual_Aggregated', 'Solar__Actual_Aggregated']\n",
    "\n",
    "missing_data = []\n",
    "for col in key_cols:\n",
    "    null_count = master.filter(F.col(col).isNull()).count()\n",
    "    missing_pct = (null_count / total_rows) * 100\n",
    "    missing_data.append({\n",
    "        'column': col,\n",
    "        'missing_pct': f\"{missing_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb9a665-1501-4617-9bfa-c5e5e779ebd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"CREATING TWO DATASET VERSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# VERSION 1: ALL COUNTRIES - Basic features only (no generation)\n",
    "print(\"\\nVERSION 1: All Countries - Load + Forecasts Only\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "master_v1 = master.select(\n",
    "    'timestamp', 'country', \n",
    "    'Actual_Load', 'Forecasted_Load', \n",
    "    'Solar', 'Wind_Onshore'\n",
    ").filter(\n",
    "    F.col('Actual_Load').isNotNull()  # Remove rows with no load data\n",
    ")\n",
    "\n",
    "print(f\"Rows: {master_v1.count():,}\")\n",
    "print(f\"Columns: {len(master_v1.columns)}\")\n",
    "print(f\"Countries: {master_v1.select('country').distinct().count()}\")\n",
    "\n",
    "# VERSION 2: 8 COUNTRIES - All features including generation\n",
    "print(\"\\nVERSION 2: 8 Countries - Complete Features\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "countries_with_gen = ['AT', 'BE', 'CH', 'DE', 'ES', 'FR', 'IT', 'NL']\n",
    "master_v2 = master.filter(\n",
    "    (F.col('country').isin(countries_with_gen)) & \n",
    "    (F.col('Actual_Load').isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"Rows: {master_v2.count():,}\")\n",
    "print(f\"Columns: {len(master_v2.columns)}\")\n",
    "print(f\"Countries: {master_v2.select('country').distinct().count()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DECISION TIME:\")\n",
    "print(\"  Version 1: More data (all countries), fewer features\")\n",
    "print(\"  Version 2: Less data (8 countries), all features\")\n",
    "print(\"\\nWhich version do you want to use for correlation analysis?\")\n",
    "print(\"(Recommendation: Version 2 for better insights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cd230c-8c95-408e-b5e9-f1b45859aa16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load all tables\n",
    "print(\"STEP 1: Loading all tables...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "GRID_SCHEMA = \"european_grid_raw__v2\"\n",
    "WEATHER_SCHEMA = \"european_weather_raw\"\n",
    "\n",
    "crossborder_flows = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.crossborder_flows\")\n",
    "generation = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.generation\")\n",
    "installed_capacity = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.installed_capacity\")\n",
    "load_actual = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_actual\")\n",
    "load_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_forecast\")\n",
    "solar_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.solar_forecast\")\n",
    "wind_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.wind_forecast\")\n",
    "weather_hourly = spark.table(f\"{CATALOG}.{WEATHER_SCHEMA}.weather_hourly\")\n",
    "\n",
    "print(\"âœ“ All tables loaded!\")\n",
    "\n",
    "# Step 2: Join tables\n",
    "print(\"\\nSTEP 2: Joining tables with LEFT JOIN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with load_actual, rename 'index' to 'timestamp'\n",
    "master = load_actual.withColumnRenamed('index', 'timestamp')\n",
    "print(f\"âœ“ Base: load_actual - {master.count():,} rows\")\n",
    "\n",
    "# Join load_forecast\n",
    "master = master.join(\n",
    "    load_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added load_forecast\")\n",
    "\n",
    "# Join generation\n",
    "master = master.join(\n",
    "    generation.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added generation\")\n",
    "\n",
    "# Join solar_forecast\n",
    "master = master.join(\n",
    "    solar_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added solar_forecast\")\n",
    "\n",
    "# Join wind_forecast\n",
    "master = master.join(\n",
    "    wind_forecast.withColumnRenamed('index', 'timestamp'),\n",
    "    on=['timestamp', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"âœ“ Added wind_forecast\")\n",
    "\n",
    "print(f\"\\nâœ“ Master dataset created: {master.count():,} rows, {len(master.columns)} columns\")\n",
    "\n",
    "# Step 3: Clean the dataset\n",
    "print(\"\\nSTEP 3: Cleaning dataset...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove rows with no actual load\n",
    "master_clean = master.filter(F.col('Actual_Load').isNotNull())\n",
    "print(f\"âœ“ Filtered rows: {master_clean.count():,} rows remaining\")\n",
    "\n",
    "# Fill NULLs with 0 for generation columns (NULL = no capacity)\n",
    "generation_cols = [col for col in master_clean.columns if 'Actual_Aggregated' in col or 'Actual_Consumption' in col]\n",
    "print(f\"âœ“ Filling {len(generation_cols)} generation columns with 0 (where NULL = no capacity)...\")\n",
    "\n",
    "for col in generation_cols:\n",
    "    master_clean = master_clean.fillna(0, subset=[col])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MASTER DATASET:\")\n",
    "print(f\"  Rows: {master_clean.count():,}\")\n",
    "print(f\"  Columns: {len(master_clean.columns)}\")\n",
    "print(f\"  Countries: {master_clean.select('country').distinct().count()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "display(master_clean.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8774797-c7b8-4aee-835e-00686263293e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Converting ALL data to Pandas (no sampling)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert entire dataset\n",
    "print(f\"Converting {master_clean.count():,} rows to Pandas...\")\n",
    "print(\"This may take 1-3 minutes...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pandas_df = master_clean.toPandas()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ SUCCESS! Pandas DataFrame created in {elapsed_time:.1f} seconds\")\n",
    "print(f\"  Shape: {pandas_df.shape[0]:,} rows Ã— {pandas_df.shape[1]} columns\")\n",
    "print(f\"  Memory usage: {pandas_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(pandas_df.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(pandas_df.dtypes)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(pandas_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8ed840-69c7-4272-9177-f2093358eb98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"STEP 7: CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = pandas_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "print(f\"\\nNumeric columns: {len(numeric_cols)}\")\n",
    "print(numeric_cols)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "print(\"\\nCalculating correlation matrix...\")\n",
    "correlation_matrix = pandas_df[numeric_cols].corr()\n",
    "\n",
    "print(f\"âœ“ Correlation matrix calculated: {correlation_matrix.shape}\")\n",
    "\n",
    "# Show correlations with Actual_Load (our target)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATIONS WITH ACTUAL_LOAD (sorted by strength):\")\n",
    "print(\"=\"*60)\n",
    "load_correlations = correlation_matrix['Actual_Load'].sort_values(ascending=False)\n",
    "print(load_correlations)\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "print(\"\\nCreating correlation heatmap...\")\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix - European Power Grid Features', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35858ec2-4aa1-4847-be30-1e627a1c6869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a cleaner visualization of top correlations\n",
    "print(\"\\nTOP 10 CORRELATIONS WITH ACTUAL_LOAD:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_correlations = load_correlations.drop('Actual_Load').head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_correlations.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.title('Top 10 Features Correlated with Actual Load', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8ae76a-4571-4aac-be04-5754de1f9cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"STEP 8: SAVING DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Option 1: Save as Delta table (BEST for Databricks)\n",
    "print(\"\\n1. Saving as Delta table...\")\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.power_grid_master\")\n",
    "print(\"   âœ“ Saved: workspace.default.power_grid_master\")\n",
    "\n",
    "# Option 2: Save correlation matrix as Delta table\n",
    "print(\"\\n2. Saving correlation matrix as Delta table...\")\n",
    "corr_df = correlation_matrix.reset_index()\n",
    "corr_df.columns = ['feature'] + list(corr_df.columns[1:])\n",
    "corr_spark = spark.createDataFrame(corr_df)\n",
    "corr_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.correlation_matrix\")\n",
    "print(\"   âœ“ Saved: workspace.default.correlation_matrix\")\n",
    "\n",
    "# Verify the saves\n",
    "print(\"\\n3. Verifying saved tables...\")\n",
    "master_count = spark.table(\"workspace.default.power_grid_master\").count()\n",
    "corr_count = spark.table(\"workspace.default.correlation_matrix\").count()\n",
    "print(f\"   âœ“ power_grid_master: {master_count:,} rows\")\n",
    "print(f\"   âœ“ correlation_matrix: {corr_count} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY - Delta Tables Saved:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Dataset: {pandas_df.shape[0]:,} rows Ã— {pandas_df.shape[1]} cols\")\n",
    "print(f\"  Memory: {pandas_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"  Date range: {pandas_df['timestamp'].min()} to {pandas_df['timestamp'].max()}\")\n",
    "print(f\"  Countries: {pandas_df['country'].nunique()}\")\n",
    "print(\"\\nâœ“ All tables saved successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HOW TO LOAD LATER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# Load main dataset:\n",
    "df = spark.table('workspace.default.power_grid_master').toPandas()\n",
    "\n",
    "# Load correlation matrix:\n",
    "corr = spark.table('workspace.default.correlation_matrix').toPandas()\n",
    "corr = corr.set_index('feature')\n",
    "\n",
    "# Or use in Spark directly:\n",
    "spark_df = spark.table('workspace.default.power_grid_master')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a74461-6087-4196-8819-62aade4caf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the exact location of your tables\n",
    "print(\"LOCATING YOUR SAVED TABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: In the Databricks UI\n",
    "print(\"\\nðŸ“ METHOD 1: Databricks UI (Left Sidebar)\")\n",
    "print(\"-\"*60)\n",
    "print(\"1. Click 'Catalog' icon in the left sidebar\")\n",
    "print(\"2. Navigate to: workspace â†’ default\")\n",
    "print(\"3. You'll see:\")\n",
    "print(\"   âœ“ power_grid_master\")\n",
    "print(\"   âœ“ correlation_matrix\")\n",
    "\n",
    "# Method 2: Show tables via SQL\n",
    "print(\"\\nðŸ“Š METHOD 2: List tables programmatically\")\n",
    "print(\"-\"*60)\n",
    "display(spark.sql(\"SHOW TABLES IN workspace.default\"))\n",
    "\n",
    "# Method 3: Get detailed table info\n",
    "print(\"\\nðŸ“‹ METHOD 3: Detailed table information\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\n1. POWER_GRID_MASTER:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED workspace.default.power_grid_master\").show(30, truncate=False)\n",
    "\n",
    "print(\"\\n2. CORRELATION_MATRIX:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED workspace.default.correlation_matrix\").show(30, truncate=False)\n",
    "\n",
    "# Show physical location\n",
    "print(\"\\nðŸ’¾ PHYSICAL STORAGE LOCATION:\")\n",
    "print(\"-\"*60)\n",
    "location_df = spark.sql(\"\"\"\n",
    "    DESCRIBE EXTENDED workspace.default.power_grid_master\n",
    "\"\"\").filter(\"col_name = 'Location'\")\n",
    "display(location_df)\n",
    "\n",
    "print(\"\\nâœ“ Your tables are stored in the Databricks workspace!\")\n",
    "print(\"âœ“ They persist across sessions and won't be deleted unless you explicitly drop them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014a5393-e321-4fd7-9b7e-a3a6bddec977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ANALYZING DATA TO DEFINE GRID FAILURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load your saved dataset\n",
    "df = spark.table('workspace.default.power_grid_master').toPandas()\n",
    "\n",
    "# Option 1: Check if we can calculate total generation\n",
    "print(\"\\n1. CHECKING TOTAL GENERATION vs LOAD:\")\n",
    "generation_cols = [col for col in df.columns if 'Actual_Aggregated' in col]\n",
    "print(f\"   Generation columns available: {len(generation_cols)}\")\n",
    "\n",
    "# Calculate total generation\n",
    "df['Total_Generation'] = df[generation_cols].sum(axis=1)\n",
    "df['Generation_Deficit'] = df['Actual_Load'] - df['Total_Generation']\n",
    "\n",
    "print(f\"   Mean generation deficit: {df['Generation_Deficit'].mean():.2f}\")\n",
    "print(f\"   % of time with deficit: {(df['Generation_Deficit'] > 0).sum() / len(df) * 100:.2f}%\")\n",
    "\n",
    "# Option 2: Forecast deviation\n",
    "print(\"\\n2. CHECKING FORECAST DEVIATIONS:\")\n",
    "df['Forecast_Error'] = abs(df['Actual_Load'] - df['Forecasted_Load'])\n",
    "df['Forecast_Error_Pct'] = (df['Forecast_Error'] / df['Forecasted_Load']) * 100\n",
    "\n",
    "print(f\"   Mean forecast error: {df['Forecast_Error'].mean():.2f} MW\")\n",
    "print(f\"   Mean forecast error %: {df['Forecast_Error_Pct'].mean():.2f}%\")\n",
    "print(f\"   95th percentile error: {df['Forecast_Error_Pct'].quantile(0.95):.2f}%\")\n",
    "\n",
    "# Large errors (potential failures)\n",
    "large_errors = df[df['Forecast_Error_Pct'] > 10]\n",
    "print(f\"   Rows with >10% error: {len(large_errors)} ({len(large_errors)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Option 3: Rapid load changes\n",
    "print(\"\\n3. CHECKING RAPID LOAD CHANGES:\")\n",
    "df_sorted = df.sort_values(['country', 'timestamp'])\n",
    "df_sorted['Load_Change'] = df_sorted.groupby('country')['Actual_Load'].diff()\n",
    "df_sorted['Load_Change_Pct'] = (df_sorted['Load_Change'] / df_sorted['Actual_Load']) * 100\n",
    "\n",
    "print(f\"   Mean hourly load change: {df_sorted['Load_Change_Pct'].abs().mean():.2f}%\")\n",
    "print(f\"   95th percentile change: {df_sorted['Load_Change_Pct'].abs().quantile(0.95):.2f}%\")\n",
    "\n",
    "rapid_changes = df_sorted[df_sorted['Load_Change_Pct'].abs() > 20]\n",
    "print(f\"   Rows with >20% change: {len(rapid_changes)} ({len(rapid_changes)/len(df_sorted)*100:.2f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION FOR FAILURE DEFINITION:\")\n",
    "print(\"=\"*60)\n",
    "display(df[['timestamp', 'country', 'Actual_Load', 'Forecasted_Load', 'Total_Generation', \n",
    "            'Forecast_Error_Pct', 'Generation_Deficit']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595da18c-06f1-4bf8-8222-9494434a8727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"CHECKING RENEWABLE DATA AVAILABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load your dataset\n",
    "df = spark.table('workspace.default.power_grid_master').toPandas()\n",
    "\n",
    "# Check renewable features\n",
    "renewable_features = [\n",
    "    'Solar_Forecast',\n",
    "    'Wind_Onshore_Forecast', \n",
    "    'Solar__Actual_Aggregated',\n",
    "    'Wind_Onshore__Actual_Aggregated',\n",
    "    'Wind_Offshore__Actual_Aggregated',\n",
    "    'Hydro_Water_Reservoir__Actual_Aggregated',\n",
    "    'Hydro_Run_of_river_and_poundage__Actual_Aggregated'\n",
    "]\n",
    "\n",
    "print(\"\\n1. RENEWABLE DATA AVAILABILITY:\")\n",
    "print(\"-\"*60)\n",
    "for col in renewable_features:\n",
    "    if col in df.columns:\n",
    "        non_null = df[col].notna().sum()\n",
    "        non_zero = (df[col] > 0).sum()\n",
    "        pct_available = (non_null / len(df)) * 100\n",
    "        pct_active = (non_zero / len(df)) * 100\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Non-null: {non_null:,} ({pct_available:.1f}%)\")\n",
    "        print(f\"  Non-zero: {non_zero:,} ({pct_active:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "# Check forecast vs actual correlation\n",
    "print(\"\\n2. FORECAST VS ACTUAL CORRELATIONS:\")\n",
    "print(\"-\"*60)\n",
    "if 'Solar_Forecast' in df.columns and 'Solar__Actual_Aggregated' in df.columns:\n",
    "    corr_solar = df[['Solar_Forecast', 'Solar__Actual_Aggregated']].corr().iloc[0, 1]\n",
    "    print(f\"Solar Forecast â†” Solar Actual: {corr_solar:.3f}\")\n",
    "\n",
    "if 'Wind_Onshore_Forecast' in df.columns and 'Wind_Onshore__Actual_Aggregated' in df.columns:\n",
    "    corr_wind = df[['Wind_Onshore_Forecast', 'Wind_Onshore__Actual_Aggregated']].corr().iloc[0, 1]\n",
    "    print(f\"Wind Forecast â†” Wind Actual: {corr_wind:.3f}\")\n",
    "\n",
    "# Countries with renewable data\n",
    "print(\"\\n3. COUNTRIES WITH RENEWABLE DATA:\")\n",
    "print(\"-\"*60)\n",
    "countries_with_solar = df[df['Solar__Actual_Aggregated'] > 0]['country'].nunique()\n",
    "countries_with_wind = df[df['Wind_Onshore__Actual_Aggregated'] > 0]['country'].nunique()\n",
    "total_countries = df['country'].nunique()\n",
    "\n",
    "print(f\"Countries with Solar data: {countries_with_solar}/{total_countries}\")\n",
    "print(f\"Countries with Wind data: {countries_with_wind}/{total_countries}\")\n",
    "\n",
    "# Alternative features we DO have\n",
    "print(\"\\n4. FEATURES WE DEFINITELY HAVE:\")\n",
    "print(\"-\"*60)\n",
    "guaranteed_features = [\n",
    "    'Forecasted_Load',\n",
    "    'Actual_Load',\n",
    "    'timestamp',\n",
    "    'country'\n",
    "]\n",
    "\n",
    "for col in guaranteed_features:\n",
    "    if col in df.columns:\n",
    "        print(f\"âœ… {col} - 100% available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA__power_grid",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
