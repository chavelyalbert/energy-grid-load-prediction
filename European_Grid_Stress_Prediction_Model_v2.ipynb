{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8921f34a-8dfb-457d-9776-112b50079ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# European Power Grid Stress Prediction Model\n",
    "# Predicts grid stress and blackout risk 4 hours in advance\n",
    "# Data source: ENTSOE Transparency Platform (2023-2025)\n",
    "# Countries: 26 European nations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Data catalog\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "SCHEMA_GRID = \"european_grid_raw__v2\"\n",
    "SCHEMA_WEATHER = \"european_weather_raw\"\n",
    "\n",
    "print(\"European Power Grid Stress Prediction Model\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Catalog: {CATALOG}\")\n",
    "print(f\"Grid Schema: {SCHEMA_GRID}\")\n",
    "print(f\"Weather Schema: {SCHEMA_WEATHER}\")\n",
    "print(\"\\nLibraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a05c61-7387-41d3-b73d-01624ea11c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load core datasets from ENTSOE transparency platform\n",
    "# Grid data: load, forecasts, cross-border flows\n",
    "# Weather data: temperature, wind, solar radiation\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load grid tables\n",
    "load_actual = spark.table(f\"{CATALOG}.{SCHEMA_GRID}.load_actual\")\n",
    "load_forecast = spark.table(f\"{CATALOG}.{SCHEMA_GRID}.load_forecast\")\n",
    "crossborder = spark.table(f\"{CATALOG}.{SCHEMA_GRID}.crossborder_flows\")\n",
    "generation = spark.table(f\"{CATALOG}.{SCHEMA_GRID}.generation\")\n",
    "\n",
    "# Load weather\n",
    "weather = spark.table(f\"{CATALOG}.{SCHEMA_WEATHER}.weather_hourly\")\n",
    "\n",
    "# Dataset sizes\n",
    "print(f\"load_actual:      {load_actual.count():>12,} rows\")\n",
    "print(f\"load_forecast:    {load_forecast.count():>12,} rows\")\n",
    "print(f\"crossborder:      {crossborder.count():>12,} rows\")\n",
    "print(f\"generation:       {generation.count():>12,} rows\")\n",
    "print(f\"weather:          {weather.count():>12,} rows\")\n",
    "\n",
    "# Temporal coverage\n",
    "date_range = load_actual.agg(\n",
    "    F.min(\"index\").alias(\"start\"),\n",
    "    F.max(\"index\").alias(\"end\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nTemporal coverage: {date_range['start']} to {date_range['end']}\")\n",
    "\n",
    "# Countries\n",
    "countries = [row['country'] for row in load_actual.select(\"country\").distinct().orderBy(\"country\").collect()]\n",
    "print(f\"Countries ({len(countries)}): {', '.join(countries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbc44ab-148a-4473-8022-62214ed19831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality assessment - check for missing values and coverage gaps\n",
    "\n",
    "print(\"Data Quality Assessment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def check_nulls(df, name):\n",
    "    \"\"\"Check null percentage for each column\"\"\"\n",
    "    total = df.count()\n",
    "    null_info = []\n",
    "    for col in df.columns:\n",
    "        null_count = df.filter(F.col(col).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_info.append((col, null_count, null_count/total*100))\n",
    "    return null_info\n",
    "\n",
    "# Check load_actual\n",
    "print(\"\\n1. LOAD ACTUAL\")\n",
    "nulls = check_nulls(load_actual, \"load_actual\")\n",
    "if nulls:\n",
    "    for col, count, pct in nulls:\n",
    "        print(f\"   {col}: {count:,} nulls ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values\")\n",
    "\n",
    "# Check load_forecast\n",
    "print(\"\\n2. LOAD FORECAST\")\n",
    "nulls = check_nulls(load_forecast, \"load_forecast\")\n",
    "if nulls:\n",
    "    for col, count, pct in nulls:\n",
    "        print(f\"   {col}: {count:,} nulls ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values\")\n",
    "\n",
    "# Check crossborder\n",
    "print(\"\\n3. CROSSBORDER FLOWS\")\n",
    "nulls = check_nulls(crossborder, \"crossborder\")\n",
    "if nulls:\n",
    "    for col, count, pct in nulls:\n",
    "        print(f\"   {col}: {count:,} nulls ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values\")\n",
    "\n",
    "# Check data coverage per country\n",
    "print(\"\\n4. RECORDS PER COUNTRY\")\n",
    "country_counts = load_actual.groupBy(\"country\").count().orderBy(\"count\", ascending=False).collect()\n",
    "print(f\"   {'Country':<10} {'Records':>12}\")\n",
    "print(f\"   {'-'*10} {'-'*12}\")\n",
    "for row in country_counts[:10]:\n",
    "    print(f\"   {row['country']:<10} {row['count']:>12,}\")\n",
    "print(f\"   ... and {len(country_counts)-10} more countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdafe61-f804-4cf3-aaaa-cf4b669d028c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build unified dataset by joining load actual, forecast, and crossborder flows\n",
    "# Join keys: timestamp (index) and country\n",
    "\n",
    "print(\"Building Unified Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Join load_actual with load_forecast\n",
    "unified = load_actual.alias(\"la\").join(\n",
    "    load_forecast.alias(\"lf\"),\n",
    "    (F.col(\"la.index\") == F.col(\"lf.index\")) & \n",
    "    (F.col(\"la.country\") == F.col(\"lf.country\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"la.index\").alias(\"timestamp\"),\n",
    "    F.col(\"la.country\"),\n",
    "    F.col(\"la.Actual_Load\").alias(\"actual_load\"),\n",
    "    F.col(\"lf.Forecasted_Load\").alias(\"forecast_load\")\n",
    ")\n",
    "\n",
    "# Step 2: Calculate forecast error (supply-demand mismatch indicator)\n",
    "unified = unified.withColumn(\n",
    "    \"forecast_error\", F.col(\"actual_load\") - F.col(\"forecast_load\")\n",
    ").withColumn(\n",
    "    \"forecast_error_pct\",\n",
    "    F.coalesce(F.try_divide(F.col(\"forecast_error\"), F.col(\"forecast_load\")) * 100, F.lit(0))\n",
    ")\n",
    "\n",
    "print(f\"Step 1: Load data joined - {unified.count():,} rows\")\n",
    "\n",
    "# Step 3: Calculate net imports from crossborder flows\n",
    "imports_df = crossborder.groupBy(\"index\", \"to_country\").agg(\n",
    "    F.sum(\"Value\").alias(\"total_imports\")\n",
    ").withColumnRenamed(\"to_country\", \"country\")\n",
    "\n",
    "exports_df = crossborder.groupBy(\"index\", \"from_country\").agg(\n",
    "    F.sum(\"Value\").alias(\"total_exports\")\n",
    ").withColumnRenamed(\"from_country\", \"country\")\n",
    "\n",
    "net_flows = imports_df.alias(\"i\").join(\n",
    "    exports_df.alias(\"e\"),\n",
    "    (F.col(\"i.index\") == F.col(\"e.index\")) & \n",
    "    (F.col(\"i.country\") == F.col(\"e.country\")),\n",
    "    \"outer\"\n",
    ").select(\n",
    "    F.coalesce(F.col(\"i.index\"), F.col(\"e.index\")).alias(\"index\"),\n",
    "    F.coalesce(F.col(\"i.country\"), F.col(\"e.country\")).alias(\"country\"),\n",
    "    F.coalesce(F.col(\"i.total_imports\"), F.lit(0)).alias(\"total_imports\"),\n",
    "    F.coalesce(F.col(\"e.total_exports\"), F.lit(0)).alias(\"total_exports\")\n",
    ").withColumn(\n",
    "    \"net_imports\", F.col(\"total_imports\") - F.col(\"total_exports\")\n",
    ")\n",
    "\n",
    "# Step 4: Join with unified dataset\n",
    "unified = unified.alias(\"u\").join(\n",
    "    net_flows.alias(\"n\"),\n",
    "    (F.col(\"u.timestamp\") == F.col(\"n.index\")) & \n",
    "    (F.col(\"u.country\") == F.col(\"n.country\")),\n",
    "    \"left\"\n",
    ").select(\n",
    "    F.col(\"u.timestamp\"),\n",
    "    F.col(\"u.country\"),\n",
    "    F.col(\"u.actual_load\"),\n",
    "    F.col(\"u.forecast_load\"),\n",
    "    F.col(\"u.forecast_error\"),\n",
    "    F.col(\"u.forecast_error_pct\"),\n",
    "    F.coalesce(F.col(\"n.total_imports\"), F.lit(0)).alias(\"total_imports\"),\n",
    "    F.coalesce(F.col(\"n.total_exports\"), F.lit(0)).alias(\"total_exports\"),\n",
    "    F.coalesce(F.col(\"n.net_imports\"), F.lit(0)).alias(\"net_imports\")\n",
    ")\n",
    "\n",
    "# Step 5: Calculate import dependency ratio\n",
    "unified = unified.withColumn(\n",
    "    \"import_ratio\",\n",
    "    F.coalesce(F.try_divide(F.col(\"net_imports\"), F.col(\"actual_load\")), F.lit(0))\n",
    ")\n",
    "\n",
    "print(f\"Step 2: Crossborder flows joined - {unified.count():,} rows\")\n",
    "print(f\"\\nColumns: {unified.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8e77ac-f0be-4330-b495-197f285e91d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Integrate weather data by mapping coordinates to countries\n",
    "# Weather data has lat/lon - we need to aggregate by country\n",
    "\n",
    "print(\"Weather Data Integration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Get unique coordinates from weather data\n",
    "print(\"Step 1: Extracting unique coordinates...\")\n",
    "unique_coords = weather.select(\"lat\", \"lon\").distinct()\n",
    "coord_count = unique_coords.count()\n",
    "print(f\"   Unique coordinate pairs: {coord_count:,}\")\n",
    "\n",
    "# Step 2: Map coordinates to countries using reverse geocoding\n",
    "print(\"\\nStep 2: Mapping coordinates to countries...\")\n",
    "\n",
    "coords_list = unique_coords.collect()\n",
    "coord_tuples = [(float(row['lat']), float(row['lon'])) for row in coords_list]\n",
    "\n",
    "# Install and use reverse_geocode\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', 'reverse_geocode', '-q', '--break-system-packages'])\n",
    "import reverse_geocode\n",
    "\n",
    "countries_result = reverse_geocode.search(coord_tuples)\n",
    "mapping_data = [(coord[0], coord[1], loc['country_code']) for coord, loc in zip(coord_tuples, countries_result)]\n",
    "coord_country_map = spark.createDataFrame(mapping_data, [\"lat\", \"lon\", \"weather_country\"])\n",
    "\n",
    "print(f\"   Coordinates mapped to countries\")\n",
    "\n",
    "# Step 3: Filter to our 26 grid countries\n",
    "our_countries = ['AT', 'BE', 'BG', 'CH', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', \n",
    "                 'HR', 'HU', 'IE', 'IT', 'LT', 'LV', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK']\n",
    "\n",
    "weather_countries = [row['weather_country'] for row in coord_country_map.select(\"weather_country\").distinct().collect()]\n",
    "overlap = set(our_countries) & set(weather_countries)\n",
    "print(f\"   Grid countries with weather data: {len(overlap)}/26\")\n",
    "\n",
    "# Step 4: Join weather with coordinate mapping and aggregate by country/hour\n",
    "print(\"\\nStep 3: Aggregating weather by country and hour...\")\n",
    "\n",
    "weather_with_country = weather.join(coord_country_map, on=[\"lat\", \"lon\"], how=\"inner\")\n",
    "weather_filtered = weather_with_country.filter(F.col(\"weather_country\").isin(our_countries))\n",
    "\n",
    "weather_agg = weather_filtered.groupBy(\n",
    "    F.col(\"weather_country\").alias(\"country\"),\n",
    "    F.date_trunc(\"hour\", F.col(\"timestamp\")).alias(\"weather_hour\")\n",
    ").agg(\n",
    "    F.avg(\"temperature_c\").alias(\"temp_avg\"),\n",
    "    F.min(\"temperature_c\").alias(\"temp_min\"),\n",
    "    F.max(\"temperature_c\").alias(\"temp_max\"),\n",
    "    F.stddev(\"temperature_c\").alias(\"temp_std\"),\n",
    "    F.avg(\"wind_speed\").alias(\"wind_avg\"),\n",
    "    F.max(\"wind_speed\").alias(\"wind_max\"),\n",
    "    F.avg(\"ssrd\").alias(\"solar_radiation_avg\")\n",
    ")\n",
    "\n",
    "print(f\"   Aggregated weather records: {weather_agg.count():,}\")\n",
    "\n",
    "# Step 5: Join weather with unified dataset\n",
    "print(\"\\nStep 4: Joining weather with grid data...\")\n",
    "\n",
    "unified = unified.withColumn(\"join_hour\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "\n",
    "unified = unified.alias(\"u\").join(\n",
    "    weather_agg.alias(\"w\"),\n",
    "    (F.col(\"u.country\") == F.col(\"w.country\")) & \n",
    "    (F.col(\"u.join_hour\") == F.col(\"w.weather_hour\")),\n",
    "    \"left\"\n",
    ").select(\n",
    "    F.col(\"u.timestamp\"),\n",
    "    F.col(\"u.country\"),\n",
    "    F.col(\"u.actual_load\"),\n",
    "    F.col(\"u.forecast_load\"),\n",
    "    F.col(\"u.forecast_error\"),\n",
    "    F.col(\"u.forecast_error_pct\"),\n",
    "    F.col(\"u.total_imports\"),\n",
    "    F.col(\"u.total_exports\"),\n",
    "    F.col(\"u.net_imports\"),\n",
    "    F.col(\"u.import_ratio\"),\n",
    "    F.col(\"w.temp_avg\"),\n",
    "    F.col(\"w.temp_min\"),\n",
    "    F.col(\"w.temp_max\"),\n",
    "    F.col(\"w.temp_std\"),\n",
    "    F.col(\"w.wind_avg\"),\n",
    "    F.col(\"w.wind_max\"),\n",
    "    F.col(\"w.solar_radiation_avg\")\n",
    ")\n",
    "\n",
    "# Verify weather coverage\n",
    "weather_coverage = unified.filter(F.col(\"temp_avg\").isNotNull()).count() / unified.count() * 100\n",
    "print(f\"   Weather coverage: {weather_coverage:.1f}%\")\n",
    "print(f\"\\nUnified dataset ready: {unified.count():,} rows, {len(unified.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2d34b8-78f8-4a02-b074-283054de076d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "# Analyze distributions, patterns, and relationships in the data\n",
    "\n",
    "print(\"Exploratory Data Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert sample to Pandas for EDA\n",
    "sample_fraction = 0.1\n",
    "eda_sample = unified.sample(fraction=sample_fraction, seed=42).toPandas()\n",
    "print(f\"Sample size for EDA: {len(eda_sample):,} rows ({sample_fraction*100:.0f}% of data)\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\n1. DESCRIPTIVE STATISTICS\")\n",
    "print(\"-\"*60)\n",
    "numeric_cols = ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "                'net_imports', 'import_ratio', 'temp_avg', 'wind_avg']\n",
    "print(eda_sample[numeric_cols].describe().round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a828010-c89c-4837-b2ea-4efd27448f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize distributions of key variables\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Key Variables', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Row 1: Grid variables\n",
    "axes[0, 0].hist(eda_sample['actual_load'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('MW')\n",
    "axes[0, 0].set_title('Actual Load')\n",
    "\n",
    "axes[0, 1].hist(eda_sample['forecast_error'].clip(-5000, 5000), bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('MW')\n",
    "axes[0, 1].set_title('Forecast Error (clipped)')\n",
    "\n",
    "axes[0, 2].hist(eda_sample['forecast_error_pct'].clip(-50, 50), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 2].set_xlabel('%')\n",
    "axes[0, 2].set_title('Forecast Error % (clipped)')\n",
    "\n",
    "axes[0, 3].hist(eda_sample['import_ratio'].clip(-1, 1), bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[0, 3].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 3].set_xlabel('Ratio')\n",
    "axes[0, 3].set_title('Import Ratio (clipped)')\n",
    "\n",
    "# Row 2: Weather and temporal patterns\n",
    "axes[1, 0].hist(eda_sample['temp_avg'], bins=50, edgecolor='black', alpha=0.7, color='orangered')\n",
    "axes[1, 0].set_xlabel('Celsius')\n",
    "axes[1, 0].set_title('Temperature')\n",
    "\n",
    "axes[1, 1].hist(eda_sample['wind_avg'], bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
    "axes[1, 1].set_xlabel('m/s')\n",
    "axes[1, 1].set_title('Wind Speed')\n",
    "\n",
    "# Hourly load pattern\n",
    "eda_sample['hour'] = pd.to_datetime(eda_sample['timestamp']).dt.hour\n",
    "hourly_load = eda_sample.groupby('hour')['actual_load'].mean()\n",
    "axes[1, 2].bar(hourly_load.index, hourly_load.values, color='steelblue', edgecolor='black')\n",
    "axes[1, 2].set_xlabel('Hour of Day')\n",
    "axes[1, 2].set_ylabel('MW')\n",
    "axes[1, 2].set_title('Average Load by Hour')\n",
    "\n",
    "# Load by country (top 10)\n",
    "country_load = eda_sample.groupby('country')['actual_load'].mean().sort_values(ascending=True).tail(10)\n",
    "axes[1, 3].barh(country_load.index, country_load.values, color='steelblue', edgecolor='black')\n",
    "axes[1, 3].set_xlabel('MW')\n",
    "axes[1, 3].set_title('Average Load by Country (Top 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"- Actual load is right-skewed (many small countries, few large)\")\n",
    "print(\"- Forecast errors are centered near zero with long tails (stress events)\")\n",
    "print(\"- Import ratio mostly between -25% to +25% (moderate cross-border dependency)\")\n",
    "print(\"- Clear daily load pattern with morning and evening peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126c7896-943e-4fa1-b1f7-01a5e14a2ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation analysis between features\n",
    "\n",
    "print(\"2. CORRELATION ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Select numeric columns for correlation\n",
    "corr_cols = ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "             'total_imports', 'total_exports', 'net_imports', 'import_ratio',\n",
    "             'temp_avg', 'temp_min', 'temp_max', 'wind_avg', 'wind_max', 'solar_radiation_avg']\n",
    "\n",
    "corr_matrix = eda_sample[corr_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 11))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, linewidths=0.5, ax=ax,\n",
    "            annot_kws={'size': 9})\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "print(\"\\nStrongest correlations (|r| > 0.3):\")\n",
    "print(\"-\"*50)\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.3:\n",
    "            print(f\"  {corr_matrix.columns[i]:<20} <-> {corr_matrix.columns[j]:<20}: {corr_val:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cde8f78-0485-49c2-9b42-6a229068dbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data against the April 28, 2025 Spain/Portugal blackout\n",
    "# This real event helps us understand what stress patterns look like\n",
    "\n",
    "print(\"3. BLACKOUT EVENT ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "print(\"April 28, 2025: Major blackout affected Spain and Portugal\")\n",
    "print(\"Blackout started approximately 10:45 local time\\n\")\n",
    "\n",
    "# Filter for ES and PT on blackout day\n",
    "blackout_day = unified.filter(\n",
    "    (F.col(\"country\").isin([\"ES\", \"PT\"])) &\n",
    "    (F.to_date(\"timestamp\") == \"2025-04-28\")\n",
    ").orderBy(\"timestamp\", \"country\").toPandas()\n",
    "\n",
    "blackout_day['hour'] = pd.to_datetime(blackout_day['timestamp']).dt.hour\n",
    "\n",
    "# Compare blackout day to normal day (one week before)\n",
    "normal_day = unified.filter(\n",
    "    (F.col(\"country\").isin([\"ES\", \"PT\"])) &\n",
    "    (F.to_date(\"timestamp\") == \"2025-04-21\")\n",
    ").orderBy(\"timestamp\", \"country\").toPandas()\n",
    "\n",
    "normal_day['hour'] = pd.to_datetime(normal_day['timestamp']).dt.hour\n",
    "\n",
    "# Aggregate by hour for comparison\n",
    "blackout_hourly = blackout_day.groupby(['country', 'hour'])['actual_load'].mean().reset_index()\n",
    "normal_hourly = normal_day.groupby(['country', 'hour'])['actual_load'].mean().reset_index()\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, country in enumerate(['ES', 'PT']):\n",
    "    ax = axes[idx]\n",
    "    normal = normal_hourly[normal_hourly['country'] == country]\n",
    "    blackout = blackout_hourly[blackout_hourly['country'] == country]\n",
    "    \n",
    "    ax.plot(normal['hour'], normal['actual_load'], 'b-o', label='Normal Day (Apr 21)', linewidth=2, markersize=4)\n",
    "    ax.plot(blackout['hour'], blackout['actual_load'], 'r-o', label='Blackout Day (Apr 28)', linewidth=2, markersize=4)\n",
    "    ax.axvline(x=11, color='red', linestyle='--', alpha=0.7, label='Blackout ~11:00')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Load (MW)')\n",
    "    ax.set_title(f'{country} - Normal vs Blackout Day')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the dramatic load drop\n",
    "print(\"Load comparison at key hours:\")\n",
    "print(\"-\"*50)\n",
    "for country in ['ES', 'PT']:\n",
    "    normal_10 = normal_hourly[(normal_hourly['country']==country) & (normal_hourly['hour']==10)]['actual_load'].values[0]\n",
    "    blackout_11 = blackout_hourly[(blackout_hourly['country']==country) & (blackout_hourly['hour']==11)]['actual_load'].values[0]\n",
    "    drop_pct = (1 - blackout_11/normal_10) * 100\n",
    "    print(f\"  {country}: Normal 10:00 = {normal_10:,.0f} MW, Blackout 11:00 = {blackout_11:,.0f} MW (drop: {drop_pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9a83f4-4401-45d1-a3de-8a726f3e0963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define stress score based on grid conditions\n",
    "# This will be used to create our prediction target\n",
    "\n",
    "print(\"4. TARGET VARIABLE DEFINITION\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Strategy: Create a stress score based on observable grid conditions,\n",
    "then define a forward-looking target to predict stress BEFORE it happens.\n",
    "\n",
    "Stress indicators:\n",
    "- Forecast error: Supply-demand mismatch (primary signal)\n",
    "- Import ratio: Cross-border dependency (vulnerability)\n",
    "- Unusual load patterns: Deviation from country norms\n",
    "\"\"\")\n",
    "\n",
    "# Calculate country-specific baselines for normalization\n",
    "country_stats = unified.groupBy(\"country\").agg(\n",
    "    F.avg(\"forecast_error\").alias(\"country_fe_mean\"),\n",
    "    F.stddev(\"forecast_error\").alias(\"country_fe_std\"),\n",
    "    F.avg(\"import_ratio\").alias(\"country_ir_mean\"),\n",
    "    F.stddev(\"import_ratio\").alias(\"country_ir_std\"),\n",
    "    F.avg(\"actual_load\").alias(\"country_load_mean\"),\n",
    "    F.stddev(\"actual_load\").alias(\"country_load_std\")\n",
    ")\n",
    "\n",
    "# Join country stats and calculate z-scores\n",
    "unified = unified.join(country_stats, on=\"country\", how=\"left\")\n",
    "\n",
    "unified = unified.withColumn(\n",
    "    \"forecast_error_zscore\",\n",
    "    (F.col(\"forecast_error\") - F.col(\"country_fe_mean\")) / (F.col(\"country_fe_std\") + 1e-6)\n",
    ").withColumn(\n",
    "    \"import_ratio_zscore\",\n",
    "    (F.col(\"import_ratio\") - F.col(\"country_ir_mean\")) / (F.col(\"country_ir_std\") + 1e-6)\n",
    ").withColumn(\n",
    "    \"load_zscore\",\n",
    "    (F.col(\"actual_load\") - F.col(\"country_load_mean\")) / (F.col(\"country_load_std\") + 1e-6)\n",
    ")\n",
    "\n",
    "# Calculate stress score (weighted combination)\n",
    "# Higher absolute z-scores = more unusual/stressed conditions\n",
    "unified = unified.withColumn(\n",
    "    \"stress_score\",\n",
    "    0.50 * F.abs(F.col(\"forecast_error_zscore\")) +\n",
    "    0.30 * F.when(F.col(\"import_ratio_zscore\") > 0, F.col(\"import_ratio_zscore\")).otherwise(0) +\n",
    "    0.20 * F.abs(F.col(\"load_zscore\"))\n",
    ")\n",
    "\n",
    "# Define stress thresholds based on percentiles\n",
    "stress_percentiles = unified.approxQuantile(\"stress_score\", [0.85, 0.95], 0.01)\n",
    "p85, p95 = stress_percentiles\n",
    "\n",
    "print(f\"Stress score percentiles:\")\n",
    "print(f\"  85th percentile: {p85:.3f}\")\n",
    "print(f\"  95th percentile: {p95:.3f}\")\n",
    "\n",
    "# Create binary stress indicator (top 15% = high stress)\n",
    "unified = unified.withColumn(\n",
    "    \"high_stress\",\n",
    "    F.when(F.col(\"stress_score\") >= p85, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "stress_dist = unified.groupBy(\"high_stress\").count().collect()\n",
    "total = sum([r['count'] for r in stress_dist])\n",
    "print(f\"\\nStress distribution:\")\n",
    "for r in stress_dist:\n",
    "    pct = r['count']/total*100\n",
    "    label = \"High stress\" if r['high_stress']==1 else \"Normal\"\n",
    "    print(f\"  {label}: {r['count']:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e36412-f71e-435e-88c0-8f85089945b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the PREDICTIVE target: Will high stress occur in the next 4 hours?\n",
    "# This is the key difference from detection - we predict BEFORE it happens\n",
    "\n",
    "print(\"5. FORWARD-LOOKING PREDICTION TARGET\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Goal: Predict if high stress will occur in the NEXT 4 hours\n",
    "      using only information available NOW.\n",
    "\n",
    "This gives grid operators lead time to take preventive action.\n",
    "\n",
    "Prediction horizon: 4 hours = 16 time steps (15-min intervals)\n",
    "\"\"\")\n",
    "\n",
    "# Define forward-looking window\n",
    "PREDICTION_HORIZON = 16  # 4 hours ahead (16 x 15-min intervals)\n",
    "\n",
    "forward_window = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(1, PREDICTION_HORIZON)\n",
    "\n",
    "# Create forward-looking target\n",
    "# If ANY of the next 16 periods has high_stress=1, then target=1\n",
    "unified = unified.withColumn(\n",
    "    \"stress_next_4h\",\n",
    "    F.when(F.max(\"high_stress\").over(forward_window) == 1, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Check new target distribution\n",
    "target_dist = unified.groupBy(\"stress_next_4h\").count().orderBy(\"stress_next_4h\").collect()\n",
    "total = sum([r['count'] for r in target_dist])\n",
    "\n",
    "print(\"Forward-looking target distribution:\")\n",
    "print(\"-\"*50)\n",
    "for r in target_dist:\n",
    "    pct = r['count']/total*100\n",
    "    label = \"Stress coming in 4h\" if r['stress_next_4h']==1 else \"No stress in 4h\"\n",
    "    print(f\"  {r['stress_next_4h']} ({label}): {r['count']:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Validate: Check blackout day - did we have warning signs?\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Validation: April 28, 2025 blackout predictions\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "blackout_check = unified.filter(\n",
    "    (F.col(\"country\") == \"ES\") &\n",
    "    (F.to_date(\"timestamp\") == \"2025-04-28\") &\n",
    "    (F.hour(\"timestamp\") >= 6) &\n",
    "    (F.hour(\"timestamp\") <= 12)\n",
    ").select(\n",
    "    \"timestamp\", \"actual_load\", \"stress_score\", \"high_stress\", \"stress_next_4h\"\n",
    ").orderBy(\"timestamp\").toPandas()\n",
    "\n",
    "print(\"\\nSpain - Morning of blackout (06:00-12:00):\")\n",
    "print(blackout_check.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16b2d88-41f4-4e9b-a75b-65844c06b0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Create features that capture patterns BEFORE stress events occur\n",
    "\n",
    "print(\"6. FEATURE ENGINEERING\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Creating predictive features using only past/current information:\n",
    "- Temporal features (hour, day, month, cyclical encoding)\n",
    "- Lag features (past values at t-1, t-4, t-16, t-96)\n",
    "- Rolling statistics (trends over 4h and 24h windows)\n",
    "- Rate of change (momentum indicators)\n",
    "- Weather features and their lags\n",
    "\"\"\")\n",
    "\n",
    "# Temporal features\n",
    "import math\n",
    "\n",
    "unified = unified.withColumn(\"hour\", F.hour(\"timestamp\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\")) \\\n",
    "    .withColumn(\"month\", F.month(\"timestamp\")) \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.dayofweek(\"timestamp\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "print(\"Temporal features added: hour, day_of_week, month, is_weekend, cyclical encodings\")\n",
    "\n",
    "# Define windows for lag and rolling features\n",
    "country_window = Window.partitionBy(\"country\").orderBy(\"timestamp\")\n",
    "roll_4h = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(-15, 0)\n",
    "roll_24h = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(-95, 0)\n",
    "\n",
    "# Lag features - past values at different horizons\n",
    "# t-1 (15 min), t-4 (1 hour), t-16 (4 hours), t-96 (24 hours)\n",
    "lag_configs = [\n",
    "    (\"forecast_error\", [1, 4, 16, 96]),\n",
    "    (\"forecast_error_pct\", [1, 4]),\n",
    "    (\"import_ratio\", [1, 4, 96]),\n",
    "    (\"actual_load\", [1, 96]),\n",
    "    (\"temp_avg\", [4, 96]),\n",
    "    (\"wind_avg\", [4])\n",
    "]\n",
    "\n",
    "for col, lags in lag_configs:\n",
    "    for lag in lags:\n",
    "        unified = unified.withColumn(f\"{col}_lag_{lag}\", F.lag(col, lag).over(country_window))\n",
    "\n",
    "print(\"Lag features added: forecast_error, import_ratio, load, temp, wind at various horizons\")\n",
    "\n",
    "# Rolling statistics - capture recent trends\n",
    "unified = unified.withColumn(\"fe_roll_4h_mean\", F.avg(\"forecast_error\").over(roll_4h)) \\\n",
    "    .withColumn(\"fe_roll_4h_std\", F.stddev(\"forecast_error\").over(roll_4h)) \\\n",
    "    .withColumn(\"fe_roll_24h_mean\", F.avg(\"forecast_error\").over(roll_24h)) \\\n",
    "    .withColumn(\"fe_roll_24h_std\", F.stddev(\"forecast_error\").over(roll_24h)) \\\n",
    "    .withColumn(\"ir_roll_4h_mean\", F.avg(\"import_ratio\").over(roll_4h)) \\\n",
    "    .withColumn(\"ir_roll_24h_mean\", F.avg(\"import_ratio\").over(roll_24h)) \\\n",
    "    .withColumn(\"load_roll_24h_mean\", F.avg(\"actual_load\").over(roll_24h)) \\\n",
    "    .withColumn(\"load_roll_24h_std\", F.stddev(\"actual_load\").over(roll_24h)) \\\n",
    "    .withColumn(\"temp_roll_24h_mean\", F.avg(\"temp_avg\").over(roll_24h)) \\\n",
    "    .withColumn(\"wind_roll_24h_mean\", F.avg(\"wind_avg\").over(roll_24h))\n",
    "\n",
    "print(\"Rolling statistics added: 4h and 24h means/stds for key variables\")\n",
    "\n",
    "# Rate of change - momentum indicators\n",
    "unified = unified.withColumn(\n",
    "    \"fe_change_1h\", F.col(\"forecast_error\") - F.col(\"forecast_error_lag_4\")\n",
    ").withColumn(\n",
    "    \"load_change_1h\", F.col(\"actual_load\") - F.col(\"actual_load_lag_1\")\n",
    ").withColumn(\n",
    "    \"load_change_pct\", \n",
    "    F.coalesce(F.try_divide(F.col(\"load_change_1h\"), F.col(\"actual_load_lag_1\")) * 100, F.lit(0))\n",
    ").withColumn(\n",
    "    \"temp_change_24h\", F.col(\"temp_avg\") - F.col(\"temp_avg_lag_96\")\n",
    ").withColumn(\n",
    "    \"fe_vs_roll_4h\", F.col(\"forecast_error\") - F.col(\"fe_roll_4h_mean\")\n",
    ")\n",
    "\n",
    "print(\"Rate of change features added: hourly and daily momentum indicators\")\n",
    "\n",
    "print(f\"\\nTotal columns after feature engineering: {len(unified.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4748ed9e-9954-4c0b-a3e4-a141ad264371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation analysis for predictive features\n",
    "# Important: Check for multicollinearity and identify strongest predictors\n",
    "\n",
    "print(\"7. FEATURE CORRELATION ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Sample for correlation analysis\n",
    "corr_sample = unified.select(\n",
    "    # Key predictive features\n",
    "    'forecast_error', 'forecast_error_pct', 'import_ratio', 'actual_load',\n",
    "    'temp_avg', 'wind_avg', 'solar_radiation_avg',\n",
    "    # Lag features\n",
    "    'forecast_error_lag_1', 'forecast_error_lag_4', 'forecast_error_lag_96',\n",
    "    'import_ratio_lag_1', 'import_ratio_lag_96',\n",
    "    # Rolling features\n",
    "    'fe_roll_4h_mean', 'fe_roll_24h_mean', 'ir_roll_4h_mean',\n",
    "    'load_roll_24h_mean', 'temp_roll_24h_mean',\n",
    "    # Rate of change\n",
    "    'fe_change_1h', 'load_change_pct', 'fe_vs_roll_4h',\n",
    "    # Target\n",
    "    'stress_next_4h'\n",
    ").sample(fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_sample.corr()['stress_next_4h'].drop('stress_next_4h').sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"Correlation with target (stress_next_4h):\")\n",
    "print(\"-\"*50)\n",
    "for feat, corr in target_corr.items():\n",
    "    bar = \"+\" * int(abs(corr) * 30) if corr > 0 else \"-\" * int(abs(corr) * 30)\n",
    "    print(f\"  {feat:<25} {corr:+.3f} {bar}\")\n",
    "\n",
    "# Plot correlation heatmap for key features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Correlation with target bar chart\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if x > 0 else 'red' for x in target_corr.values]\n",
    "ax1.barh(range(len(target_corr)), target_corr.values, color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(target_corr)))\n",
    "ax1.set_yticklabels(target_corr.index)\n",
    "ax1.set_xlabel('Correlation with stress_next_4h')\n",
    "ax1.set_title('Feature Correlation with Target')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Feature correlation heatmap (subset)\n",
    "key_features = ['forecast_error', 'forecast_error_pct', 'import_ratio', \n",
    "                'fe_roll_4h_mean', 'fe_roll_24h_mean', 'ir_roll_4h_mean',\n",
    "                'fe_change_1h', 'fe_vs_roll_4h', 'temp_avg', 'wind_avg', 'stress_next_4h']\n",
    "corr_matrix = corr_sample[key_features].corr()\n",
    "\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            vmin=-1, vmax=1, square=True, linewidths=0.5, ax=ax2, annot_kws={'size': 8})\n",
    "ax2.set_title('Key Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Forecast error features show strongest correlation with future stress\")\n",
    "print(\"- Rolling means capture trend information useful for prediction\")\n",
    "print(\"- Some multicollinearity exists (e.g., fe_roll_4h_mean and fe_roll_24h_mean)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6385a020-04dd-4c51-bd73-6c7e78dc6f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare final feature set for modeling\n",
    "\n",
    "print(\"8. PREPARE MODELING DATASET\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Define feature columns - NO target leakage\n",
    "feature_cols = [\n",
    "    # Base grid features\n",
    "    'actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "    'total_imports', 'total_exports', 'net_imports', 'import_ratio',\n",
    "    \n",
    "    # Weather features\n",
    "    'temp_avg', 'temp_min', 'temp_max', 'wind_avg', 'wind_max', 'solar_radiation_avg',\n",
    "    \n",
    "    # Temporal features\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    \n",
    "    # Lag features\n",
    "    'forecast_error_lag_1', 'forecast_error_lag_4', 'forecast_error_lag_16', 'forecast_error_lag_96',\n",
    "    'forecast_error_pct_lag_1', 'forecast_error_pct_lag_4',\n",
    "    'import_ratio_lag_1', 'import_ratio_lag_4', 'import_ratio_lag_96',\n",
    "    'actual_load_lag_1', 'actual_load_lag_96',\n",
    "    'temp_avg_lag_4', 'temp_avg_lag_96', 'wind_avg_lag_4',\n",
    "    \n",
    "    # Rolling statistics\n",
    "    'fe_roll_4h_mean', 'fe_roll_4h_std', 'fe_roll_24h_mean', 'fe_roll_24h_std',\n",
    "    'ir_roll_4h_mean', 'ir_roll_24h_mean',\n",
    "    'load_roll_24h_mean', 'load_roll_24h_std',\n",
    "    'temp_roll_24h_mean', 'wind_roll_24h_mean',\n",
    "    \n",
    "    # Rate of change\n",
    "    'fe_change_1h', 'load_change_1h', 'load_change_pct',\n",
    "    'temp_change_24h', 'fe_vs_roll_4h'\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures by category:\")\n",
    "print(f\"  Base grid:     8 features\")\n",
    "print(f\"  Weather:       6 features\")\n",
    "print(f\"  Temporal:      8 features\")\n",
    "print(f\"  Lag:          14 features\")\n",
    "print(f\"  Rolling:      10 features\")\n",
    "print(f\"  Rate change:   5 features\")\n",
    "\n",
    "# Select columns and drop nulls (from lag features)\n",
    "id_cols = ['timestamp', 'country']\n",
    "target_col = 'stress_next_4h'\n",
    "\n",
    "model_df = unified.select(id_cols + feature_cols + [target_col])\n",
    "model_df_clean = model_df.dropna()\n",
    "\n",
    "rows_before = model_df.count()\n",
    "rows_after = model_df_clean.count()\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "print(f\"\\nDataset preparation:\")\n",
    "print(f\"  Rows before null removal: {rows_before:,}\")\n",
    "print(f\"  Rows after null removal:  {rows_after:,}\")\n",
    "print(f\"  Rows dropped (lag nulls): {rows_dropped:,} ({rows_dropped/rows_before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918d4897-9a7f-4626-9497-3fe3e0d5ca41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporal train/validation/test split\n",
    "# Critical: Must respect time order - no future data leakage\n",
    "\n",
    "print(\"9. TEMPORAL TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Split strategy (respecting temporal order):\n",
    "- Train:      2023-01-01 to 2024-12-31 (2 years)\n",
    "- Validation: 2025-01-01 to 2025-06-30 (6 months, includes blackout)\n",
    "- Test:       2025-07-01 to 2025-11-07 (4 months, unseen data)\n",
    "\"\"\")\n",
    "\n",
    "# Define split dates\n",
    "train_end = \"2024-12-31 23:59:59\"\n",
    "val_end = \"2025-06-30 23:59:59\"\n",
    "\n",
    "# Split data\n",
    "train_df = model_df_clean.filter(F.col(\"timestamp\") <= train_end)\n",
    "val_df = model_df_clean.filter(\n",
    "    (F.col(\"timestamp\") > train_end) & (F.col(\"timestamp\") <= val_end)\n",
    ")\n",
    "test_df = model_df_clean.filter(F.col(\"timestamp\") > val_end)\n",
    "\n",
    "# Count and percentages\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"Split sizes:\")\n",
    "print(f\"  Train: {train_count:>10,} rows ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Val:   {val_count:>10,} rows ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_count:>10,} rows ({test_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Total: {total_count:>10,} rows\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(f\"\\nTarget distribution (stress_next_4h = 1):\")\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    stress_rate = df.filter(F.col(\"stress_next_4h\") == 1).count() / df.count() * 100\n",
    "    print(f\"  {name}: {stress_rate:.1f}%\")\n",
    "\n",
    "# Convert to Pandas for modeling\n",
    "print(\"\\nConverting to Pandas...\")\n",
    "train_pd = train_df.toPandas()\n",
    "val_pd = val_df.toPandas()\n",
    "test_pd = test_df.toPandas()\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_pd[feature_cols]\n",
    "y_train = train_pd[target_col]\n",
    "\n",
    "X_val = val_pd[feature_cols]\n",
    "y_val = val_pd[target_col]\n",
    "\n",
    "X_test = test_pd[feature_cols]\n",
    "y_test = test_pd[target_col]\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1835be-e38e-4155-b7df-4c19420426cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "# Compare multiple models: Baseline, XGBoost, LightGBM, then Ensemble\n",
    "\n",
    "print(\"10. MODEL TRAINING\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Install required libraries\n",
    "%pip install xgboost lightgbm --quiet\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Calculate class weight for imbalanced data\n",
    "pos_count = y_train.sum()\n",
    "neg_count = len(y_train) - pos_count\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "print(f\"Class balance - Positive: {pos_count:,} ({pos_count/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Class balance - Negative: {neg_count:,} ({neg_count/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 1. Baseline: Logistic Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model 1: Logistic Regression (Baseline)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_pred = lr_model.predict(X_val_scaled)\n",
    "lr_prob = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_val, lr_pred),\n",
    "    'precision': precision_score(y_val, lr_pred),\n",
    "    'recall': recall_score(y_val, lr_pred),\n",
    "    'f1': f1_score(y_val, lr_pred),\n",
    "    'auc': roc_auc_score(y_val, lr_prob)\n",
    "}\n",
    "print(f\"Validation AUC: {results['Logistic Regression']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['Logistic Regression']['f1']:.4f}\")\n",
    "\n",
    "# 2. XGBoost\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model 2: XGBoost\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "xgb_pred = xgb_model.predict(X_val)\n",
    "xgb_prob = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'accuracy': accuracy_score(y_val, xgb_pred),\n",
    "    'precision': precision_score(y_val, xgb_pred),\n",
    "    'recall': recall_score(y_val, xgb_pred),\n",
    "    'f1': f1_score(y_val, xgb_pred),\n",
    "    'auc': roc_auc_score(y_val, xgb_prob)\n",
    "}\n",
    "print(f\"Validation AUC: {results['XGBoost']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['XGBoost']['f1']:.4f}\")\n",
    "\n",
    "# 3. LightGBM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model 3: LightGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "lgb_pred = lgb_model.predict(X_val)\n",
    "lgb_prob = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "results['LightGBM'] = {\n",
    "    'accuracy': accuracy_score(y_val, lgb_pred),\n",
    "    'precision': precision_score(y_val, lgb_pred),\n",
    "    'recall': recall_score(y_val, lgb_pred),\n",
    "    'f1': f1_score(y_val, lgb_pred),\n",
    "    'auc': roc_auc_score(y_val, lgb_prob)\n",
    "}\n",
    "print(f\"Validation AUC: {results['LightGBM']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['LightGBM']['f1']:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODELS COMPARISON (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<22} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'AUC':>10}\")\n",
    "print(\"-\"*72)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<22} {metrics['accuracy']:>10.4f} {metrics['precision']:>10.4f} \"\n",
    "          f\"{metrics['recall']:>10.4f} {metrics['f1']:>10.4f} {metrics['auc']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757f0df1-bfe3-466e-9abc-ee305c9c741f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build Stacking and Voting Ensembles\n",
    "# Combine strengths of multiple models\n",
    "\n",
    "print(\"11. ENSEMBLE MODELS\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Ensemble strategies:\n",
    "1. Voting Ensemble: Average probabilities from all models\n",
    "2. Stacking Ensemble: Use base model predictions as features for meta-learner\n",
    "\"\"\")\n",
    "\n",
    "# 1. Soft Voting Ensemble (average probabilities)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble 1: Soft Voting (Probability Averaging)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average the probabilities from all three models\n",
    "voting_prob = (lr_prob + xgb_prob + lgb_prob) / 3\n",
    "voting_pred = (voting_prob >= 0.5).astype(int)\n",
    "\n",
    "results['Voting Ensemble'] = {\n",
    "    'accuracy': accuracy_score(y_val, voting_pred),\n",
    "    'precision': precision_score(y_val, voting_pred),\n",
    "    'recall': recall_score(y_val, voting_pred),\n",
    "    'f1': f1_score(y_val, voting_pred),\n",
    "    'auc': roc_auc_score(y_val, voting_prob)\n",
    "}\n",
    "print(f\"Validation AUC: {results['Voting Ensemble']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['Voting Ensemble']['f1']:.4f}\")\n",
    "\n",
    "# 2. Weighted Voting (weight by individual AUC performance)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble 2: Weighted Voting (AUC-weighted)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Weight by AUC score\n",
    "lr_weight = results['Logistic Regression']['auc']\n",
    "xgb_weight = results['XGBoost']['auc']\n",
    "lgb_weight = results['LightGBM']['auc']\n",
    "total_weight = lr_weight + xgb_weight + lgb_weight\n",
    "\n",
    "weighted_prob = (lr_weight * lr_prob + xgb_weight * xgb_prob + lgb_weight * lgb_prob) / total_weight\n",
    "weighted_pred = (weighted_prob >= 0.5).astype(int)\n",
    "\n",
    "results['Weighted Voting'] = {\n",
    "    'accuracy': accuracy_score(y_val, weighted_pred),\n",
    "    'precision': precision_score(y_val, weighted_pred),\n",
    "    'recall': recall_score(y_val, weighted_pred),\n",
    "    'f1': f1_score(y_val, weighted_pred),\n",
    "    'auc': roc_auc_score(y_val, weighted_prob)\n",
    "}\n",
    "print(f\"Weights - LR: {lr_weight/total_weight:.2f}, XGB: {xgb_weight/total_weight:.2f}, LGB: {lgb_weight/total_weight:.2f}\")\n",
    "print(f\"Validation AUC: {results['Weighted Voting']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['Weighted Voting']['f1']:.4f}\")\n",
    "\n",
    "# 3. Stacking Ensemble with meta-learner\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble 3: Stacking (Meta-learner on base predictions)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create meta-features from base model predictions\n",
    "meta_train = np.column_stack([\n",
    "    lr_model.predict_proba(X_train_scaled)[:, 1],\n",
    "    xgb_model.predict_proba(X_train)[:, 1],\n",
    "    lgb_model.predict_proba(X_train)[:, 1]\n",
    "])\n",
    "\n",
    "meta_val = np.column_stack([\n",
    "    lr_prob,\n",
    "    xgb_prob,\n",
    "    lgb_prob\n",
    "])\n",
    "\n",
    "# Train meta-learner (Logistic Regression on base model probabilities)\n",
    "meta_learner = LogisticRegression(random_state=42)\n",
    "meta_learner.fit(meta_train, y_train)\n",
    "\n",
    "stacking_prob = meta_learner.predict_proba(meta_val)[:, 1]\n",
    "stacking_pred = meta_learner.predict(meta_val)\n",
    "\n",
    "results['Stacking Ensemble'] = {\n",
    "    'accuracy': accuracy_score(y_val, stacking_pred),\n",
    "    'precision': precision_score(y_val, stacking_pred),\n",
    "    'recall': recall_score(y_val, stacking_pred),\n",
    "    'f1': f1_score(y_val, stacking_pred),\n",
    "    'auc': roc_auc_score(y_val, stacking_prob)\n",
    "}\n",
    "\n",
    "# Show meta-learner coefficients (how much each base model contributes)\n",
    "print(f\"Meta-learner coefficients:\")\n",
    "print(f\"  Logistic Regression: {meta_learner.coef_[0][0]:.3f}\")\n",
    "print(f\"  XGBoost:             {meta_learner.coef_[0][1]:.3f}\")\n",
    "print(f\"  LightGBM:            {meta_learner.coef_[0][2]:.3f}\")\n",
    "print(f\"\\nValidation AUC: {results['Stacking Ensemble']['auc']:.4f}\")\n",
    "print(f\"Validation F1:  {results['Stacking Ensemble']['f1']:.4f}\")\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS COMPARISON (Validation Set)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<22} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'AUC':>10}\")\n",
    "print(\"-\"*72)\n",
    "for model_name, metrics in sorted(results.items(), key=lambda x: x[1]['auc'], reverse=True):\n",
    "    print(f\"{model_name:<22} {metrics['accuracy']:>10.4f} {metrics['precision']:>10.4f} \"\n",
    "          f\"{metrics['recall']:>10.4f} {metrics['f1']:>10.4f} {metrics['auc']:>10.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
    "print(f\"\\nBest model by AUC: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca06a3c4-6126-4a28-9f72-6521efdcba76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model (XGBoost)\n",
    "# Confusion matrix, classification report, and test set performance\n",
    "\n",
    "print(\"12. MODEL EVALUATION\")\n",
    "print(\"-\"*60)\n",
    "print(\"Best model: XGBoost (AUC: 0.8219)\")\n",
    "\n",
    "# Evaluate on test set\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "xgb_test_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, xgb_test_pred),\n",
    "    'precision': precision_score(y_test, xgb_test_pred),\n",
    "    'recall': recall_score(y_test, xgb_test_pred),\n",
    "    'f1': f1_score(y_test, xgb_test_pred),\n",
    "    'auc': roc_auc_score(y_test, xgb_test_prob)\n",
    "}\n",
    "\n",
    "print(f\"\\nTEST SET PERFORMANCE:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {test_metrics['auc']:.4f}\")\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Validation confusion matrix\n",
    "cm_val = confusion_matrix(y_val, xgb_pred)\n",
    "sns.heatmap(cm_val, annot=True, fmt=',d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Stress', 'Stress in 4h'],\n",
    "            yticklabels=['No Stress', 'Stress in 4h'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Validation Set Confusion Matrix\\n(AUC: {results[\"XGBoost\"][\"auc\"]:.4f}, F1: {results[\"XGBoost\"][\"f1\"]:.4f})')\n",
    "\n",
    "# Test confusion matrix\n",
    "cm_test = confusion_matrix(y_test, xgb_test_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt=',d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['No Stress', 'Stress in 4h'],\n",
    "            yticklabels=['No Stress', 'Stress in 4h'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'Test Set Confusion Matrix\\n(AUC: {test_metrics[\"auc\"]:.4f}, F1: {test_metrics[\"f1\"]:.4f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nCLASSIFICATION REPORT (Test Set):\")\n",
    "print(\"-\"*50)\n",
    "print(classification_report(y_test, xgb_test_pred, \n",
    "                            target_names=['No Stress (0)', 'Stress in 4h (1)']))\n",
    "\n",
    "# Operational metrics\n",
    "tn, fp, fn, tp = cm_test.ravel()\n",
    "print(\"\\nOPERATIONAL METRICS:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  True Positives (correctly predicted stress):   {tp:,}\")\n",
    "print(f\"  True Negatives (correctly predicted normal):   {tn:,}\")\n",
    "print(f\"  False Positives (false alarms):                {fp:,}\")\n",
    "print(f\"  False Negatives (missed stress events):        {fn:,}\")\n",
    "print(f\"\\n  Miss Rate (FN / actual positives):  {fn/(tp+fn)*100:.1f}%\")\n",
    "print(f\"  False Alarm Rate (FP / actual neg):  {fp/(tn+fp)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8db1bb6-c175-459f-b18c-e02a8085b066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# Understand which features drive predictions\n",
    "\n",
    "print(\"13. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Get feature importance from XGBoost\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"-\"*50)\n",
    "for i, row in importance_df.head(20).iterrows():\n",
    "    bar = \"#\" * int(row['importance'] * 100)\n",
    "    print(f\"  {row['feature']:<30} {row['importance']:.4f} {bar}\")\n",
    "\n",
    "# Group by feature category\n",
    "def categorize_feature(feat):\n",
    "    if feat in ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "                'total_imports', 'total_exports', 'net_imports', 'import_ratio']:\n",
    "        return 'Base Grid'\n",
    "    elif feat in ['temp_avg', 'temp_min', 'temp_max', 'wind_avg', 'wind_max', 'solar_radiation_avg']:\n",
    "        return 'Weather'\n",
    "    elif feat in ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']:\n",
    "        return 'Temporal'\n",
    "    elif 'lag' in feat:\n",
    "        return 'Lag'\n",
    "    elif 'roll' in feat:\n",
    "        return 'Rolling'\n",
    "    else:\n",
    "        return 'Rate of Change'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(categorize_feature)\n",
    "category_importance = importance_df.groupby('category')['importance'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nImportance by Feature Category:\")\n",
    "print(\"-\"*50)\n",
    "for cat, imp in category_importance.items():\n",
    "    print(f\"  {cat:<20} {imp*100:.1f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 15 features bar chart\n",
    "top_15 = importance_df.head(15)\n",
    "colors = {'Base Grid': 'steelblue', 'Weather': 'coral', 'Temporal': 'green', \n",
    "          'Lag': 'purple', 'Rolling': 'orange', 'Rate of Change': 'teal'}\n",
    "bar_colors = [colors[cat] for cat in top_15['category']]\n",
    "\n",
    "axes[0].barh(range(len(top_15)), top_15['importance'].values, color=bar_colors)\n",
    "axes[0].set_yticks(range(len(top_15)))\n",
    "axes[0].set_yticklabels(top_15['feature'].values)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 15 Features by Importance')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors[cat], label=cat) for cat in colors]\n",
    "axes[0].legend(handles=legend_elements, loc='lower right', fontsize=8)\n",
    "\n",
    "# Category pie chart\n",
    "axes[1].pie(category_importance.values, labels=category_importance.index, \n",
    "            autopct='%1.1f%%', startangle=90, \n",
    "            colors=[colors[cat] for cat in category_importance.index])\n",
    "axes[1].set_title('Feature Importance by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"- Import ratio and its lags are critical (cross-border dependency = vulnerability)\")\n",
    "print(\"- Temporal features important (time-of-day patterns in stress)\")\n",
    "print(\"- Rolling statistics capture trend information\")\n",
    "print(\"- Weather contributes but is not dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db77489-fe19-410d-a067-49168f7255c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ROC Curve and Precision-Recall Curve analysis\n",
    "# Evaluate model performance across different thresholds\n",
    "\n",
    "print(\"14. ROC AND PRECISION-RECALL ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "# Calculate curves for test set\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, xgb_test_prob)\n",
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, xgb_test_prob)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'XGBoost (AUC = {test_metrics[\"auc\"]:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.2)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Test Set)')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "baseline_pr = y_test.sum() / len(y_test)  # Random classifier baseline\n",
    "axes[1].plot(recall_curve, precision_curve, 'g-', linewidth=2, label=f'XGBoost (AUC = {pr_auc:.4f})')\n",
    "axes[1].axhline(y=baseline_pr, color='k', linestyle='--', linewidth=1, label=f'Baseline ({baseline_pr:.2f})')\n",
    "axes[1].fill_between(recall_curve, precision_curve, alpha=0.2, color='green')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve (Test Set)')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Threshold analysis\n",
    "print(\"\\nTHRESHOLD ANALYSIS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Threshold':<12} {'Precision':>10} {'Recall':>10} {'F1':>10} {'False Alarms':>14}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    pred = (xgb_test_prob >= threshold).astype(int)\n",
    "    prec = precision_score(y_test, pred)\n",
    "    rec = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    print(f\"{threshold:<12} {prec:>10.4f} {rec:>10.4f} {f1:>10.4f} {fp:>14,}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Lower threshold (0.3): More warnings, fewer missed events, more false alarms\")\n",
    "print(\"- Higher threshold (0.7): Fewer warnings, more missed events, fewer false alarms\")\n",
    "print(\"- Recommended: 0.4-0.5 for balance between catching events and avoiding alarm fatigue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29820c06-1fd7-432c-8663-0f49b5f80bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate model against the April 28, 2025 Spain/Portugal blackout\n",
    "# This is the ultimate test: could we have predicted it in advance?\n",
    "\n",
    "print(\"15. BLACKOUT VALIDATION - APRIL 28, 2025\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Critical test: Can our model provide early warning before the blackout?\n",
    "The blackout struck Spain and Portugal around 10:45.\n",
    "A useful model should predict stress_next_4h=1 by ~07:00.\n",
    "\"\"\")\n",
    "\n",
    "# Get predictions for Spain on blackout day\n",
    "blackout_data = val_pd[\n",
    "    (val_pd['country'] == 'ES') & \n",
    "    (pd.to_datetime(val_pd['timestamp']).dt.date == pd.to_datetime('2025-04-28').date())\n",
    "].copy()\n",
    "\n",
    "blackout_data['predicted_prob'] = xgb_model.predict_proba(blackout_data[feature_cols])[:, 1]\n",
    "blackout_data['predicted_stress'] = (blackout_data['predicted_prob'] >= 0.5).astype(int)\n",
    "blackout_data['hour'] = pd.to_datetime(blackout_data['timestamp']).dt.hour\n",
    "blackout_data['minute'] = pd.to_datetime(blackout_data['timestamp']).dt.minute\n",
    "\n",
    "# Display key hours\n",
    "print(\"Spain - April 28, 2025 (Blackout Day)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Time':<12} {'Load (MW)':>12} {'Actual':>8} {'Predicted':>10} {'Prob':>8} {'Status':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for _, row in blackout_data.iterrows():\n",
    "    hour = row['hour']\n",
    "    minute = row['minute']\n",
    "    if hour >= 5 and hour <= 14 and minute == 0:  # Show hourly from 05:00 to 14:00\n",
    "        time_str = f\"{hour:02d}:{minute:02d}\"\n",
    "        load = row['actual_load']\n",
    "        actual = int(row[target_col])\n",
    "        pred = int(row['predicted_stress'])\n",
    "        prob = row['predicted_prob']\n",
    "        \n",
    "        # Status indicator\n",
    "        if hour < 10 or (hour == 10 and minute < 45):\n",
    "            if pred == 1 and actual == 1:\n",
    "                status = \"EARLY WARNING\"\n",
    "            elif pred == 1 and actual == 0:\n",
    "                status = \"False Alarm\"\n",
    "            elif pred == 0 and actual == 1:\n",
    "                status = \"MISSED\"\n",
    "            else:\n",
    "                status = \"Normal\"\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                status = \"DETECTED\"\n",
    "            else:\n",
    "                status = \"MISSED\"\n",
    "        \n",
    "        print(f\"{time_str:<12} {load:>12,.0f} {actual:>8} {pred:>10} {prob:>8.2%} {status:<15}\")\n",
    "\n",
    "# Calculate early warning metrics\n",
    "pre_blackout = blackout_data[blackout_data['hour'] < 11]\n",
    "early_warnings = pre_blackout[pre_blackout['predicted_stress'] == 1]\n",
    "actual_stress_pre = pre_blackout[pre_blackout[target_col] == 1]\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"EARLY WARNING ANALYSIS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Pre-blackout periods (before 11:00): {len(pre_blackout)}\")\n",
    "print(f\"  Periods with actual stress_next_4h=1: {len(actual_stress_pre)}\")\n",
    "print(f\"  Early warnings issued: {len(early_warnings)}\")\n",
    "\n",
    "if len(actual_stress_pre) > 0:\n",
    "    early_detection_rate = len(early_warnings[early_warnings[target_col] == 1]) / len(actual_stress_pre) * 100\n",
    "    print(f\"  Early detection rate: {early_detection_rate:.1f}%\")\n",
    "\n",
    "# First early warning time\n",
    "if len(early_warnings) > 0:\n",
    "    first_warning = early_warnings.iloc[0]\n",
    "    first_warning_time = pd.to_datetime(first_warning['timestamp'])\n",
    "    blackout_time = pd.to_datetime('2025-04-28 10:45:00')\n",
    "    lead_time = (blackout_time - first_warning_time).total_seconds() / 3600\n",
    "    print(f\"\\n  First early warning: {first_warning_time.strftime('%H:%M')}\")\n",
    "    print(f\"  Lead time before blackout: {lead_time:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98bd1b49-6f44-4b03-a6a8-7de8ba7640cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze why the model failed to predict in advance\n",
    "# Look at feature values before vs during the blackout\n",
    "\n",
    "print(\"16. PREDICTION FAILURE ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "Why did the model fail to predict the blackout in advance?\n",
    "Let's examine the feature patterns before and during the event.\n",
    "\"\"\")\n",
    "\n",
    "# Get data for analysis\n",
    "blackout_analysis = blackout_data[\n",
    "    (blackout_data['hour'] >= 6) & (blackout_data['hour'] <= 12)\n",
    "].copy()\n",
    "\n",
    "# Key features that should indicate incoming stress\n",
    "key_features = ['forecast_error_pct', 'import_ratio', 'import_ratio_lag_1', \n",
    "                'fe_roll_4h_mean', 'ir_roll_4h_mean', 'load_change_pct']\n",
    "\n",
    "print(\"Feature values on blackout morning (Spain):\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Time':<8} {'FE%':>8} {'IR':>8} {'IR_lag1':>8} {'FE_roll':>10} {'IR_roll':>10} {'Load_chg%':>10} {'Prob':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for _, row in blackout_analysis.iterrows():\n",
    "    hour = row['hour']\n",
    "    minute = row['minute']\n",
    "    if minute == 0:\n",
    "        time_str = f\"{hour:02d}:00\"\n",
    "        print(f\"{time_str:<8} \"\n",
    "              f\"{row['forecast_error_pct']:>8.2f} \"\n",
    "              f\"{row['import_ratio']:>8.3f} \"\n",
    "              f\"{row['import_ratio_lag_1']:>8.3f} \"\n",
    "              f\"{row['fe_roll_4h_mean']:>10.1f} \"\n",
    "              f\"{row['ir_roll_4h_mean']:>10.3f} \"\n",
    "              f\"{row['load_change_pct']:>10.2f} \"\n",
    "              f\"{row['predicted_prob']:>8.2%}\")\n",
    "\n",
    "# Compare to a normal day\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Comparison with normal day (April 21, 2025 - one week before):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "normal_data = val_pd[\n",
    "    (val_pd['country'] == 'ES') & \n",
    "    (pd.to_datetime(val_pd['timestamp']).dt.date == pd.to_datetime('2025-04-21').date())\n",
    "].copy()\n",
    "\n",
    "normal_analysis = normal_data[\n",
    "    (pd.to_datetime(normal_data['timestamp']).dt.hour >= 6) & \n",
    "    (pd.to_datetime(normal_data['timestamp']).dt.hour <= 12)\n",
    "].copy()\n",
    "\n",
    "normal_analysis['hour'] = pd.to_datetime(normal_analysis['timestamp']).dt.hour\n",
    "normal_analysis['minute'] = pd.to_datetime(normal_analysis['timestamp']).dt.minute\n",
    "normal_analysis['predicted_prob'] = xgb_model.predict_proba(normal_analysis[feature_cols])[:, 1]\n",
    "\n",
    "print(f\"{'Time':<8} {'FE%':>8} {'IR':>8} {'IR_lag1':>8} {'FE_roll':>10} {'IR_roll':>10} {'Load_chg%':>10} {'Prob':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for _, row in normal_analysis.iterrows():\n",
    "    hour = row['hour']\n",
    "    minute = row['minute']\n",
    "    if minute == 0:\n",
    "        time_str = f\"{hour:02d}:00\"\n",
    "        print(f\"{time_str:<8} \"\n",
    "              f\"{row['forecast_error_pct']:>8.2f} \"\n",
    "              f\"{row['import_ratio']:>8.3f} \"\n",
    "              f\"{row['import_ratio_lag_1']:>8.3f} \"\n",
    "              f\"{row['fe_roll_4h_mean']:>10.1f} \"\n",
    "              f\"{row['ir_roll_4h_mean']:>10.3f} \"\n",
    "              f\"{row['load_change_pct']:>10.2f} \"\n",
    "              f\"{row['predicted_prob']:>8.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "The morning of the blackout looked NORMAL in the data:\n",
    "- Forecast errors were small (good forecasts)\n",
    "- Import ratios were typical\n",
    "- No unusual patterns in the features\n",
    "\n",
    "The blackout was caused by a sudden, unexpected failure - not by\n",
    "gradually building stress that our features could detect.\n",
    "\n",
    "This is a fundamental limitation: some blackouts are unpredictable\n",
    "from grid-level data alone. They may require:\n",
    "- Generation-level data (individual plant status)\n",
    "- Real-time protection system data\n",
    "- Frequency/voltage measurements\n",
    "- External event information (weather events, accidents)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d37c841-a380-4382-9afd-3e1c41b26b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pivot the narrative - stress level prediction model that identifies elevated risk periods. When stress is high, blackout risk increases. This is still valuable for grid operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6349012-c99a-434c-8508-107f2e3e5d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reframe: Stress Level Prediction Model\n",
    "# High stress periods indicate elevated blackout risk\n",
    "\n",
    "print(\"17. STRESS LEVEL PREDICTION MODEL\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "MODEL PURPOSE: Predict grid stress levels 4 hours in advance\n",
    "OPERATIONAL VALUE: High stress periods indicate elevated blackout risk\n",
    "\n",
    "When the model predicts high stress:\n",
    "- Grid operators should increase monitoring\n",
    "- Reserve capacity should be prepared\n",
    "- Cross-border coordination should be enhanced\n",
    "\n",
    "Note: Not all high-stress periods lead to blackouts, but blackouts\n",
    "typically occur during high-stress conditions.\n",
    "\"\"\")\n",
    "\n",
    "# Create multi-level risk predictions\n",
    "def assign_risk_level(prob):\n",
    "    if prob >= 0.7:\n",
    "        return 'CRITICAL'\n",
    "    elif prob >= 0.5:\n",
    "        return 'HIGH'\n",
    "    elif prob >= 0.3:\n",
    "        return 'ELEVATED'\n",
    "    else:\n",
    "        return 'NORMAL'\n",
    "\n",
    "test_pd_copy = test_pd.copy()\n",
    "test_pd_copy['predicted_prob'] = xgb_test_prob\n",
    "test_pd_copy['risk_level'] = test_pd_copy['predicted_prob'].apply(assign_risk_level)\n",
    "\n",
    "# Risk level distribution\n",
    "print(\"RISK LEVEL DISTRIBUTION (Test Set):\")\n",
    "print(\"-\"*50)\n",
    "risk_dist = test_pd_copy['risk_level'].value_counts()\n",
    "risk_order = ['CRITICAL', 'HIGH', 'ELEVATED', 'NORMAL']\n",
    "\n",
    "for level in risk_order:\n",
    "    if level in risk_dist.index:\n",
    "        count = risk_dist[level]\n",
    "        pct = count / len(test_pd_copy) * 100\n",
    "        # Calculate actual stress rate at this level\n",
    "        level_data = test_pd_copy[test_pd_copy['risk_level'] == level]\n",
    "        actual_stress_rate = level_data[target_col].mean() * 100\n",
    "        print(f\"  {level:<12} {count:>8,} ({pct:>5.1f}%) - Actual stress rate: {actual_stress_rate:.1f}%\")\n",
    "\n",
    "# Visualize risk calibration\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"RISK CALIBRATION (Does predicted risk match actual outcomes?):\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "prob_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "test_pd_copy['prob_bin'] = pd.cut(test_pd_copy['predicted_prob'], bins=prob_bins)\n",
    "calibration = test_pd_copy.groupby('prob_bin')[target_col].agg(['mean', 'count'])\n",
    "calibration.columns = ['actual_stress_rate', 'count']\n",
    "\n",
    "print(f\"{'Predicted Prob':<18} {'Actual Rate':>12} {'Count':>10}\")\n",
    "print(\"-\"*50)\n",
    "for idx, row in calibration.iterrows():\n",
    "    print(f\"{str(idx):<18} {row['actual_stress_rate']*100:>11.1f}% {int(row['count']):>10,}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration curve\n",
    "bin_centers = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "actual_rates = calibration['actual_stress_rate'].values\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "axes[0].plot(bin_centers, actual_rates, 'bo-', markersize=10, linewidth=2, label='Model calibration')\n",
    "axes[0].fill_between(bin_centers, actual_rates, bin_centers, alpha=0.3)\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Actual Stress Rate')\n",
    "axes[0].set_title('Model Calibration Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Risk level vs actual stress rate\n",
    "risk_rates = test_pd_copy.groupby('risk_level')[target_col].mean().reindex(risk_order) * 100\n",
    "colors = ['#dc2626', '#f59e0b', '#3b82f6', '#10b981']\n",
    "bars = axes[1].bar(risk_rates.index, risk_rates.values, color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Actual Stress Rate (%)')\n",
    "axes[1].set_title('Actual Stress Rate by Predicted Risk Level')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, risk_rates.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                f'{val:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(\"-\"*50)\n",
    "print(\"- CRITICAL risk: Very high chance of stress occurring\")\n",
    "print(\"- HIGH risk: Significant chance, increase monitoring\")\n",
    "print(\"- ELEVATED risk: Some risk, be prepared\")\n",
    "print(\"- NORMAL: Low risk, standard operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d68fc8-d390-449c-93e8-09a74343033f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Country-level performance analysis\n",
    "# Some countries may be more predictable than others\n",
    "\n",
    "print(\"18. COUNTRY-LEVEL PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Calculate metrics per country\n",
    "country_metrics = []\n",
    "\n",
    "for country in test_pd['country'].unique():\n",
    "    country_data = test_pd[test_pd['country'] == country]\n",
    "    country_idx = test_pd['country'] == country\n",
    "    \n",
    "    y_true = country_data[target_col]\n",
    "    y_pred = xgb_test_pred[country_idx]\n",
    "    y_prob = xgb_test_prob[country_idx]\n",
    "    \n",
    "    if len(y_true.unique()) > 1:  # Need both classes for AUC\n",
    "        country_metrics.append({\n",
    "            'country': country,\n",
    "            'samples': len(country_data),\n",
    "            'stress_rate': y_true.mean() * 100,\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'auc': roc_auc_score(y_true, y_prob)\n",
    "        })\n",
    "\n",
    "country_df = pd.DataFrame(country_metrics).sort_values('auc', ascending=False)\n",
    "\n",
    "print(f\"{'Country':<8} {'Samples':>8} {'Stress%':>8} {'Accuracy':>9} {'Precision':>10} {'Recall':>8} {'F1':>8} {'AUC':>8}\")\n",
    "print(\"-\"*75)\n",
    "for _, row in country_df.iterrows():\n",
    "    print(f\"{row['country']:<8} {row['samples']:>8,} {row['stress_rate']:>7.1f}% \"\n",
    "          f\"{row['accuracy']:>9.3f} {row['precision']:>10.3f} {row['recall']:>8.3f} \"\n",
    "          f\"{row['f1']:>8.3f} {row['auc']:>8.3f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Best performing (by AUC):  {country_df.iloc[0]['country']} ({country_df.iloc[0]['auc']:.3f})\")\n",
    "print(f\"  Worst performing (by AUC): {country_df.iloc[-1]['country']} ({country_df.iloc[-1]['auc']:.3f})\")\n",
    "print(f\"  Average AUC across countries: {country_df['auc'].mean():.3f}\")\n",
    "\n",
    "# Visualize country performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# AUC by country\n",
    "country_df_sorted = country_df.sort_values('auc', ascending=True)\n",
    "colors = ['#dc2626' if x < 0.75 else '#f59e0b' if x < 0.85 else '#10b981' for x in country_df_sorted['auc']]\n",
    "axes[0].barh(country_df_sorted['country'], country_df_sorted['auc'], color=colors)\n",
    "axes[0].axvline(x=0.8, color='black', linestyle='--', linewidth=1, label='Good threshold')\n",
    "axes[0].set_xlabel('AUC-ROC')\n",
    "axes[0].set_title('Model Performance by Country')\n",
    "axes[0].set_xlim(0.5, 1.0)\n",
    "\n",
    "# Stress rate vs AUC\n",
    "axes[1].scatter(country_df['stress_rate'], country_df['auc'], s=100, alpha=0.7, c='steelblue')\n",
    "for _, row in country_df.iterrows():\n",
    "    axes[1].annotate(row['country'], (row['stress_rate'], row['auc']), fontsize=8)\n",
    "axes[1].set_xlabel('Stress Rate (%)')\n",
    "axes[1].set_ylabel('AUC-ROC')\n",
    "axes[1].set_title('Stress Rate vs Model Performance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd03b15-5e0a-40d4-9376-1f06e554c73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final Summary and Conclusions\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EUROPEAN GRID STRESS PREDICTION MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "PROJECT OBJECTIVE\n",
    "-----------------\n",
    "Predict power grid stress 4 hours in advance across 26 European countries\n",
    "to enable proactive grid management and reduce blackout risk.\n",
    "\n",
    "DATA SOURCES\n",
    "------------\n",
    "- ENTSOE Transparency Platform (load, forecasts, cross-border flows)\n",
    "- ERA5 Weather Data (temperature, wind, solar radiation)\n",
    "- Period: January 2023 - November 2025\n",
    "- Granularity: 15-minute intervals\n",
    "- Records: 1.48 million grid records + 1 billion weather records\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nMODEL ARCHITECTURE\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "- Algorithm: XGBoost Binary Classifier\n",
    "- Features: 51 engineered features\n",
    "  - Base Grid: 8 (load, forecast error, imports/exports)\n",
    "  - Weather: 6 (temperature, wind, solar radiation)\n",
    "  - Temporal: 8 (hour, day, month, cyclical encodings)\n",
    "  - Lag: 14 (historical values at t-1, t-4, t-16, t-96)\n",
    "  - Rolling: 10 (4h and 24h means/stds)\n",
    "  - Rate of Change: 5 (momentum indicators)\n",
    "\n",
    "- Target: stress_next_4h (will stress occur in next 4 hours?)\n",
    "- Class Balance: 42% positive (stress coming), 58% negative\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nMODEL PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\"\"\n",
    "Validation Set:\n",
    "  - AUC-ROC:   0.8219\n",
    "  - F1 Score:  0.7268\n",
    "  - Precision: 0.7220\n",
    "  - Recall:    0.7318\n",
    "\n",
    "Test Set:\n",
    "  - AUC-ROC:   0.8261\n",
    "  - F1 Score:  0.6902\n",
    "  - Precision: 0.7007\n",
    "  - Recall:    0.6800\n",
    "\n",
    "Ensemble Comparison:\n",
    "  - XGBoost (best):     AUC 0.8219\n",
    "  - Weighted Voting:    AUC 0.8206\n",
    "  - LightGBM:           AUC 0.8146\n",
    "  - Logistic Baseline:  AUC 0.6473\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRISK LEVEL CALIBRATION\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\"\"\n",
    "Risk Level        Predicted Range    Actual Stress Rate\n",
    "-----------       ---------------    ------------------\n",
    "CRITICAL          prob >= 0.7        83.0%\n",
    "HIGH              0.5 <= prob < 0.7  52.9%\n",
    "ELEVATED          0.3 <= prob < 0.5  33.5%\n",
    "NORMAL            prob < 0.3         13.7%\n",
    "\n",
    "The model is well-calibrated - predicted probabilities closely\n",
    "match actual stress occurrence rates.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTOP PREDICTIVE FEATURES\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "1. forecast_error_pct (12.1%) - Supply/demand mismatch\n",
    "2. import_ratio (6.7%)        - Cross-border dependency\n",
    "3. import_ratio_lag_1 (4.8%)  - Recent dependency trend\n",
    "4. forecast_error (3.9%)      - Absolute mismatch\n",
    "5. load_roll_24h_mean (3.2%)  - Daily load context\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nCOUNTRY PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\"\"\n",
    "Best Performing:  Netherlands (AUC: 0.942), Finland (0.868), Norway (0.866)\n",
    "Worst Performing: Portugal (AUC: 0.543), Switzerland (0.592), Denmark (0.653)\n",
    "\n",
    "Average AUC: 0.766\n",
    "Countries with AUC > 0.8: 14 out of 26\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nLIMITATIONS\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "1. Cannot predict sudden equipment failures or cascading blackouts\n",
    "   - April 28, 2025 Spain/Portugal blackout: Model detected AFTER event\n",
    "   - Pre-blackout features looked normal\n",
    "   \n",
    "2. Performance varies by country\n",
    "   - Works best for countries with stable patterns (NL, FI, NO)\n",
    "   - Struggles with volatile grids (PT, CH, SE)\n",
    "\n",
    "3. Requires real-time data feed for operational use\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nOPERATIONAL RECOMMENDATIONS\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "1. Deploy with 4-hour prediction window for proactive monitoring\n",
    "2. Use risk level thresholds:\n",
    "   - CRITICAL: Immediate attention, activate reserves\n",
    "   - HIGH: Increase monitoring frequency\n",
    "   - ELEVATED: Standard heightened awareness\n",
    "   - NORMAL: Routine operations\n",
    "\n",
    "3. Country-specific calibration recommended for PT, CH, SE\n",
    "4. Integrate with generation and frequency data for improved accuracy\n",
    "5. Update model quarterly with new data\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL READY FOR DEPLOYMENT\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11c4abc-dd5a-48ef-a583-fccbd5fe2938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save model and files for Streamlit deployment\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Save to workspace\n",
    "output_dir = \"/Workspace/Users/peter.ducati@gmail.com/European_Grid_Stress_Prediction_Model\"\n",
    "streamlit_dir = f\"{output_dir}/streamlit_app\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(streamlit_dir, exist_ok=True)\n",
    "\n",
    "print(\"19. SAVING MODEL AND FILES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# 1. Save XGBoost model\n",
    "with open(f\"{output_dir}/xgboost_model.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_model,\n",
    "        'feature_cols': feature_cols,\n",
    "        'threshold': 0.5\n",
    "    }, f)\n",
    "print(\"1. Saved: xgboost_model.pkl\")\n",
    "\n",
    "# 2. Save feature configuration\n",
    "feature_config = {\n",
    "    'features': feature_cols,\n",
    "    'feature_count': len(feature_cols),\n",
    "    'feature_categories': {\n",
    "        'base_grid': ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "                      'total_imports', 'total_exports', 'net_imports', 'import_ratio'],\n",
    "        'weather': ['temp_avg', 'temp_min', 'temp_max', 'wind_avg', 'wind_max', 'solar_radiation_avg'],\n",
    "        'temporal': ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
    "        'lag': [f for f in feature_cols if 'lag' in f],\n",
    "        'rolling': [f for f in feature_cols if 'roll' in f],\n",
    "        'rate_of_change': ['fe_change_1h', 'load_change_1h', 'load_change_pct', 'temp_change_24h', 'fe_vs_roll_4h']\n",
    "    },\n",
    "    'countries': sorted(test_pd['country'].unique().tolist()),\n",
    "    'country_names': {\n",
    "        'AT': 'Austria', 'BE': 'Belgium', 'BG': 'Bulgaria', 'CH': 'Switzerland',\n",
    "        'CZ': 'Czech Republic', 'DE': 'Germany', 'DK': 'Denmark', 'EE': 'Estonia',\n",
    "        'ES': 'Spain', 'FI': 'Finland', 'FR': 'France', 'GR': 'Greece',\n",
    "        'HR': 'Croatia', 'HU': 'Hungary', 'IE': 'Ireland', 'IT': 'Italy',\n",
    "        'LT': 'Lithuania', 'LV': 'Latvia', 'NL': 'Netherlands', 'NO': 'Norway',\n",
    "        'PL': 'Poland', 'PT': 'Portugal', 'RO': 'Romania', 'SE': 'Sweden',\n",
    "        'SI': 'Slovenia', 'SK': 'Slovakia'\n",
    "    }\n",
    "}\n",
    "with open(f\"{output_dir}/feature_config.json\", 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(\"2. Saved: feature_config.json\")\n",
    "\n",
    "# 3. Save performance metrics\n",
    "performance = {\n",
    "    'validation': {\n",
    "        'auc': 0.8219, 'f1': 0.7268, 'precision': 0.7220, 'recall': 0.7318, 'accuracy': 0.7390\n",
    "    },\n",
    "    'test': {\n",
    "        'auc': 0.8261, 'f1': 0.6902, 'precision': 0.7007, 'recall': 0.6800, 'accuracy': 0.7567\n",
    "    },\n",
    "    'risk_calibration': {\n",
    "        'CRITICAL': {'threshold': 0.7, 'actual_stress_rate': 0.83},\n",
    "        'HIGH': {'threshold': 0.5, 'actual_stress_rate': 0.53},\n",
    "        'ELEVATED': {'threshold': 0.3, 'actual_stress_rate': 0.34},\n",
    "        'NORMAL': {'threshold': 0.0, 'actual_stress_rate': 0.14}\n",
    "    },\n",
    "    'country_auc': country_df.set_index('country')['auc'].to_dict()\n",
    "}\n",
    "with open(f\"{output_dir}/performance_metrics.json\", 'w') as f:\n",
    "    json.dump(performance, f, indent=2)\n",
    "print(\"3. Saved: performance_metrics.json\")\n",
    "\n",
    "# 4. Save sample data for Streamlit\n",
    "sample_data = test_pd.copy()\n",
    "sample_data['predicted_prob'] = xgb_test_prob\n",
    "sample_data['risk_level'] = sample_data['predicted_prob'].apply(assign_risk_level)\n",
    "sample_data.to_csv(f\"{streamlit_dir}/prediction_data.csv\", index=False)\n",
    "print(\"4. Saved: streamlit_app/prediction_data.csv\")\n",
    "\n",
    "# 5. Copy model to streamlit folder\n",
    "with open(f\"{streamlit_dir}/model.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_model,\n",
    "        'feature_cols': feature_cols,\n",
    "        'threshold': 0.5\n",
    "    }, f)\n",
    "print(\"5. Saved: streamlit_app/model.pkl\")\n",
    "\n",
    "# 6. Save requirements.txt\n",
    "requirements = \"\"\"streamlit>=1.28.0\n",
    "pandas>=2.0.0\n",
    "numpy>=1.24.0\n",
    "plotly>=5.18.0\n",
    "xgboost>=2.0.0\n",
    "scikit-learn>=1.3.0\n",
    "\"\"\"\n",
    "with open(f\"{streamlit_dir}/requirements.txt\", 'w') as f:\n",
    "    f.write(requirements)\n",
    "print(\"6. Saved: streamlit_app/requirements.txt\")\n",
    "\n",
    "# 7. Save Streamlit app\n",
    "streamlit_code = '''import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "st.set_page_config(page_title=\"Grid Stress Predictor\", page_icon=\"\", layout=\"wide\")\n",
    "\n",
    "COUNTRY_NAMES = {\n",
    "    'AT': 'Austria', 'BE': 'Belgium', 'BG': 'Bulgaria', 'CH': 'Switzerland',\n",
    "    'CZ': 'Czech Republic', 'DE': 'Germany', 'DK': 'Denmark', 'EE': 'Estonia',\n",
    "    'ES': 'Spain', 'FI': 'Finland', 'FR': 'France', 'GR': 'Greece',\n",
    "    'HR': 'Croatia', 'HU': 'Hungary', 'IE': 'Ireland', 'IT': 'Italy',\n",
    "    'LT': 'Lithuania', 'LV': 'Latvia', 'NL': 'Netherlands', 'NO': 'Norway',\n",
    "    'PL': 'Poland', 'PT': 'Portugal', 'RO': 'Romania', 'SE': 'Sweden',\n",
    "    'SI': 'Slovenia', 'SK': 'Slovakia'\n",
    "}\n",
    "\n",
    "RISK_COLORS = {'CRITICAL': '#dc2626', 'HIGH': '#f59e0b', 'ELEVATED': '#3b82f6', 'NORMAL': '#10b981'}\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    return pd.read_csv('prediction_data.csv')\n",
    "\n",
    "def main():\n",
    "    st.title(\"European Grid Stress Prediction\")\n",
    "    st.markdown(\"4-hour ahead stress prediction across 26 European countries\")\n",
    "    \n",
    "    df = load_data()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.header(\"Filters\")\n",
    "    countries = st.sidebar.multiselect(\"Countries\", sorted(df['country'].unique()), \n",
    "                                        default=['DE', 'FR', 'ES', 'IT'])\n",
    "    \n",
    "    # Filter data\n",
    "    filtered = df[df['country'].isin(countries)] if countries else df\n",
    "    \n",
    "    # Current risk overview\n",
    "    st.header(\"Current Risk Overview\")\n",
    "    cols = st.columns(4)\n",
    "    for i, level in enumerate(['CRITICAL', 'HIGH', 'ELEVATED', 'NORMAL']):\n",
    "        count = len(filtered[filtered['risk_level'] == level])\n",
    "        pct = count / len(filtered) * 100 if len(filtered) > 0 else 0\n",
    "        cols[i].metric(level, f\"{pct:.1f}%\", delta=None)\n",
    "    \n",
    "    # Risk timeline\n",
    "    st.header(\"Risk Timeline\")\n",
    "    timeline = filtered.groupby(['timestamp', 'risk_level']).size().unstack(fill_value=0)\n",
    "    fig = px.area(timeline, title=\"Risk Level Distribution Over Time\")\n",
    "    fig.update_layout(template=\"plotly_dark\", height=400)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Country comparison\n",
    "    st.header(\"Country Risk Comparison\")\n",
    "    country_risk = filtered.groupby('country')['predicted_prob'].mean().sort_values(ascending=False)\n",
    "    fig2 = px.bar(x=country_risk.index, y=country_risk.values, \n",
    "                  labels={'x': 'Country', 'y': 'Avg Risk Probability'},\n",
    "                  title=\"Average Stress Probability by Country\")\n",
    "    fig2.update_layout(template=\"plotly_dark\")\n",
    "    st.plotly_chart(fig2, use_container_width=True)\n",
    "    \n",
    "    # Alerts table\n",
    "    st.header(\"Current Alerts\")\n",
    "    alerts = filtered[filtered['risk_level'].isin(['CRITICAL', 'HIGH'])].sort_values('predicted_prob', ascending=False)\n",
    "    if len(alerts) > 0:\n",
    "        st.dataframe(alerts[['timestamp', 'country', 'actual_load', 'predicted_prob', 'risk_level']].head(20))\n",
    "    else:\n",
    "        st.success(\"No active alerts\")\n",
    "    \n",
    "    # Model info\n",
    "    with st.expander(\"Model Information\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **Model**: XGBoost Classifier (51 features)\n",
    "        \n",
    "        **Performance**: AUC 0.826, F1 0.69\n",
    "        \n",
    "        **Risk Calibration**:\n",
    "        - CRITICAL (prob >= 0.7): 83% actual stress rate\n",
    "        - HIGH (0.5-0.7): 53% actual stress rate\n",
    "        - ELEVATED (0.3-0.5): 34% actual stress rate\n",
    "        - NORMAL (< 0.3): 14% actual stress rate\n",
    "        \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open(f\"{streamlit_dir}/app.py\", 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "print(\"7. Saved: streamlit_app/app.py\")\n",
    "\n",
    "# 8. Save README\n",
    "readme = \"\"\"# European Grid Stress Prediction Model\n",
    "\n",
    "## Overview\n",
    "Predicts power grid stress 4 hours in advance across 26 European countries.\n",
    "\n",
    "## Performance\n",
    "- AUC-ROC: 0.826\n",
    "- F1 Score: 0.69\n",
    "- Well-calibrated risk levels (CRITICAL = 83% actual stress rate)\n",
    "\n",
    "## Files\n",
    "- xgboost_model.pkl: Trained model\n",
    "- feature_config.json: Feature definitions\n",
    "- performance_metrics.json: Model metrics\n",
    "- streamlit_app/: Dashboard application\n",
    "\n",
    "## Run Streamlit\n",
    "```bash\n",
    "cd streamlit_app\n",
    "pip install -r requirements.txt\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "## Risk Levels\n",
    "- CRITICAL (prob >= 0.7): Immediate attention required\n",
    "- HIGH (0.5-0.7): Increase monitoring\n",
    "- ELEVATED (0.3-0.5): Heightened awareness\n",
    "- NORMAL (< 0.3): Routine operations\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{output_dir}/README.md\", 'w') as f:\n",
    "    f.write(readme)\n",
    "print(\"8. Saved: README.md\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FILES SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLocation: {output_dir}\")\n",
    "print(\"\"\"\n",
    "Files:\n",
    "  - xgboost_model.pkl\n",
    "  - feature_config.json\n",
    "  - performance_metrics.json\n",
    "  - README.md\n",
    "  - streamlit_app/\n",
    "      - app.py\n",
    "      - model.pkl\n",
    "      - prediction_data.csv\n",
    "      - requirements.txt\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "European_Grid_Stress_Prediction_Model_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
