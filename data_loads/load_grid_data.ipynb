{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294503bb-14ea-4870-981c-295274b701b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBAL CONFIG VARIABLES (REPLACING THE OLD Config CLASS)\n",
    "# ============================================================================\n",
    "\n",
    "API_KEY = '7b785108-53d7-42f8-931e-3d28c4323c68'\n",
    "\n",
    "COUNTRIES = {\n",
    "    'ES': 'Spain', 'PT': 'Portugal', 'FR': 'France', 'DE': 'Germany',\n",
    "    'IT': 'Italy', 'GB': 'Great Britain', 'NL': 'Netherlands',\n",
    "    'BE': 'Belgium', 'AT': 'Austria', 'CH': 'Switzerland', 'PL': 'Poland',\n",
    "    'CZ': 'Czechia', 'DK': 'Denmark', 'SE': 'Sweden', 'NO': 'Norway',\n",
    "    'FI': 'Finland', 'GR': 'Greece', 'IE': 'Ireland', 'RO': 'Romania',\n",
    "    'BG': 'Bulgaria', 'HU': 'Hungary', 'SK': 'Slovakia', 'SI': 'Slovenia',\n",
    "    'HR': 'Croatia', 'EE': 'Estonia', 'LT': 'Lithuania', 'LV': 'Latvia'\n",
    "}\n",
    "\n",
    "# YOU REQUESTED THIS EXACT BLOCK KEPT UNCHANGED\n",
    "VALID_BORDERS = {\n",
    "    ('ES', 'PT'), ('ES', 'FR'),\n",
    "    ('FR', 'BE'), ('FR', 'CH'), ('FR', 'DE'), ('FR', 'IT'),\n",
    "    ('BE', 'NL'), ('BE', 'DE'),\n",
    "    ('NL', 'DE'), ('NL', 'GB'),\n",
    "    ('GB', 'NL'), ('GB', 'FR'), ('GB', 'IE'),\n",
    "    ('DE', 'CZ'), ('DE', 'PL'), ('DE', 'CH'), ('DE', 'DK'), ('DE', 'AT'),\n",
    "    ('DK', 'DE'), ('DK', 'NO'), ('DK', 'SE'),\n",
    "    ('SE', 'NO'), ('SE', 'FI'), ('SE', 'DK'),\n",
    "    ('NO', 'NL'), ('NO', 'GB'), ('NO', 'SE'), ('NO', 'DK'),\n",
    "    ('FI', 'EE'), ('FI', 'SE'),\n",
    "    ('EE', 'LV'),\n",
    "    ('LV', 'LT'),\n",
    "    ('LT', 'PL'),\n",
    "    ('PL', 'SK'), ('PL', 'CZ'),\n",
    "    ('CZ', 'AT'), ('CZ', 'SK'),\n",
    "    ('AT', 'SI'), ('AT', 'IT'), ('AT', 'CH'), ('AT', 'CZ'), ('AT', 'DE'),\n",
    "    ('SI', 'HR'), ('SI', 'IT'), ('SI', 'AT'),\n",
    "    ('HR', 'HU'), ('HR', 'SI'),\n",
    "    ('HU', 'SK'), ('HU', 'RO'), ('HU', 'HR'), ('HU', 'AT'),\n",
    "    ('SK', 'HU'), ('SK', 'CZ'), ('SK', 'PL'),\n",
    "    ('RO', 'BG'), ('RO', 'HU'),\n",
    "    ('BG', 'GR'), ('BG', 'RO'),\n",
    "    ('GR', 'BG')\n",
    "}\n",
    "\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE   = '2025-10-31'\n",
    "\n",
    "# Databricks Delta database name\n",
    "DATABASE = \"european_grid_raw\"\n",
    "\n",
    "DATASETS = [\n",
    "    \"load_actual\",\n",
    "    \"load_forecast\",\n",
    "    \"generation\",\n",
    "    \"wind_forecast\",\n",
    "    \"solar_forecast\",\n",
    "    \"installed_capacity\",\n",
    "    \"crossborder_flows\",\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# COLUMN SANITIZER\n",
    "# ============================================================================\n",
    "\n",
    "INVALID_CHARS_PATTERN = re.compile(r\"[^0-9A-Za-z_]+\")\n",
    "\n",
    "def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten MultiIndex columns and remove characters invalid for Delta.\n",
    "    - MultiIndex ('Biomass', 'Actual Aggregated') -> 'Biomass__Actual_Aggregated'\n",
    "    - Replace any non [0-9A-Za-z_] chars with '_'\n",
    "    - If name starts with a digit, prefix with '_'\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        # Flatten MultiIndex tuples\n",
    "        if isinstance(col, tuple):\n",
    "            parts = [str(p) for p in col if p is not None and str(p) != \"\"]\n",
    "            name = \"__\".join(parts) if parts else \"col\"\n",
    "        else:\n",
    "            name = str(col)\n",
    "\n",
    "        name = name.strip().replace(\" \", \"_\")\n",
    "\n",
    "        # Replace any remaining bad chars (., /, -, quotes, parens, etc.)\n",
    "        name = INVALID_CHARS_PATTERN.sub(\"_\", name)\n",
    "\n",
    "        # Avoid starting with a digit\n",
    "        if name and name[0].isdigit():\n",
    "            name = \"_\" + name\n",
    "\n",
    "        new_cols.append(name)\n",
    "\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def truncate_table(dataset_name: str):\n",
    "    \"\"\"\n",
    "    TRUNCATE the Delta table if it exists.\n",
    "    Keeps schema, just removes all rows.\n",
    "    \"\"\"\n",
    "    full_name = f\"{DATABASE}.{dataset_name}\"\n",
    "    if spark.catalog.tableExists(full_name):\n",
    "        print(f\"  → Truncating table {full_name}\")\n",
    "        spark.sql(f\"TRUNCATE TABLE {full_name}\")\n",
    "    else:\n",
    "        print(f\"  → Table {full_name} does not exist yet (will be created on write)\")\n",
    "\n",
    "# ============================================================================\n",
    "# DELTA WRITER\n",
    "# ============================================================================\n",
    "\n",
    "def write_dataset(dataset_name: str, country_code: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Write a single country dataset into a shared Delta table.\n",
    "\n",
    "    - Table name: european_grid_raw.<dataset_name>\n",
    "      e.g. european_grid_raw.load_actual\n",
    "    - Adds column:\n",
    "        * country  (2-letter code, e.g. 'DE')\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return\n",
    "\n",
    "    # bring index into a column (e.g. time) and add country\n",
    "    df = df.reset_index()\n",
    "    df[\"country\"] = country_code\n",
    "\n",
    "    # sanitize columns (handles MultiIndex + bad characters)\n",
    "    df = sanitize_columns(df)\n",
    "\n",
    "    table_name = dataset_name\n",
    "    full_name = f\"{DATABASE}.{table_name}\"\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    (\n",
    "        spark_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        # optional but recommended for performance:\n",
    "        # .partitionBy(\"country\")\n",
    "        .saveAsTable(full_name)\n",
    "    )\n",
    "\n",
    "    print(f\"  → Saved to Delta: {full_name} ({spark_df.count()} rows)\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLECTOR\n",
    "# ============================================================================\n",
    "\n",
    "class EuropeanGridDataCollector:\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        self.client = EntsoePandasClient(api_key=api_key)\n",
    "        self.countries = COUNTRIES\n",
    "\n",
    "        self.start = pd.Timestamp(START_DATE, tz=\"UTC\")\n",
    "        self.end   = pd.Timestamp(END_DATE,   tz=\"UTC\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # SINGLE COUNTRY DATA\n",
    "    # -------------------------------\n",
    "    def collect_country_data(self, country_code):\n",
    "        c = country_code\n",
    "        print(f\"\\n==== Collecting for {c} ({self.countries[c]}) ====\")\n",
    "\n",
    "        try:\n",
    "            print(f\"    → load_actual...\")\n",
    "            df = self.client.query_load(c, start=self.start, end=self.end)\n",
    "            write_dataset(\"load_actual\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ load_actual: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → load_forecast...\")\n",
    "            df = self.client.query_load_forecast(c, start=self.start, end=self.end)\n",
    "            write_dataset(\"load_forecast\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ load_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → generation...\")\n",
    "            df = self.client.query_generation(c, start=self.start, end=self.end)\n",
    "            write_dataset(\"generation\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ generation: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → wind_forecast...\")\n",
    "            df = self.client.query_wind_and_solar_forecast(\n",
    "                c, start=self.start, end=self.end, psr_type='B19'\n",
    "            )\n",
    "            write_dataset(\"wind_forecast\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ wind_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → solar_forecast...\")\n",
    "            df = self.client.query_wind_and_solar_forecast(\n",
    "                c, start=self.start, end=self.end, psr_type='B16'\n",
    "            )\n",
    "            write_dataset(\"solar_forecast\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ solar_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → installed_capacity...\")\n",
    "            df = self.client.query_installed_generation_capacity(\n",
    "                c, start=self.start, end=self.end\n",
    "            )\n",
    "            write_dataset(\"installed_capacity\", c, df)\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ installed_capacity: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # CROSS-BORDER FLOWS\n",
    "    # -------------------------------\n",
    "    def collect_crossborder_flows(self):\n",
    "        print(\"\\n=== Collecting Cross-Border Flows ===\")\n",
    "\n",
    "        flows_list = []\n",
    "\n",
    "        for from_c, to_c in VALID_BORDERS:\n",
    "            print(f\"  → {from_c} ↔ {to_c}...\", end=\"\")\n",
    "\n",
    "            try:\n",
    "                flow = self.client.query_crossborder_flows(\n",
    "                    from_c, to_c, start=self.start, end=self.end\n",
    "                )\n",
    "                if flow is not None and len(flow) > 0:\n",
    "                    df = pd.DataFrame(flow)\n",
    "                    df[\"from_country\"] = from_c\n",
    "                    df[\"to_country\"]   = to_c\n",
    "                    flows_list.append(df)\n",
    "                    print(\" ✓\")\n",
    "                else:\n",
    "                    print(\" ✗ No data\")\n",
    "            except Exception:\n",
    "                print(\" ✗ Failed\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if flows_list:\n",
    "            df_all = pd.concat(flows_list, ignore_index=True)\n",
    "            df_all = sanitize_columns(df_all)\n",
    "\n",
    "            table_name = \"crossborder_flows\"\n",
    "            full_name = f\"{DATABASE}.{table_name}\"\n",
    "\n",
    "            spark_df = spark.createDataFrame(df_all)\n",
    "\n",
    "            (\n",
    "                spark_df.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .saveAsTable(full_name)\n",
    "            )\n",
    "\n",
    "            print(f\"  → Saved cross-border flows table {full_name} ({spark_df.count()} rows)\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # MAIN COLLECTOR\n",
    "    # -------------------------------\n",
    "    def collect_all(self):\n",
    "        print(\"=== Truncating tables for fresh run ===\")\n",
    "        for ds in DATASETS:\n",
    "            truncate_table(ds)\n",
    "\n",
    "        for c in self.countries.keys():\n",
    "            self.collect_country_data(c)\n",
    "\n",
    "        self.collect_crossborder_flows()\n",
    "\n",
    "# ============================================================================\n",
    "# RUN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "collector = EuropeanGridDataCollector(api_key=API_KEY)\n",
    "collector.collect_all()\n",
    "print(\"\\nCOMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/chavely.albert@gmail.com/energy-grid-load-prediction/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_grid_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
