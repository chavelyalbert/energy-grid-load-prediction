{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbecf0c1-f869-4450-a991-35e959bf8a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Daily ERA5 Weather Data Pipeline (GRIB ‚Üí Delta, Databricks-safe, sequential)\n",
    "\n",
    "- REQUIRED: a date range MIN_DATE ‚Üí MAX_DATE (inclusive)\n",
    "- Downloads *daily* ERA5 single-level data (hourly) for full Europe bounding box\n",
    "- Variables: 10m_u_component_of_wind, 10m_v_component_of_wind,\n",
    "             2m_temperature, surface_solar_radiation_downwards\n",
    "- One GRIB file per day.\n",
    "- For each day:\n",
    "    * Download GRIB\n",
    "    * Parse with pygrib ‚Üí Pandas DataFrame\n",
    "    * Add load_id + load_ts + source_file\n",
    "    * Write immediately into Delta table (partition overwrite by day)\n",
    "    * Delete GRIB file\n",
    "- Delta table: workspace.european_weather_raw.weather_hourly\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygrib\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class WeatherConfig:\n",
    "    # Copernicus CDS API (in real life: use secrets / env vars!)\n",
    "    CDS_API_URL = \"https://cds.climate.copernicus.eu/api\"\n",
    "    CDS_API_KEY = \"b2efa6a6-1215-42b6-9a47-2282344a0c23\"  # TODO: move to secret/env\n",
    "\n",
    "    # Europe bounding box [North, West, South, East]\n",
    "    EUROPE_BBOX = [72, -25, 35, 45]\n",
    "\n",
    "    # REQUIRED date range (inclusive)\n",
    "    MIN_DATE = \"2023-06-12\"      # restart from failed date\n",
    "    MAX_DATE = \"2025-11-08\"      # adjust if you want to limit to a shorter window\n",
    "\n",
    "    # ERA5 dataset & variables\n",
    "    DATASET = \"reanalysis-era5-single-levels\"\n",
    "    VARIABLES = [\n",
    "        \"10m_u_component_of_wind\",            # -> shortName 10u\n",
    "        \"10m_v_component_of_wind\",            # -> shortName 10v\n",
    "        \"2m_temperature\",                     # -> shortName 2t\n",
    "        \"surface_solar_radiation_downwards\",  # -> shortName ssrd\n",
    "    ]\n",
    "\n",
    "    # Where to store temporary GRIB files (local/DBFS path)\n",
    "    OUTPUT_DIR = \"data/weather_grib_daily\"\n",
    "\n",
    "    # Delta target (Unity Catalog)\n",
    "    CATALOG = \"workspace\"\n",
    "    TARGET_DB = \"european_weather_raw\"\n",
    "    TARGET_TABLE = f\"{CATALOG}.{TARGET_DB}.weather_hourly\"\n",
    "\n",
    "    # Full reload toggle: if True, drop the table at the beginning\n",
    "    FULL_RELOAD: bool = False\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATE HELPERS\n",
    "# ============================================================================\n",
    "\n",
    "def iter_dates():\n",
    "    \"\"\"Return list of dates to process, based on REQUIRED MIN_DATE/MAX_DATE.\"\"\"\n",
    "    if not WeatherConfig.MIN_DATE or not WeatherConfig.MAX_DATE:\n",
    "        raise RuntimeError(\"WeatherConfig.MIN_DATE and MAX_DATE must be set.\")\n",
    "\n",
    "    start = date.fromisoformat(WeatherConfig.MIN_DATE)\n",
    "    end = date.fromisoformat(WeatherConfig.MAX_DATE)\n",
    "    if end < start:\n",
    "        raise RuntimeError(\"MAX_DATE must be >= MIN_DATE.\")\n",
    "\n",
    "    current = start\n",
    "    days = []\n",
    "    while current <= end:\n",
    "        days.append(current)\n",
    "        current += timedelta(days=1)\n",
    "    return days\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SPARK SESSION HELPER\n",
    "# ============================================================================\n",
    "\n",
    "def init_spark_session() -> SparkSession:\n",
    "    \"\"\"Create or get SparkSession and ensure catalog/schema are set.\"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {WeatherConfig.CATALOG}\")\n",
    "    spark.sql(\n",
    "        f\"CREATE SCHEMA IF NOT EXISTS {WeatherConfig.CATALOG}.{WeatherConfig.TARGET_DB}\"\n",
    "    )\n",
    "    spark.sql(f\"USE CATALOG {WeatherConfig.CATALOG}\")\n",
    "    spark.sql(f\"USE {WeatherConfig.TARGET_DB}\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DOWNLOAD\n",
    "# ============================================================================\n",
    "\n",
    "def build_target_path(day: date) -> str:\n",
    "    \"\"\"Local/DBFS file path for a given day.\"\"\"\n",
    "    os.makedirs(WeatherConfig.OUTPUT_DIR, exist_ok=True)\n",
    "    return os.path.join(\n",
    "        WeatherConfig.OUTPUT_DIR,\n",
    "        f\"era5_europe_all_{day.strftime('%Y%m%d')}.grib\",\n",
    "    )\n",
    "\n",
    "\n",
    "def download_one_day(day: date) -> str | None:\n",
    "    \"\"\"Download one day of ERA5 data as GRIB. Returns local path or None.\"\"\"\n",
    "    target = build_target_path(day)\n",
    "\n",
    "    # Remove any existing partial file\n",
    "    if os.path.exists(target):\n",
    "        try:\n",
    "            os.remove(target)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    abs_target = os.path.abspath(target)\n",
    "    print(f\"\\nüì• Downloading ERA5 for {day.isoformat()} ‚Üí {abs_target}\")\n",
    "\n",
    "    client = cdsapi.Client(\n",
    "        url=WeatherConfig.CDS_API_URL,\n",
    "        key=WeatherConfig.CDS_API_KEY,\n",
    "        quiet=True,\n",
    "    )\n",
    "\n",
    "    req = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": WeatherConfig.VARIABLES,\n",
    "        \"year\": f\"{day.year:04d}\",\n",
    "        \"month\": f\"{day.month:02d}\",\n",
    "        \"day\": f\"{day.day:02d}\",\n",
    "        \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "        \"area\": WeatherConfig.EUROPE_BBOX,\n",
    "        \"format\": \"grib\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        client.retrieve(WeatherConfig.DATASET, req, target)\n",
    "        print(f\"   ‚úì Downloaded {day.isoformat()} ‚Üí {abs_target}\")\n",
    "        return target\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Failed to download {day.isoformat()}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GRIB ‚Üí PANDAS\n",
    "# ============================================================================\n",
    "\n",
    "def grib_day_to_dataframe(grib_path: str, load_id: str, load_ts: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a GRIB file for a single day into a flat DataFrame.\n",
    "\n",
    "    Columns:\n",
    "      timestamp, lat, lon, u10, v10, t2m, ssrd,\n",
    "      wind_speed, temperature_c,\n",
    "      year, month, day, load_id, load_ts, source_file\n",
    "    \"\"\"\n",
    "    abs_path = os.path.abspath(grib_path)\n",
    "    source_file = os.path.basename(grib_path)\n",
    "    print(f\"üîç Parsing GRIB file: {abs_path}\")\n",
    "\n",
    "    # Parse date from filename (era5_europe_all_YYYYMMDD.grib)\n",
    "    token = source_file.replace(\"era5_europe_all_\", \"\").replace(\".grib\", \"\")\n",
    "    year = int(token[0:4])\n",
    "    month = int(token[4:6])\n",
    "    day = int(token[6:8])\n",
    "    day_date = date(year, month, day)\n",
    "\n",
    "    with pygrib.open(grib_path) as grbs:\n",
    "        messages = list(grbs)\n",
    "\n",
    "    if not messages:\n",
    "        raise ValueError(f\"No GRIB messages found in {grib_path}\")\n",
    "\n",
    "    sample = messages[0]\n",
    "    lats, lons = sample.latlons()\n",
    "    n_lat, n_lon = lats.shape\n",
    "    n_points = n_lat * n_lon\n",
    "\n",
    "    timestamps_list = sorted({g.validDate for g in messages})\n",
    "    n_times = len(timestamps_list)\n",
    "\n",
    "    print(\n",
    "        f\"   üóìÔ∏è Date: {day_date.isoformat()}, \"\n",
    "        f\"timestamps: {n_times}, grid: {n_lat}x{n_lon} ‚Üí {n_points:,} points/time\"\n",
    "    )\n",
    "    print(f\"   üìÑ source_file = {source_file}\")\n",
    "\n",
    "    # Map timestamp ‚Üí index\n",
    "    ts_index = {ts: i for i, ts in enumerate(timestamps_list)}\n",
    "\n",
    "    # Pre-allocate arrays (float32 for memory)\n",
    "    arr_u10 = np.full((n_times, n_points), np.nan, dtype=np.float32)\n",
    "    arr_v10 = np.full((n_times, n_points), np.nan, dtype=np.float32)\n",
    "    arr_t2m = np.full((n_times, n_points), np.nan, dtype=np.float32)\n",
    "    arr_ssrd = np.full((n_times, n_points), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Fill arrays according to shortName\n",
    "    for msg in messages:\n",
    "        ts = msg.validDate\n",
    "        idx = ts_index[ts]\n",
    "        vals = msg.values.astype(np.float32).reshape(-1)\n",
    "        short = msg.shortName\n",
    "\n",
    "        if short == \"10u\":\n",
    "            arr_u10[idx, :] = vals\n",
    "        elif short == \"10v\":\n",
    "            arr_v10[idx, :] = vals\n",
    "        elif short == \"2t\":\n",
    "            arr_t2m[idx, :] = vals\n",
    "        elif short == \"ssrd\":\n",
    "            arr_ssrd[idx, :] = vals\n",
    "\n",
    "    # Flatten time √ó grid\n",
    "    u10_flat = arr_u10.reshape(-1)\n",
    "    v10_flat = arr_v10.reshape(-1)\n",
    "    t2m_flat = arr_t2m.reshape(-1)\n",
    "    ssrd_flat = arr_ssrd.reshape(-1)\n",
    "\n",
    "    # Derived variables\n",
    "    wind_speed = np.sqrt(u10_flat ** 2 + v10_flat ** 2)\n",
    "    temperature_c = t2m_flat - 273.15\n",
    "\n",
    "    # Timestamps repeated per grid point\n",
    "    ts_array = np.array(timestamps_list, dtype=\"datetime64[ns]\")\n",
    "    ts_repeated = np.repeat(ts_array, n_points)\n",
    "\n",
    "    # Lat/lon tiled over time\n",
    "    lats_flat = lats.astype(np.float32).ravel()\n",
    "    lons_flat = lons.astype(np.float32).ravel()\n",
    "    lat_tiled = np.tile(lats_flat, n_times)\n",
    "    lon_tiled = np.tile(lons_flat, n_times)\n",
    "\n",
    "    n_rows = len(ts_repeated)\n",
    "    print(f\"   Building DataFrame: rows={n_rows:,}\")\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": ts_repeated,\n",
    "            \"lat\": lat_tiled,\n",
    "            \"lon\": lon_tiled,\n",
    "            \"u10\": u10_flat,\n",
    "            \"v10\": v10_flat,\n",
    "            \"t2m\": t2m_flat,\n",
    "            \"ssrd\": ssrd_flat,\n",
    "            \"wind_speed\": wind_speed,\n",
    "            \"temperature_c\": temperature_c,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Partition & load metadata\n",
    "    df[\"year\"] = year\n",
    "    df[\"month\"] = month\n",
    "    df[\"day\"] = day\n",
    "    df[\"load_id\"] = load_id\n",
    "    df[\"load_ts\"] = load_ts\n",
    "    df[\"source_file\"] = source_file\n",
    "\n",
    "    print(\n",
    "        \"   ‚úÖ DataFrame ready: \"\n",
    "        f\"shape={df.shape}, \"\n",
    "        f\"timestamp range=[{df['timestamp'].min()} ‚Ä¶ {df['timestamp'].max()}]\"\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DELTA WRITE (PARTITION OVERWRITE PER DAY)\n",
    "# ============================================================================\n",
    "\n",
    "def write_df_to_delta(spark: SparkSession, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Overwrite the partition for (year, month, day) using Delta + replaceWhere.\n",
    "    This makes the pipeline idempotent per day (no duplicates if re-run).\n",
    "    \"\"\"\n",
    "\n",
    "    year = int(df[\"year\"].iloc[0])\n",
    "    month = int(df[\"month\"].iloc[0])\n",
    "    day = int(df[\"day\"].iloc[0])\n",
    "\n",
    "    condition = f\"year = {year} AND month = {month} AND day = {day}\"\n",
    "\n",
    "    print(\n",
    "        f\"   üíæ Writing to {WeatherConfig.TARGET_TABLE} \"\n",
    "        f\"(overwrite partition {year}-{month:02d}-{day:02d})\"\n",
    "    )\n",
    "    print(f\"   üîé Rows to write: {len(df):,}\")\n",
    "    print(f\"   üîé Columns: {list(df.columns)}\")\n",
    "\n",
    "    sdf = spark.createDataFrame(df)\n",
    "\n",
    "    (\n",
    "        sdf.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", condition)\n",
    "        .saveAsTable(WeatherConfig.TARGET_TABLE)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"   ‚úÖ Partition overwrite complete for {year}-{month:02d}-{day:02d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüå§Ô∏è  EUROPEAN WEATHER DATA PIPELINE (GRIB ‚Üí DELTA, DAILY, SEQUENTIAL) üå§Ô∏è\\n\")\n",
    "    print(\"üì¶ Required Python packages on the cluster:\")\n",
    "    print(\"   pip install cdsapi pygrib numpy pandas\\n\")\n",
    "\n",
    "    if WeatherConfig.CDS_API_KEY in (None, \"\", \"YOUR_UID:YOUR_API_KEY\"):\n",
    "        raise RuntimeError(\"CDS_API_KEY is not configured correctly in WeatherConfig.\")\n",
    "\n",
    "    days = iter_dates()\n",
    "    print(f\"üìÖ Date window: {WeatherConfig.MIN_DATE} ‚Üí {WeatherConfig.MAX_DATE}\")\n",
    "    print(f\"   Total days to process: {len(days)}\\n\")\n",
    "\n",
    "    # Prepare Spark and handle FULL_RELOAD\n",
    "    spark = init_spark_session()\n",
    "\n",
    "    if WeatherConfig.FULL_RELOAD:\n",
    "        print(f\"üß® FULL_RELOAD = TRUE ‚Üí dropping table {WeatherConfig.TARGET_TABLE}‚Ä¶\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {WeatherConfig.TARGET_TABLE}\")\n",
    "        print(\"   ‚úÖ Table dropped. A fresh one will be created on first write.\\n\")\n",
    "\n",
    "    # Run-level load metadata\n",
    "    run_load_id = str(uuid.uuid4())\n",
    "    run_load_ts = datetime.utcnow()\n",
    "\n",
    "    print(f\"üÜî run_load_id = {run_load_id}\")\n",
    "    print(f\"üïí run_load_ts = {run_load_ts.isoformat()}Z\\n\")\n",
    "\n",
    "    processed_days = 0\n",
    "    total_days = len(days)\n",
    "\n",
    "    for d in days:\n",
    "        # 1) Download\n",
    "        grib_path = download_one_day(d)\n",
    "        if not grib_path:\n",
    "            print(f\"   ‚ö†Ô∏è Skipping {d.isoformat()} due to download failure.\")\n",
    "            continue\n",
    "\n",
    "        # 2) Parse GRIB ‚Üí DataFrame\n",
    "        try:\n",
    "            df_day = grib_day_to_dataframe(grib_path, run_load_id, run_load_ts)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚úó Error parsing {d.isoformat()}: {e}\")\n",
    "            # still try to clean up file\n",
    "            try:\n",
    "                os.remove(grib_path)\n",
    "                print(f\"   üßπ Deleted temp file (after parse error): {os.path.abspath(grib_path)}\")\n",
    "            except OSError:\n",
    "                print(f\"   ‚ö†Ô∏è Could not delete temp file (after parse error): {os.path.abspath(grib_path)}\")\n",
    "            continue\n",
    "\n",
    "        # 3) Write to Delta (partition overwrite) with session-loss retry\n",
    "        try:\n",
    "            write_df_to_delta(spark, df_day)\n",
    "            processed_days += 1\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            print(f\"   ‚úó Error writing to Delta for {d.isoformat()}: {msg}\")\n",
    "\n",
    "            if \"SESSION_NOT_FOUND\" in msg or \"INVALID_HANDLE\" in msg:\n",
    "                print(\"   üîÑ Detected lost Spark session. Recreating SparkSession and retrying once...\")\n",
    "                spark = init_spark_session()\n",
    "                try:\n",
    "                    write_df_to_delta(spark, df_day)\n",
    "                    processed_days += 1\n",
    "                    print(\"   ‚úÖ Retry succeeded.\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"   ‚ùå Retry failed for {d.isoformat()}: {e2}\")\n",
    "            else:\n",
    "                print(\"   ‚ùå Non-recoverable write error, skipping this day.\")\n",
    "\n",
    "        # 4) Delete GRIB file\n",
    "        try:\n",
    "            abs_grib = os.path.abspath(grib_path)\n",
    "            os.remove(grib_path)\n",
    "            print(f\"   üßπ Deleted temp file: {abs_grib}\")\n",
    "        except OSError:\n",
    "            print(f\"   ‚ö†Ô∏è Could not delete temp file: {os.path.abspath(grib_path)}\")\n",
    "\n",
    "    print(\"\\nüéâ Pipeline finished!\")\n",
    "    print(f\"   Days successfully processed: {processed_days}/{total_days}\")\n",
    "    print(f\"   Delta table: {WeatherConfig.TARGET_TABLE}\")\n",
    "    print(\n",
    "        f\"\"\"\n",
    "üîç Example query (SQL):\n",
    "\n",
    "SELECT year, month, day, COUNT(*) AS rows\n",
    "FROM {WeatherConfig.TARGET_TABLE}\n",
    "GROUP BY year, month, day\n",
    "ORDER BY year, month, day;\n",
    "\"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/chavely.albert@gmail.com/energy-grid-load-prediction/requirements.txt",
     "pygrib"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_weather__lon_lat",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
