{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bbe823-3c51-46d0-a667-17f215190593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"EXPLORING NEW DATASET: electricity_and_weather_europe\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the table\n",
    "df = spark.table(\"workspace.default.electricity_and_weather_europe\")\n",
    "\n",
    "print(\"\\n1. BASIC INFO:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Total rows: {df.count():,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n2. COLUMN NAMES AND TYPES:\")\n",
    "print(\"-\"*60)\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n3. SAMPLE DATA (first 10 rows):\")\n",
    "print(\"-\"*60)\n",
    "display(df.limit(10))\n",
    "\n",
    "print(\"\\n4. COLUMN LIST:\")\n",
    "print(\"-\"*60)\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\n5. CHECK FOR DUPLICATES:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Total rows: {df.count():,}\")\n",
    "print(f\"Distinct rows: {df.distinct().count():,}\")\n",
    "\n",
    "print(\"\\n6. DATE RANGE:\")\n",
    "print(\"-\"*60)\n",
    "date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"Time columns found: {date_cols}\")\n",
    "if date_cols:\n",
    "    for col in date_cols:\n",
    "        df.select(\n",
    "            F.min(col).alias('min_date'),\n",
    "            F.max(col).alias('max_date')\n",
    "        ).show()\n",
    "\n",
    "print(\"\\n7. COUNTRIES:\")\n",
    "print(\"-\"*60)\n",
    "country_cols = [col for col in df.columns if 'country' in col.lower()]\n",
    "if country_cols:\n",
    "    for col in country_cols:\n",
    "        print(f\"\\nUnique values in {col}:\")\n",
    "        df.select(col).distinct().orderBy(col).show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05563d25-d0cd-4852-9bbd-dca9abeaf94e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"STEP 2: DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "df = spark.table(\"workspace.default.electricity_and_weather_europe\")\n",
    "\n",
    "print(\"\\n1. MISSING VALUES CHECK:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Count nulls for each column\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "null_counts = df.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Convert to pandas for easier viewing\n",
    "null_df = null_counts.toPandas().T\n",
    "null_df.columns = ['null_count']\n",
    "null_df['null_pct'] = (null_df['null_count'] / df.count()) * 100\n",
    "null_df = null_df[null_df['null_count'] > 0].sort_values('null_pct', ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with missing values:\")\n",
    "if len(null_df) > 0:\n",
    "    display(null_df.head(20))\n",
    "else:\n",
    "    print(\"‚úì No missing values found!\")\n",
    "\n",
    "print(\"\\n2. DUPLICATE CHECK:\")\n",
    "print(\"-\"*60)\n",
    "total_rows = df.count()\n",
    "distinct_rows = df.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"Duplicates: {duplicates:,} ({duplicates/total_rows*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n3. TIME COVERAGE:\")\n",
    "print(\"-\"*60)\n",
    "time_stats = df.select(\n",
    "    F.min('index').alias('start_date'),\n",
    "    F.max('index').alias('end_date'),\n",
    "    F.count('index').alias('total_records')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Start date: {time_stats['start_date']}\")\n",
    "print(f\"End date: {time_stats['end_date']}\")\n",
    "print(f\"Total records: {time_stats['total_records']:,}\")\n",
    "\n",
    "# Check time frequency\n",
    "print(\"\\n4. COUNTRY COVERAGE:\")\n",
    "print(\"-\"*60)\n",
    "country_counts = df.groupBy('country').count().orderBy(F.desc('count'))\n",
    "display(country_counts)\n",
    "\n",
    "print(\"\\n5. BASIC STATISTICS (Key Columns):\")\n",
    "print(\"-\"*60)\n",
    "key_cols = ['Actual_Load', 'net_imports', 'mean_temperature_c', 'mean_wind_speed']\n",
    "df.select(key_cols).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5ab26c-eea7-4973-9269-7aeaefe1b180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"STEP 3: FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to Pandas for easier analysis\n",
    "df_pandas = df.toPandas()  # ‚Üê Using 'df' not 'df_renamed'\n",
    "\n",
    "print(\"\\n1. GENERATION FEATURES:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Get all generation columns\n",
    "generation_cols = [col for col in df_pandas.columns if '__Actual_Aggregated' in col]\n",
    "print(f\"Total generation types: {len(generation_cols)}\")\n",
    "\n",
    "# Calculate total generation\n",
    "df_pandas['Total_Generation'] = df_pandas[generation_cols].sum(axis=1)\n",
    "\n",
    "print(f\"\\nTotal Generation statistics:\")\n",
    "print(f\"  Mean: {df_pandas['Total_Generation'].mean():,.2f} MW\")\n",
    "print(f\"  Median: {df_pandas['Total_Generation'].median():,.2f} MW\")\n",
    "print(f\"  Max: {df_pandas['Total_Generation'].max():,.2f} MW\")\n",
    "\n",
    "# Top generation sources by average contribution\n",
    "print(f\"\\nTop 10 Generation Sources (by average MW):\")\n",
    "gen_means = df_pandas[generation_cols].mean().sort_values(ascending=False).head(10)\n",
    "for col, val in gen_means.items():\n",
    "    col_name = col.replace('__Actual_Aggregated', '')\n",
    "    print(f\"  {col_name:.<50} {val:>10,.2f} MW\")\n",
    "\n",
    "print(\"\\n2. SUPPLY vs DEMAND BALANCE:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Calculate supply\n",
    "df_pandas['Supply'] = df_pandas['Total_Generation'] + df_pandas['net_imports']\n",
    "df_pandas['Imbalance'] = df_pandas['Supply'] - df_pandas['Actual_Load']\n",
    "df_pandas['Imbalance_Pct'] = (df_pandas['Imbalance'] / df_pandas['Actual_Load']) * 100\n",
    "\n",
    "print(f\"Average Actual_Load: {df_pandas['Actual_Load'].mean():>12,.2f} MW\")\n",
    "print(f\"Average Total_Generation: {df_pandas['Total_Generation'].mean():>12,.2f} MW\")\n",
    "print(f\"Average net_imports: {df_pandas['net_imports'].mean():>12,.2f} MW\")\n",
    "print(f\"Average Supply: {df_pandas['Supply'].mean():>12,.2f} MW\")\n",
    "print(f\"Average Imbalance: {df_pandas['Imbalance'].mean():>12,.2f} MW ({df_pandas['Imbalance_Pct'].mean():.2f}%)\")\n",
    "\n",
    "# Check for deficits\n",
    "deficit_count = (df_pandas['Imbalance'] < 0).sum()\n",
    "surplus_count = (df_pandas['Imbalance'] > 0).sum()\n",
    "balanced_count = (df_pandas['Imbalance'] == 0).sum()\n",
    "\n",
    "print(f\"\\nSupply-Demand Balance:\")\n",
    "print(f\"  Deficit (Supply < Demand): {deficit_count:>8,} ({deficit_count/len(df_pandas)*100:>5.2f}%)\")\n",
    "print(f\"  Surplus (Supply > Demand): {surplus_count:>8,} ({surplus_count/len(df_pandas)*100:>5.2f}%)\")\n",
    "print(f\"  Balanced (Supply = Demand): {balanced_count:>8,} ({balanced_count/len(df_pandas)*100:>5.2f}%)\")\n",
    "\n",
    "if deficit_count > 0:\n",
    "    deficits = df_pandas[df_pandas['Imbalance'] < 0]\n",
    "    print(f\"\\n  Deficit Statistics:\")\n",
    "    print(f\"    Mean deficit: {deficits['Imbalance'].mean():,.2f} MW\")\n",
    "    print(f\"    Worst deficit: {deficits['Imbalance'].min():,.2f} MW\")\n",
    "    print(f\"    Countries with deficits: {deficits['country'].nunique()}\")\n",
    "\n",
    "print(\"\\n3. WEATHER FEATURES:\")\n",
    "print(\"-\"*60)\n",
    "weather_cols = ['mean_ssrd', 'mean_wind_speed', 'mean_temperature_c']\n",
    "weather_stats = df_pandas[weather_cols].describe()\n",
    "print(weather_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654507d6-329f-4c5b-bcad-d57ec653a8c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nSTEP 4: BLACKOUT DEFINITION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. IMBALANCE DISTRIBUTION:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Percentiles of imbalance\n",
    "percentiles = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",
    "print(\"\\nImbalance Percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = df_pandas['Imbalance'].quantile(p)\n",
    "    print(f\"  {p*100:>5.1f}% (worst {100-p*100:.1f}%): {val:>10,.2f} MW\")\n",
    "\n",
    "print(\"\\n2. DEFICIT SEVERITY BY COUNTRY:\")\n",
    "print(\"-\"*60)\n",
    "deficit_by_country = df_pandas[df_pandas['Imbalance'] < 0].groupby('country').agg({\n",
    "    'Imbalance': ['mean', 'min', 'count']\n",
    "}).round(2)\n",
    "deficit_by_country.columns = ['Mean_Deficit_MW', 'Worst_Deficit_MW', 'Deficit_Count']\n",
    "deficit_by_country = deficit_by_country.sort_values('Mean_Deficit_MW')\n",
    "print(deficit_by_country.head(10))\n",
    "\n",
    "print(\"\\n3. POTENTIAL BLACKOUT DEFINITIONS:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Option 1: Extreme deficit (worse than 95th percentile)\n",
    "threshold_extreme = df_pandas['Imbalance'].quantile(0.05)\n",
    "blackout_extreme = (df_pandas['Imbalance'] < threshold_extreme).sum()\n",
    "print(f\"\\nOption 1: Extreme Deficit (< {threshold_extreme:.0f} MW)\")\n",
    "print(f\"  Would flag: {blackout_extreme:,} records ({blackout_extreme/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Option 2: Very severe deficit (< -10,000 MW)\n",
    "blackout_10k = (df_pandas['Imbalance'] < -10000).sum()\n",
    "print(f\"\\nOption 2: Severe Deficit (< -10,000 MW)\")\n",
    "print(f\"  Would flag: {blackout_10k:,} records ({blackout_10k/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Option 3: Deficit worse than -50% of load\n",
    "df_pandas['Deficit_Pct_of_Load'] = (df_pandas['Imbalance'] / df_pandas['Actual_Load']) * 100\n",
    "blackout_50pct = (df_pandas['Deficit_Pct_of_Load'] < -50).sum()\n",
    "print(f\"\\nOption 3: Deficit > 50% of Load\")\n",
    "print(f\"  Would flag: {blackout_50pct:,} records ({blackout_50pct/len(df_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Imbalance distribution\n",
    "axes[0, 0].hist(df_pandas['Imbalance'], bins=100, edgecolor='black')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Balance')\n",
    "axes[0, 0].axvline(x=threshold_extreme, color='orange', linestyle='--', linewidth=2, label='5th percentile')\n",
    "axes[0, 0].set_xlabel('Imbalance (MW)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Supply-Demand Imbalance Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Deficit % distribution\n",
    "axes[0, 1].hist(df_pandas['Deficit_Pct_of_Load'], bins=100, range=(-100, 50), edgecolor='black')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Balance')\n",
    "axes[0, 1].axvline(x=-50, color='orange', linestyle='--', linewidth=2, label='-50%')\n",
    "axes[0, 1].set_xlabel('Deficit as % of Load')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Deficit Percentage Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Deficit by country\n",
    "deficit_counts = df_pandas[df_pandas['Imbalance'] < 0].groupby('country').size().sort_values(ascending=False)\n",
    "axes[1, 0].barh(deficit_counts.index[:15], deficit_counts.values[:15])\n",
    "axes[1, 0].set_xlabel('Number of Deficit Hours')\n",
    "axes[1, 0].set_ylabel('Country')\n",
    "axes[1, 0].set_title('Top 15 Countries by Deficit Frequency')\n",
    "\n",
    "# Plot 4: Time series sample (first 1000 records)\n",
    "sample = df_pandas.head(1000)\n",
    "axes[1, 1].plot(sample.index, sample['Actual_Load'], label='Actual Load', alpha=0.7)\n",
    "axes[1, 1].plot(sample.index, sample['Supply'], label='Supply', alpha=0.7)\n",
    "axes[1, 1].fill_between(sample.index, sample['Actual_Load'], sample['Supply'], \n",
    "                         where=(sample['Supply'] < sample['Actual_Load']), \n",
    "                         alpha=0.3, color='red', label='Deficit')\n",
    "axes[1, 1].set_xlabel('Record Index')\n",
    "axes[1, 1].set_ylabel('MW')\n",
    "axes[1, 1].set_title('Load vs Supply (First 1000 records)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Since generation data is incomplete, define 'blackout risk' as:\")\n",
    "print(\"  üéØ Imbalance < 5th percentile (extreme deficit)\")\n",
    "print(f\"  üéØ Threshold: {threshold_extreme:.0f} MW\")\n",
    "print(f\"  üéØ This flags ~5% of data ({blackout_extreme:,} records)\")\n",
    "print(\"\\nThis represents UNUSUALLY BAD deficits, not just routine missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bd5426-6551-4959-a548-585ce809e405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"COMPARING ALL FAILURE DEFINITIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Option 1: Extreme Deficit (current)\n",
    "df_pandas['blackout_deficit'] = (df_pandas['Imbalance'] < df_pandas['Imbalance'].quantile(0.05)).astype(int)\n",
    "\n",
    "# Option 2: Rapid Change\n",
    "df_pandas['load_change'] = df_pandas.groupby('country')['Actual_Load'].diff()\n",
    "df_pandas['load_change_pct'] = (df_pandas['load_change'].abs() / df_pandas['Actual_Load']) * 100\n",
    "df_pandas['blackout_rapid'] = (df_pandas['load_change_pct'] > 15).astype(int)\n",
    "\n",
    "# Option 3: Peak Stress\n",
    "df_pandas['max_load_by_country'] = df_pandas.groupby('country')['Actual_Load'].transform('max')\n",
    "df_pandas['load_pct_of_max'] = (df_pandas['Actual_Load'] / df_pandas['max_load_by_country']) * 100\n",
    "df_pandas['blackout_peak'] = (df_pandas['load_pct_of_max'] > 90).astype(int)\n",
    "\n",
    "print(\"\\nFailure Type Frequencies:\")\n",
    "print(f\"  Extreme Deficit:  {df_pandas['blackout_deficit'].sum():>8,} ({df_pandas['blackout_deficit'].mean()*100:>5.2f}%)\")\n",
    "print(f\"  Rapid Change:     {df_pandas['blackout_rapid'].sum():>8,} ({df_pandas['blackout_rapid'].mean()*100:>5.2f}%)\")\n",
    "print(f\"  Peak Stress:      {df_pandas['blackout_peak'].sum():>8,} ({df_pandas['blackout_peak'].mean()*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nOverlap Analysis:\")\n",
    "overlap_all = (df_pandas['blackout_deficit'] & df_pandas['blackout_rapid'] & df_pandas['blackout_peak']).sum()\n",
    "overlap_deficit_rapid = (df_pandas['blackout_deficit'] & df_pandas['blackout_rapid']).sum()\n",
    "overlap_deficit_peak = (df_pandas['blackout_deficit'] & df_pandas['blackout_peak']).sum()\n",
    "overlap_rapid_peak = (df_pandas['blackout_rapid'] & df_pandas['blackout_peak']).sum()\n",
    "\n",
    "print(f\"  All 3 conditions:      {overlap_all:>8,}\")\n",
    "print(f\"  Deficit + Rapid:       {overlap_deficit_rapid:>8,}\")\n",
    "print(f\"  Deficit + Peak:        {overlap_deficit_peak:>8,}\")\n",
    "print(f\"  Rapid + Peak:          {overlap_rapid_peak:>8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3be4b75-59aa-4400-bc62-6c02b3f61410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "print(\"STEP 1: ADDING FORECASTED_LOAD FROM ORIGINAL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the main dataset\n",
    "df = spark.table(\"workspace.default.electricity_and_weather_europe\")\n",
    "print(f\"Main dataset loaded: {df.count():,} rows\")\n",
    "\n",
    "# Load the original load_forecast table\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "GRID_SCHEMA = \"european_grid_raw__v2\"\n",
    "\n",
    "load_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_forecast\")\n",
    "\n",
    "print(f\"\\n1. Original load_forecast table:\")\n",
    "print(f\"   Rows: {load_forecast.count():,}\")\n",
    "print(f\"   Columns: {load_forecast.columns}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n2. Sample forecast data:\")\n",
    "display(load_forecast.limit(5))\n",
    "\n",
    "# Select only needed columns\n",
    "load_forecast_clean = load_forecast.select(\n",
    "    F.col('index'),\n",
    "    F.col('country'),\n",
    "    F.col('Forecasted_Load')\n",
    ")\n",
    "\n",
    "# Join with current dataset\n",
    "print(f\"\\n3. Joining with electricity_and_weather_europe...\")\n",
    "\n",
    "df_with_forecast = df.join(\n",
    "    load_forecast_clean,\n",
    "    on=['index', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n4. Joined dataset:\")\n",
    "print(f\"   Rows: {df_with_forecast.count():,}\")\n",
    "print(f\"   Columns: {len(df_with_forecast.columns)}\")\n",
    "\n",
    "# Check match rate\n",
    "forecast_nulls = df_with_forecast.filter(F.col('Forecasted_Load').isNull()).count()\n",
    "print(f\"   Matched rows: {df_with_forecast.count() - forecast_nulls:,}\")\n",
    "print(f\"   Unmatched rows: {forecast_nulls:,} ({forecast_nulls/df_with_forecast.count()*100:.2f}%)\")\n",
    "\n",
    "# Convert to pandas for analysis\n",
    "df_pandas_complete = df_with_forecast.toPandas()\n",
    "\n",
    "# Calculate forecast error\n",
    "df_pandas_complete['Forecast_Error'] = abs(df_pandas_complete['Actual_Load'] - df_pandas_complete['Forecasted_Load'])\n",
    "df_pandas_complete['Forecast_Error_Pct'] = (df_pandas_complete['Forecast_Error'] / df_pandas_complete['Forecasted_Load']) * 100\n",
    "\n",
    "print(f\"\\n5. Forecast Error Statistics:\")\n",
    "print(f\"   Mean error: {df_pandas_complete['Forecast_Error'].mean():,.2f} MW\")\n",
    "print(f\"   Mean error %: {df_pandas_complete['Forecast_Error_Pct'].mean():.2f}%\")\n",
    "print(f\"   Median error %: {df_pandas_complete['Forecast_Error_Pct'].median():.2f}%\")\n",
    "print(f\"   95th percentile: {df_pandas_complete['Forecast_Error_Pct'].quantile(0.95):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS! Forecasted_Load added to dataset\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4ffcf5-cd91-44dc-b0aa-43415bb4df97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"FIXING DATA QUALITY ISSUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for problematic forecast values\n",
    "print(\"\\n1. Checking Forecasted_Load issues:\")\n",
    "zero_forecasts = (df_pandas_complete['Forecasted_Load'] == 0).sum()\n",
    "null_forecasts = df_pandas_complete['Forecasted_Load'].isna().sum()\n",
    "very_small_forecasts = (df_pandas_complete['Forecasted_Load'] < 1).sum()\n",
    "\n",
    "print(f\"   Zero forecasts: {zero_forecasts:,}\")\n",
    "print(f\"   Null forecasts: {null_forecasts:,}\")\n",
    "print(f\"   Very small (<1 MW) forecasts: {very_small_forecasts:,}\")\n",
    "\n",
    "# Clean the data: Remove rows with problematic forecasts\n",
    "print(\"\\n2. Cleaning data...\")\n",
    "original_count = len(df_pandas_complete)\n",
    "\n",
    "df_pandas_complete = df_pandas_complete[\n",
    "    (df_pandas_complete['Forecasted_Load'].notna()) & \n",
    "    (df_pandas_complete['Forecasted_Load'] > 10)  # Keep only reasonable forecasts\n",
    "].copy()\n",
    "\n",
    "removed_count = original_count - len(df_pandas_complete)\n",
    "print(f\"   Removed {removed_count:,} problematic rows\")\n",
    "print(f\"   Remaining: {len(df_pandas_complete):,} rows\")\n",
    "\n",
    "# Recalculate forecast error (now safe)\n",
    "df_pandas_complete['Forecast_Error'] = abs(df_pandas_complete['Actual_Load'] - df_pandas_complete['Forecasted_Load'])\n",
    "df_pandas_complete['Forecast_Error_Pct'] = (df_pandas_complete['Forecast_Error'] / df_pandas_complete['Forecasted_Load']) * 100\n",
    "\n",
    "print(f\"\\n3. CLEAN Forecast Error Statistics:\")\n",
    "print(f\"   Mean error: {df_pandas_complete['Forecast_Error'].mean():,.2f} MW\")\n",
    "print(f\"   Mean error %: {df_pandas_complete['Forecast_Error_Pct'].mean():.2f}%\")\n",
    "print(f\"   Median error %: {df_pandas_complete['Forecast_Error_Pct'].median():.2f}%\")\n",
    "print(f\"   95th percentile: {df_pandas_complete['Forecast_Error_Pct'].quantile(0.95):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CLEANED! Ready for Step 2\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3760cd9f-c124-4771-bade-0d09c0512769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"STEP 2: CREATING ALL 4 FAILURE TYPE DEFINITIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate necessary columns for all failure types\n",
    "\n",
    "# Get generation columns and calculate totals\n",
    "generation_cols = [col for col in df_pandas_complete.columns if '__Actual_Aggregated' in col]\n",
    "df_pandas_complete['Total_Generation'] = df_pandas_complete[generation_cols].sum(axis=1)\n",
    "df_pandas_complete['Supply'] = df_pandas_complete['Total_Generation'] + df_pandas_complete['net_imports']\n",
    "df_pandas_complete['Imbalance'] = df_pandas_complete['Supply'] - df_pandas_complete['Actual_Load']\n",
    "\n",
    "# Failure Type 1: Extreme Deficit\n",
    "df_pandas_complete['blackout_deficit'] = (df_pandas_complete['Imbalance'] < df_pandas_complete['Imbalance'].quantile(0.05)).astype(int)\n",
    "\n",
    "# Failure Type 2: Forecast Deviation\n",
    "df_pandas_complete['blackout_forecast_deviation'] = (df_pandas_complete['Forecast_Error_Pct'] > 10).astype(int)\n",
    "\n",
    "# Failure Type 3: Rapid Change\n",
    "df_pandas_complete['load_change'] = df_pandas_complete.groupby('country')['Actual_Load'].diff()\n",
    "df_pandas_complete['load_change_pct'] = (df_pandas_complete['load_change'].abs() / df_pandas_complete['Actual_Load']) * 100\n",
    "df_pandas_complete['blackout_rapid'] = (df_pandas_complete['load_change_pct'] > 15).astype(int)\n",
    "\n",
    "# Failure Type 4: Peak Stress\n",
    "df_pandas_complete['max_load_by_country'] = df_pandas_complete.groupby('country')['Actual_Load'].transform('max')\n",
    "df_pandas_complete['load_pct_of_max'] = (df_pandas_complete['Actual_Load'] / df_pandas_complete['max_load_by_country']) * 100\n",
    "df_pandas_complete['blackout_peak'] = (df_pandas_complete['load_pct_of_max'] > 90).astype(int)\n",
    "\n",
    "print(\"\\nAll 4 failure types created!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9863d631-5474-4dc4-9c7e-06215fc7cfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"STEP 3: COMPARING ALL FAILURE TYPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nFailure Type Frequencies:\")\n",
    "print(f\"  1. Extreme Deficit:        {df_pandas_complete['blackout_deficit'].sum():>8,} ({df_pandas_complete['blackout_deficit'].mean()*100:>5.2f}%)\")\n",
    "print(f\"  2. Forecast Deviation:     {df_pandas_complete['blackout_forecast_deviation'].sum():>8,} ({df_pandas_complete['blackout_forecast_deviation'].mean()*100:>5.2f}%)\")\n",
    "print(f\"  3. Rapid Change:           {df_pandas_complete['blackout_rapid'].sum():>8,} ({df_pandas_complete['blackout_rapid'].mean()*100:>5.2f}%)\")\n",
    "print(f\"  4. Peak Stress:            {df_pandas_complete['blackout_peak'].sum():>8,} ({df_pandas_complete['blackout_peak'].mean()*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nOverlap Analysis:\")\n",
    "overlap_all = (df_pandas_complete['blackout_deficit'] & \n",
    "               df_pandas_complete['blackout_forecast_deviation'] & \n",
    "               df_pandas_complete['blackout_rapid'] & \n",
    "               df_pandas_complete['blackout_peak']).sum()\n",
    "overlap_def_fore = (df_pandas_complete['blackout_deficit'] & df_pandas_complete['blackout_forecast_deviation']).sum()\n",
    "overlap_def_rapid = (df_pandas_complete['blackout_deficit'] & df_pandas_complete['blackout_rapid']).sum()\n",
    "overlap_def_peak = (df_pandas_complete['blackout_deficit'] & df_pandas_complete['blackout_peak']).sum()\n",
    "overlap_fore_rapid = (df_pandas_complete['blackout_forecast_deviation'] & df_pandas_complete['blackout_rapid']).sum()\n",
    "overlap_fore_peak = (df_pandas_complete['blackout_forecast_deviation'] & df_pandas_complete['blackout_peak']).sum()\n",
    "overlap_rapid_peak = (df_pandas_complete['blackout_rapid'] & df_pandas_complete['blackout_peak']).sum()\n",
    "\n",
    "print(f\"  All 4 conditions:              {overlap_all:>8,}\")\n",
    "print(f\"  Deficit + Forecast:            {overlap_def_fore:>8,}\")\n",
    "print(f\"  Deficit + Rapid:               {overlap_def_rapid:>8,}\")\n",
    "print(f\"  Deficit + Peak:                {overlap_def_peak:>8,}\")\n",
    "print(f\"  Forecast + Rapid:              {overlap_fore_rapid:>8,}\")\n",
    "print(f\"  Forecast + Peak:               {overlap_fore_peak:>8,}\")\n",
    "print(f\"  Rapid + Peak:                  {overlap_rapid_peak:>8,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING COMPLETE DATASET FOR TEAM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "df_complete_spark = spark.createDataFrame(df_pandas_complete)\n",
    "\n",
    "# Save as Delta table\n",
    "table_name = \"workspace.default.electricity_weather_forecast_complete\"\n",
    "df_complete_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\nSaved to: {table_name}\")\n",
    "print(f\"  Rows: {df_complete_spark.count():,}\")\n",
    "print(f\"  Columns: {len(df_complete_spark.columns)}\")\n",
    "\n",
    "# Verify the table was saved\n",
    "print(f\"\\nVerifying save...\")\n",
    "test_load = spark.table(table_name)\n",
    "print(f\"  Verified: {test_load.count():,} rows in saved table\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HOW YOUR COLLEAGUES CAN ACCESS IT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# In a new notebook, they can load it with:\n",
    "df = spark.table('workspace.default.electricity_weather_forecast_complete')\n",
    "\n",
    "# Or convert to pandas:\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "# Available columns include:\n",
    "# - All original features (generation, weather, etc.)\n",
    "# - Forecasted_Load (joined from original dataset)\n",
    "# - Forecast_Error and Forecast_Error_Pct\n",
    "# - All 4 blackout target variables:\n",
    "#   * blackout_deficit\n",
    "#   * blackout_forecast_deviation\n",
    "#   * blackout_rapid\n",
    "#   * blackout_peak\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Time period: 2019-2021\")\n",
    "print(f\"  Countries: 26 European countries\")\n",
    "print(f\"  Total records: {len(df_pandas_complete):,}\")\n",
    "print(f\"  Features: {len(df_pandas_complete.columns)}\")\n",
    "print(f\"  Forecast match rate: 98.2%\")\n",
    "print(f\"  Average forecast error: 4.48%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION FOR MODELING:\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eddc29c-65e0-4006-b4c0-fb6aca6eaefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"FINDING YOUR SAVED TABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check all tables in workspace.default\n",
    "print(\"\\n1. Tables in workspace.default:\")\n",
    "tables_workspace = spark.sql(\"SHOW TABLES IN workspace.default\").toPandas()\n",
    "display(tables_workspace)\n",
    "\n",
    "# Check if our table exists\n",
    "table_name = \"workspace.default.electricity_weather_forecast_complete\"\n",
    "table_exists = spark.catalog.tableExists(table_name)\n",
    "print(f\"\\n2. Does table exist? {table_exists}\")\n",
    "\n",
    "# Try to load it\n",
    "if table_exists:\n",
    "    print(\"\\n3. Table found! Loading...\")\n",
    "    test_df = spark.table(table_name)\n",
    "    print(f\"   Rows: {test_df.count():,}\")\n",
    "    print(f\"   Columns: {len(test_df.columns)}\")\n",
    "else:\n",
    "    print(\"\\n3. Table not found. Let's check other locations...\")\n",
    "    \n",
    "    # Check current database\n",
    "    print(f\"\\nCurrent database: {spark.catalog.currentDatabase()}\")\n",
    "    \n",
    "    # Check all databases\n",
    "    print(f\"\\nAll databases:\")\n",
    "    databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "    display(databases)\n",
    "\n",
    "# Also check what we have in memory\n",
    "print(\"\\n4. Available DataFrames in memory:\")\n",
    "print(f\"   df_pandas_complete: {len(df_pandas_complete) if 'df_pandas_complete' in locals() else 'Not found'}\")\n",
    "print(f\"   df_complete_spark: {'Found' if 'df_complete_spark' in locals() else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f780a3a-aba2-41c5-8ea1-26c4b519a30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"CHECKING SAVED TABLE CONTENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the saved table\n",
    "df_saved = spark.table('workspace.default.electricity_weather_forecast_complete')\n",
    "\n",
    "# Convert to pandas to see all columns\n",
    "df_saved_pandas = df_saved.toPandas()\n",
    "\n",
    "print(f\"\\n1. TOTAL COLUMNS: {len(df_saved_pandas.columns)}\")\n",
    "\n",
    "print(f\"\\n2. ORIGINAL FEATURES (from electricity_and_weather_europe):\")\n",
    "original_features = [col for col in df_saved_pandas.columns if not col.startswith('blackout') \n",
    "                     and col not in ['Forecasted_Load', 'Forecast_Error', 'Forecast_Error_Pct', \n",
    "                                     'Total_Generation', 'Supply', 'Imbalance', 'Imbalance_Pct',\n",
    "                                     'load_change', 'load_change_pct', 'max_load_by_country', 'load_pct_of_max']]\n",
    "print(f\"   Count: {len(original_features)}\")\n",
    "print(f\"   Examples: {original_features[:5]}\")\n",
    "\n",
    "print(f\"\\n3. ADDED FROM FORECAST TABLE:\")\n",
    "forecast_features = ['Forecasted_Load', 'Forecast_Error', 'Forecast_Error_Pct']\n",
    "for feat in forecast_features:\n",
    "    exists = feat in df_saved_pandas.columns\n",
    "    print(f\"   {feat}: {'‚úÖ YES' if exists else '‚ùå NO'}\")\n",
    "\n",
    "print(f\"\\n4. ENGINEERED FEATURES (from failure analysis):\")\n",
    "engineered_features = {\n",
    "    'Total_Generation': 'Sum of all generation types',\n",
    "    'Supply': 'Total_Generation + net_imports',\n",
    "    'Imbalance': 'Supply - Actual_Load',\n",
    "    'Imbalance_Pct': 'Imbalance as % of load',\n",
    "    'load_change': 'Hour-to-hour load change',\n",
    "    'load_change_pct': 'Load change as %',\n",
    "    'max_load_by_country': 'Historical max per country',\n",
    "    'load_pct_of_max': 'Current load as % of max'\n",
    "}\n",
    "for feat, description in engineered_features.items():\n",
    "    exists = feat in df_saved_pandas.columns\n",
    "    print(f\"   {feat}: {'‚úÖ YES' if exists else '‚ùå NO'} - {description}\")\n",
    "\n",
    "print(f\"\\n5. TARGET VARIABLES:\")\n",
    "target_features = {\n",
    "    'blackout_deficit': 'Extreme supply deficit (5th percentile)',\n",
    "    'blackout_forecast_deviation': 'Forecast error >10%',\n",
    "    'blackout_rapid': 'Rapid load change >15%',\n",
    "    'blackout_peak': 'Load >90% of max capacity'\n",
    "}\n",
    "for feat, description in target_features.items():\n",
    "    exists = feat in df_saved_pandas.columns\n",
    "    print(f\"   {feat}: {'‚úÖ YES' if exists else '‚ùå NO'} - {description}\")\n",
    "\n",
    "print(f\"\\n6. COMPLETE COLUMN LIST:\")\n",
    "print(df_saved_pandas.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total columns saved: {len(df_saved_pandas.columns)}\")\n",
    "print(f\"Ready for modeling: {'‚úÖ YES' if 'blackout_forecast_deviation' in df_saved_pandas.columns else '‚ùå NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bab036-d966-4ca9-8265-be3e6ce10206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"CREATING CLEAN DATASET - ONLY ADDING FORECASTED_LOAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Load original dataset\n",
    "df_original = spark.table(\"workspace.default.electricity_and_weather_europe\")\n",
    "print(f\"\\n1. Original dataset loaded: {df_original.count():,} rows, {len(df_original.columns)} columns\")\n",
    "\n",
    "# Step 2: Load forecast data\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "GRID_SCHEMA = \"european_grid_raw__v2\"\n",
    "load_forecast = spark.table(f\"{CATALOG}.{GRID_SCHEMA}.load_forecast\")\n",
    "\n",
    "# Select only needed columns\n",
    "load_forecast_clean = load_forecast.select(\n",
    "    F.col('index'),\n",
    "    F.col('country'),\n",
    "    F.col('Forecasted_Load')\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Forecast data loaded: {load_forecast_clean.count():,} rows\")\n",
    "\n",
    "# Step 3: Join - ONLY add Forecasted_Load\n",
    "df_clean = df_original.join(\n",
    "    load_forecast_clean,\n",
    "    on=['index', 'country'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n3. Joined dataset:\")\n",
    "print(f\"   Rows: {df_clean.count():,}\")\n",
    "print(f\"   Columns: {len(df_clean.columns)} (should be {len(df_original.columns) + 1})\")\n",
    "\n",
    "# Check match rate\n",
    "forecast_nulls = df_clean.filter(F.col('Forecasted_Load').isNull()).count()\n",
    "print(f\"   Matched rows: {df_clean.count() - forecast_nulls:,}\")\n",
    "print(f\"   Unmatched rows: {forecast_nulls:,} ({forecast_nulls/df_clean.count()*100:.2f}%)\")\n",
    "\n",
    "# Show columns\n",
    "print(f\"\\n4. Columns in clean dataset:\")\n",
    "print(df_clean.columns)\n",
    "\n",
    "# Step 4: Save clean version\n",
    "print(f\"\\n5. Saving clean dataset...\")\n",
    "table_name_clean = \"workspace.default.electricity_weather_with_forecast\"\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name_clean)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to: {table_name_clean}\")\n",
    "print(f\"   Rows: {df_clean.count():,}\")\n",
    "print(f\"   Columns: {len(df_clean.columns)}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n6. Verification:\")\n",
    "test_df = spark.table(table_name_clean)\n",
    "print(f\"   Loaded: {test_df.count():,} rows\")\n",
    "display(test_df.limit(5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEAN DATASET READY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Your colleagues can load it with:\n",
    "df = spark.table('{table_name_clean}')\n",
    "\n",
    "This dataset contains:\n",
    "‚úÖ All original 64 columns from electricity_and_weather_europe\n",
    "‚úÖ ONLY 1 added column: Forecasted_Load\n",
    "‚ùå NO engineered features\n",
    "‚ùå NO target variables\n",
    "\n",
    "Total: 65 columns (64 original + 1 forecast)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
