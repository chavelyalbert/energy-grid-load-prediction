{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c87e88-9604-426c-aa8c-79393a33bff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "EUROPEAN POWER GRID STRESS PREDICTION - PRODUCTION MODEL\n",
    "================================================================================\n",
    "Project: Capstone - Blackout Risk Prediction\n",
    "Date: November 2025\n",
    "\n",
    "OBJECTIVE:\n",
    "Predict grid stress scores (0-75) for European power grids using only\n",
    "legitimate operational features available in real-time.\n",
    "\n",
    "DATA LEAKAGE PREVENTION:\n",
    "Excluded features that create circular dependencies:\n",
    "- net_imports: Used to calculate T7/T8 components of target\n",
    "- stress_lag_*: Using target to predict target\n",
    "- reserve_margin_ml, forecast_load_error: Components of target scoring\n",
    "\n",
    "LEGITIMATE FEATURES USED:\n",
    "- Load data: Actual and forecasted electricity demand\n",
    "- Weather: Temperature, wind speed, solar radiation\n",
    "- Temporal: Hour, day, week patterns (cyclical encoding)\n",
    "- Historical: Lag features of load, imports, temperature (past values)\n",
    "- Derived: Rolling statistics, load-weather interactions\n",
    "\n",
    "TARGET: grid_stress_score (0-75 points)\n",
    "- 0-24: Normal operations\n",
    "- 25-49: Moderate stress\n",
    "- 50-74: High stress (blackout risk)\n",
    "- 75: Critical\n",
    "\n",
    "DATASET:\n",
    "- Train: 386,525 records (2023-2024)\n",
    "- Validation: 111,670 records (Jan-Jul 2025)\n",
    "- Test: 53,599 records (Aug-Nov 2025)\n",
    "- Countries: 23 European nations\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Install packages\n",
    "%pip install xgboost==2.0.3 lightgbm==4.1.0\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EUROPEAN GRID STRESS PREDICTION - PRODUCTION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f2f149-8b9f-4f96-9b7a-4c5693f6ebf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 1: DATA LOADING & INITIAL EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load datasets\n",
    "train_df = spark.table(\"workspace.default.train_set_imputed\").toPandas()\n",
    "val_df = spark.table(\"workspace.default.validation_set_imputed\").toPandas()\n",
    "test_df = spark.table(\"workspace.default.test_set_imputed\").toPandas()\n",
    "\n",
    "print(f\"\\n‚úì Data loaded: {train_df.shape[0] + val_df.shape[0] + test_df.shape[0]:,} total records\")\n",
    "print(f\"  Train:      {train_df.shape[0]:>8,} rows √ó {train_df.shape[1]:>2} columns\")\n",
    "print(f\"  Validation: {val_df.shape[0]:>8,} rows √ó {val_df.shape[1]:>2} columns\")\n",
    "print(f\"  Test:       {test_df.shape[0]:>8,} rows √ó {test_df.shape[1]:>2} columns\")\n",
    "\n",
    "# Target analysis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TARGET VARIABLE: grid_stress_score\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nDistribution Statistics:\")\n",
    "print(f\"  Mean:   {train_df['grid_stress_score'].mean():.2f}\")\n",
    "print(f\"  Median: {train_df['grid_stress_score'].median():.2f}\")\n",
    "print(f\"  Std:    {train_df['grid_stress_score'].std():.2f}\")\n",
    "print(f\"  Range:  [{train_df['grid_stress_score'].min():.1f}, {train_df['grid_stress_score'].max():.1f}]\")\n",
    "\n",
    "print(f\"\\nValue Distribution:\")\n",
    "stress_counts = train_df['grid_stress_score'].value_counts().sort_index()\n",
    "for score, count in stress_counts.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    category = \"NORMAL\" if score < 25 else \"MODERATE\" if score < 50 else \"HIGH RISK\"\n",
    "    print(f\"  {score:>5.1f}: {count:>8,} ({pct:>5.2f}%) - {category}\")\n",
    "\n",
    "# Temporal coverage\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TEMPORAL COVERAGE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Start: {df['index'].min()}\")\n",
    "    print(f\"  End:   {df['index'].max()}\")\n",
    "    print(f\"  Days:  {(df['index'].max() - df['index'].min()).days}\")\n",
    "\n",
    "# Country distribution\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COUNTRY DISTRIBUTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "country_counts = train_df['country'].value_counts()\n",
    "print(f\"\\nTotal countries: {len(country_counts)}\")\n",
    "print(f\"\\nRecords per country:\")\n",
    "for country, count in country_counts.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    avg_stress = train_df[train_df['country'] == country]['grid_stress_score'].mean()\n",
    "    print(f\"  {country:>2}: {count:>8,} ({pct:>4.2f}%) - Avg stress: {avg_stress:>5.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Initial exploration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5480764-c1f6-4811-8b3d-ad7d6c21069f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 2: FEATURE ENGINEERING (NO LEAKAGE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_clean_features(df):\n",
    "    \"\"\"\n",
    "    Create features WITHOUT any data leakage.\n",
    "    Excludes: net_imports, stress_lag_*, reserve_margin_ml, forecast_load_error\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nSorting data by country and time...\")\n",
    "    df = df.sort_values(['country', 'index']).reset_index(drop=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEMPORAL FEATURES\n",
    "    # ========================================================================\n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    df['hour'] = df['index'].dt.hour\n",
    "    df['month'] = df['index'].dt.month\n",
    "    df['day_of_week'] = df['index'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Peak hours\n",
    "    df['is_morning_peak'] = df['hour'].isin([7, 8, 9]).astype(int)\n",
    "    df['is_evening_peak'] = df['hour'].isin([18, 19, 20, 21]).astype(int)\n",
    "    df['is_peak_hour'] = (df['is_morning_peak'] | df['is_evening_peak']).astype(int)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LAG FEATURES (Using past values only - NO stress lags!)\n",
    "    # ========================================================================\n",
    "    print(\"Creating lag features (load, imports, temperature)...\")\n",
    "    \n",
    "    # Load lags\n",
    "    for lag in [1, 24]:\n",
    "        df[f'load_lag_{lag}h'] = df.groupby('country')['Actual_Load'].shift(lag)\n",
    "    \n",
    "    # Import lags (using past net_imports - legitimate!)\n",
    "    df['imports_lag_1h'] = df.groupby('country')['net_imports'].shift(1)\n",
    "    \n",
    "    # Temperature lags\n",
    "    df['temp_lag_1h'] = df.groupby('country')['mean_temperature_c'].shift(1)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ROLLING STATISTICS\n",
    "    # ========================================================================\n",
    "    print(\"Creating rolling statistics...\")\n",
    "    \n",
    "    df['load_rolling_mean_24h'] = df.groupby('country')['Actual_Load'].transform(\n",
    "        lambda x: x.rolling(window=24, min_periods=1).mean()\n",
    "    )\n",
    "    df['load_rolling_std_24h'] = df.groupby('country')['Actual_Load'].transform(\n",
    "        lambda x: x.rolling(window=24, min_periods=1).std()\n",
    "    )\n",
    "    \n",
    "    df['imports_rolling_mean_24h'] = df.groupby('country')['net_imports'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=24, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Change features\n",
    "    df['load_change_1h'] = df.groupby('country')['Actual_Load'].diff(1)\n",
    "    df['load_change_24h'] = df.groupby('country')['Actual_Load'].diff(24)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # INTERACTION FEATURES\n",
    "    # ========================================================================\n",
    "    print(\"Creating interaction features...\")\n",
    "    \n",
    "    # Load-forecast interactions\n",
    "    df['load_forecast_diff'] = df['Actual_Load'] - df['Forecasted_Load']\n",
    "    df['load_forecast_ratio'] = df['Actual_Load'] / (df['Forecasted_Load'] + 1e-6)\n",
    "    df['load_forecast_error_pct'] = np.abs(df['load_forecast_diff']) / (df['Forecasted_Load'] + 1e-6) * 100\n",
    "    \n",
    "    # Weather-load interactions\n",
    "    df['load_per_temp'] = df['Actual_Load'] / (df['mean_temperature_c'] + 20)\n",
    "    df['temp_load_product'] = df['mean_temperature_c'] * df['Actual_Load'] / 10000\n",
    "    \n",
    "    # Weather extremes\n",
    "    df['is_very_cold'] = (df['mean_temperature_c'] < 0).astype(int)\n",
    "    df['temp_extreme'] = df['is_very_cold'].astype(int)\n",
    "    \n",
    "    # Wind power potential\n",
    "    df['wind_power_index'] = df['mean_wind_speed'] ** 3 / 100\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SEASONALITY\n",
    "    # ========================================================================\n",
    "    print(\"Creating seasonality features...\")\n",
    "    \n",
    "    df['hourly_avg_load'] = df.groupby(['country', 'hour'])['Actual_Load'].transform('mean')\n",
    "    df['load_deviation_from_hourly_avg'] = df['Actual_Load'] - df['hourly_avg_load']\n",
    "    \n",
    "    df['daily_avg_load'] = df.groupby(['country', 'day_of_week'])['Actual_Load'].transform('mean')\n",
    "    df['load_deviation_from_daily_avg'] = df['Actual_Load'] - df['daily_avg_load']\n",
    "    \n",
    "    print(\"‚úì Feature engineering complete\\n\")\n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "print(\"Applying feature engineering...\")\n",
    "train_df = create_clean_features(train_df)\n",
    "val_df = create_clean_features(val_df)\n",
    "test_df = create_clean_features(test_df)\n",
    "\n",
    "print(f\"‚úì Feature engineering complete\")\n",
    "print(f\"  Total columns: {train_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5bb32d-4467-4856-a24f-291b7449527a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 3: COMPREHENSIVE EDA & CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Define clean feature set (exclude leakage and metadata)\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 1] Defining clean feature set...\")\n",
    "\n",
    "LEAKAGE_COLS = [\n",
    "    # Metadata\n",
    "    'index', 'country',\n",
    "    # Target\n",
    "    'grid_stress_score',\n",
    "    # Data leakage - components of target\n",
    "    'reserve_margin_ml', 'forecast_load_error', 'load_rel_error',\n",
    "    'net_imports',  # Used to calculate T7/T8\n",
    "    'P10_net', 'P90_net',  # Thresholds\n",
    "    'score_reserve_margin', 'score_load_error', 'score_T7', 'score_T8',\n",
    "    'T7_high_exports', 'T8_high_imports',\n",
    "    # Redundant temporal\n",
    "    'hour', 'month', 'day_of_week'\n",
    "]\n",
    "\n",
    "# Get feature candidates\n",
    "all_cols = train_df.columns.tolist()\n",
    "feature_candidates = [col for col in all_cols if col not in LEAKAGE_COLS]\n",
    "\n",
    "print(f\"  Total columns: {len(all_cols)}\")\n",
    "print(f\"  Excluded: {len(LEAKAGE_COLS)}\")\n",
    "print(f\"  Feature candidates: {len(feature_candidates)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Select numeric features for correlation\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 3] Preparing numeric features for correlation analysis...\")\n",
    "\n",
    "numeric_features = []\n",
    "for col in feature_candidates:\n",
    "    if train_df[col].dtype in ['int64', 'float64']:\n",
    "        missing_pct = train_df[col].isnull().sum() / len(train_df)\n",
    "        if missing_pct < 0.80:  # Keep if <80% missing\n",
    "            numeric_features.append(col)\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate correlations with target\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 4] Calculating correlations with target...\")\n",
    "\n",
    "correlations = {}\n",
    "for feat in numeric_features:\n",
    "    valid_count = train_df[feat].notna().sum()\n",
    "    if valid_count > 100:\n",
    "        corr = train_df[feat].corr(train_df['grid_stress_score'])\n",
    "        if not np.isnan(corr):\n",
    "            correlations[feat] = corr\n",
    "\n",
    "corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])\n",
    "corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()\n",
    "corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 25 Features by Correlation with grid_stress_score:\")\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<50} {'Correlation':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, (feat, row) in enumerate(corr_df.head(25).iterrows(), 1):\n",
    "    print(f\"{idx:<6} {feat:<50} {row['Correlation']:>12.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 1: Correlation Heatmap - Top Features\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 5] Creating correlation matrix visualization...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "# Plot 1: Correlation heatmap of top 20 features + target\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "top_20_features = corr_df.head(20).index.tolist()\n",
    "heatmap_data = train_df[top_20_features + ['grid_stress_score']].corr()\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax1, vmin=-1, vmax=1, annot_kws={'size': 7})\n",
    "ax1.set_title('Correlation Matrix: Top 20 Features + Target', \n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "plt.setp(ax1.get_yticklabels(), rotation=0, fontsize=8)\n",
    "\n",
    "# Plot 2: Feature importance by correlation (bar chart)\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "top_20 = corr_df.head(20).sort_values('Correlation', ascending=True)\n",
    "colors = ['red' if x < 0 else 'green' for x in top_20['Correlation']]\n",
    "bars = ax2.barh(range(len(top_20)), top_20['Correlation'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_20)))\n",
    "ax2.set_yticklabels(top_20.index, fontsize=8)\n",
    "ax2.set_xlabel('Correlation with grid_stress_score', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Top 20 Features by Correlation', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, top_20['Correlation'])):\n",
    "    ax2.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}', \n",
    "             va='center', fontsize=7, fontweight='bold')\n",
    "\n",
    "\n",
    "# Plot 3: Target distribution\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "ax3.hist(train_df['grid_stress_score'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax3.axvline(x=25, color='orange', linestyle='--', linewidth=2, label='Moderate (25)')\n",
    "ax3.axvline(x=50, color='red', linestyle='--', linewidth=2, label='High Risk (50)')\n",
    "ax3.set_xlabel('Grid Stress Score', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Target Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "mean_val = train_df['grid_stress_score'].mean()\n",
    "median_val = train_df['grid_stress_score'].median()\n",
    "ax3.text(0.98, 0.97, f'Mean: {mean_val:.2f}\\nMedian: {median_val:.2f}',\n",
    "         transform=ax3.transAxes, fontsize=10, verticalalignment='top',\n",
    "         horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Plot 4: Country stress comparison\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "country_stress = train_df.groupby('country')['grid_stress_score'].mean().sort_values(ascending=True)\n",
    "colors_country = ['red' if x > 35 else 'orange' if x > 28 else 'green' for x in country_stress.values]\n",
    "bars = ax4.barh(range(len(country_stress)), country_stress.values, color=colors_country, alpha=0.7, edgecolor='black')\n",
    "ax4.set_yticks(range(len(country_stress)))\n",
    "ax4.set_yticklabels(country_stress.index, fontsize=8)\n",
    "ax4.set_xlabel('Average Grid Stress Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Average Stress by Country', fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.axvline(x=mean_val, color='black', linestyle='--', linewidth=1.5, alpha=0.5, \n",
    "            label=f'Overall Avg ({mean_val:.1f})')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, country_stress.values)):\n",
    "    ax4.text(val + 0.5, i, f'{val:.1f}', va='center', fontsize=7)\n",
    "\n",
    "plt.suptitle('European Grid Stress Prediction - Exploratory Data Analysis', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Correlation matrix and distributions created\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 2: Time Series Patterns\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 6] Creating time series pattern analysis...\")\n",
    "\n",
    "fig2 = plt.figure(figsize=(20, 10))\n",
    "\n",
    "sample_country = 'DE'\n",
    "sample_data = train_df[train_df['country'] == sample_country].sort_values('index').head(168*2)\n",
    "\n",
    "ax5 = plt.subplot(3, 1, 1)\n",
    "ax5.plot(sample_data['index'], sample_data['grid_stress_score'], linewidth=1.5, color='darkblue')\n",
    "ax5.axhline(y=50, color='red', linestyle='--', linewidth=2, label='High Risk (50)')\n",
    "ax5.axhline(y=25, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Moderate (25)')\n",
    "ax5.set_ylabel('Grid Stress Score', fontsize=11, fontweight='bold')\n",
    "ax5.set_title(f'Grid Stress Time Series - {sample_country} (2 weeks)', fontsize=14, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "plt.savefig('../images/stress_per_country.png')\n",
    "\n",
    "ax6 = plt.subplot(3, 1, 2)\n",
    "ax6.plot(sample_data['index'], sample_data['Actual_Load'], linewidth=1.5, color='green', label='Actual Load')\n",
    "ax6.plot(sample_data['index'], sample_data['Forecasted_Load'], linewidth=1.5, color='orange', \n",
    "         linestyle='--', label='Forecasted Load')\n",
    "ax6.set_ylabel('Load (MW)', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Load: Actual vs Forecasted', fontsize=14, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(alpha=0.3)\n",
    "\n",
    "ax7 = plt.subplot(3, 1, 3)\n",
    "ax7.plot(sample_data['index'], sample_data['mean_temperature_c'], linewidth=1.5, color='red', label='Temperature')\n",
    "ax7_twin = ax7.twinx()\n",
    "ax7_twin.plot(sample_data['index'], sample_data['mean_wind_speed'], linewidth=1.5, color='blue', label='Wind Speed')\n",
    "ax7.set_ylabel('Temperature (¬∞C)', fontsize=11, fontweight='bold', color='red')\n",
    "ax7_twin.set_ylabel('Wind Speed (m/s)', fontsize=11, fontweight='bold', color='blue')\n",
    "ax7.set_xlabel('Time', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Weather Conditions', fontsize=14, fontweight='bold')\n",
    "ax7.legend(loc='upper left')\n",
    "ax7_twin.legend(loc='upper right')\n",
    "ax7.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Time series patterns visualized\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EDA COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60ad27f-0559-4ad4-a51c-daf8b8159e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: DATA PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Select final features (keep best performers, remove redundant)\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 2] Selecting final feature set...\")\n",
    "\n",
    "# Keep only essential features\n",
    "features_to_keep = [\n",
    "    # Load features\n",
    "    'Actual_Load', 'Forecasted_Load',\n",
    "    \n",
    "    # Load lags\n",
    "    'load_lag_1h', 'load_lag_24h',\n",
    "    \n",
    "    # Load derived\n",
    "    'load_rolling_mean_24h', 'load_rolling_std_24h',\n",
    "    'load_change_1h', 'load_change_24h',\n",
    "    'load_forecast_diff', 'load_forecast_ratio', 'load_forecast_error_pct',\n",
    "    'load_deviation_from_hourly_avg', 'load_deviation_from_daily_avg',\n",
    "    \n",
    "    # Weather features\n",
    "    'mean_temperature_c', 'mean_wind_speed', 'mean_ssrd',\n",
    "    'solar_forecast', 'wind_forecast',\n",
    "    'temp_lag_1h',\n",
    "    \n",
    "    # Weather derived\n",
    "    'load_per_temp', 'temp_load_product', 'is_very_cold', 'temp_extreme',\n",
    "    'wind_power_index',\n",
    "    \n",
    "    # Import features (past values only!)\n",
    "    'imports_lag_1h',\n",
    "    'imports_rolling_mean_24h',\n",
    "    \n",
    "    # Temporal features\n",
    "    'hour_sin', 'hour_cos',\n",
    "    'month_sin', 'month_cos',\n",
    "    'day_of_week_sin', 'day_of_week_cos',\n",
    "    'is_weekend', 'is_morning_peak', 'is_evening_peak', 'is_peak_hour',\n",
    "    \n",
    "    # Country\n",
    "    'country'\n",
    "]\n",
    "\n",
    "# Add any generation features that aren't too sparse\n",
    "generation_features = [f for f in feature_candidates \n",
    "                      if 'Actual_Aggregated' in f \n",
    "                      and train_df[f].isnull().sum() / len(train_df) < 0.80]\n",
    "\n",
    "final_features = features_to_keep + generation_features\n",
    "\n",
    "# Remove any that don't exist\n",
    "final_features = [f for f in final_features if f in train_df.columns]\n",
    "\n",
    "print(f\"  Selected {len(final_features)} features\")\n",
    "print(f\"    Core features: {len(features_to_keep)}\")\n",
    "print(f\"    Generation features: {len(generation_features)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare datasets\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 3] Preparing train/val/test datasets...\")\n",
    "\n",
    "X_train = train_df[final_features].copy()\n",
    "X_val = val_df[final_features].copy()\n",
    "X_test = test_df[final_features].copy()\n",
    "\n",
    "y_train = train_df['grid_stress_score'].copy()\n",
    "y_val = val_df['grid_stress_score'].copy()\n",
    "y_test = test_df['grid_stress_score'].copy()\n",
    "\n",
    "print(\"  Filling missing values with 0...\")\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "# One-hot encode country\n",
    "if 'country' in X_train.columns:\n",
    "    print(\"  One-hot encoding country...\")\n",
    "    X_train = pd.get_dummies(X_train, columns=['country'], prefix='country', drop_first=False)\n",
    "    X_val = pd.get_dummies(X_val, columns=['country'], prefix='country', drop_first=False)\n",
    "    X_test = pd.get_dummies(X_test, columns=['country'], prefix='country', drop_first=False)\n",
    "    \n",
    "    all_columns = X_train.columns\n",
    "    X_val = X_val.reindex(columns=all_columns, fill_value=0)\n",
    "    X_test = X_test.reindex(columns=all_columns, fill_value=0)\n",
    "\n",
    "print(f\"\\n‚úì Datasets prepared:\")\n",
    "print(f\"  X_train: {X_train.shape[0]:>8,} rows √ó {X_train.shape[1]:>3} features\")\n",
    "print(f\"  X_val:   {X_val.shape[0]:>8,} rows √ó {X_val.shape[1]:>3} features\")\n",
    "print(f\"  X_test:  {X_test.shape[0]:>8,} rows √ó {X_test.shape[1]:>3} features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Final verification - ensure no leakage\n",
    "# ============================================================================\n",
    "print(\"\\n[Step 4] Final data leakage verification...\")\n",
    "\n",
    "leakage_found = []\n",
    "\n",
    "# Check for prohibited features\n",
    "prohibited = ['net_imports', 'stress_lag', 'stress_change', 'reserve_margin_ml', \n",
    "              'forecast_load_error', 'load_rel_error']\n",
    "\n",
    "for col in X_train.columns:\n",
    "    for prob in prohibited:\n",
    "        if prob in col.lower():\n",
    "            leakage_found.append(col)\n",
    "            break\n",
    "\n",
    "if len(leakage_found) == 0:\n",
    "    print(\"  ‚úì No data leakage detected\")\n",
    "    print(\"  ‚úì No net_imports (used in T7/T8)\")\n",
    "    print(\"  ‚úì No stress_lag (target to predict target)\")\n",
    "    print(\"  ‚úì Model is production-ready\")\n",
    "else:\n",
    "    print(f\"  ‚ùå WARNING: Found {len(leakage_found)} suspicious features:\")\n",
    "    for feat in leakage_found:\n",
    "        print(f\"     - {feat}\")\n",
    "\n",
    "# Show feature categories\n",
    "print(f\"\\n[Step 5] Feature summary:\")\n",
    "load_feats = [f for f in X_train.columns if 'load' in f.lower() or 'Actual_Load' in f or 'Forecasted_Load' in f]\n",
    "weather_feats = [f for f in X_train.columns if any(x in f.lower() for x in ['temp', 'wind', 'solar', 'ssrd'])]\n",
    "temporal_feats = [f for f in X_train.columns if any(x in f for x in ['hour_', 'month_', 'day_of_week', 'weekend', 'peak'])]\n",
    "import_feats = [f for f in X_train.columns if 'import' in f.lower()]\n",
    "country_feats = [f for f in X_train.columns if 'country_' in f]\n",
    "generation_feats = [f for f in X_train.columns if 'Actual_Aggregated' in f]\n",
    "\n",
    "print(f\"  Load features:       {len(load_feats)}\")\n",
    "print(f\"  Weather features:    {len(weather_feats)}\")\n",
    "print(f\"  Temporal features:   {len(temporal_feats)}\")\n",
    "print(f\"  Import features:     {len(import_feats)}\")\n",
    "print(f\"  Generation features: {len(generation_feats)}\")\n",
    "print(f\"  Country indicators:  {len(country_feats)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660c1602-78a0-4f69-aadc-0874feb25def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 8: CLASSIFICATION MODELS + THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PART A: Train Dedicated Classification Models\n",
    "# ============================================================================\n",
    "print(\"\\n[PART A] Training dedicated classification models...\")\n",
    "print(\"Current approach: Using regression model + threshold\")\n",
    "print(\"New approach: Train models specifically for binary classification\\n\")\n",
    "\n",
    "# Create binary labels (threshold = 50 for training)\n",
    "TRAIN_THRESHOLD = 50\n",
    "y_train_binary = (y_train >= TRAIN_THRESHOLD).astype(int)\n",
    "y_val_binary = (y_val >= TRAIN_THRESHOLD).astype(int)\n",
    "y_test_binary = (y_test >= TRAIN_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Binary class distribution (Test Set):\")\n",
    "print(f\"  Low Risk (0):  {(y_test_binary == 0).sum():>6,} ({(y_test_binary == 0).sum()/len(y_test_binary)*100:.2f}%)\")\n",
    "print(f\"  High Risk (1): {(y_test_binary == 1).sum():>6,} ({(y_test_binary == 1).sum()/len(y_test_binary)*100:.2f}%)\")\n",
    "\n",
    "# Import classification models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ============================================================================\n",
    "# Define classification model suite (10 models)\n",
    "# ============================================================================\n",
    "classification_models = {\n",
    "    # Logistic Regression (2)\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Logistic Regression (balanced)': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    \n",
    "    # Decision Tree (2)\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(max_depth=15, random_state=42),\n",
    "    'Decision Tree (balanced)': DecisionTreeClassifier(max_depth=15, class_weight='balanced', random_state=42),\n",
    "    \n",
    "    # Random Forest (2)\n",
    "    'Random Forest Classifier': RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=20, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'Random Forest (balanced)': RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=20, class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    # Gradient Boosting (1)\n",
    "    'Gradient Boosting Classifier': GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=7, learning_rate=0.1, random_state=42\n",
    "    ),\n",
    "    \n",
    "    # XGBoost (2)\n",
    "    'XGBoost Classifier': XGBClassifier(\n",
    "        n_estimators=100, max_depth=7, learning_rate=0.1, \n",
    "        random_state=42, n_jobs=-1, eval_metric='logloss'\n",
    "    ),\n",
    "    'XGBoost (scale_pos_weight)': XGBClassifier(\n",
    "        n_estimators=100, max_depth=7, learning_rate=0.1, \n",
    "        scale_pos_weight=3,  # Give more weight to minority class\n",
    "        random_state=42, n_jobs=-1, eval_metric='logloss'\n",
    "    ),\n",
    "    \n",
    "    # LightGBM (1)\n",
    "    'LightGBM Classifier': lgb.LGBMClassifier(\n",
    "        n_estimators=100, max_depth=7, learning_rate=0.1,\n",
    "        random_state=42, n_jobs=-1, verbose=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"\\n Training {len(classification_models)} classification models...\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\\n\")\n",
    "\n",
    "clf_results = []\n",
    "\n",
    "print(f\"{'Model':<40} {'Time':>10} {'Accuracy':>10} {'Precision':>12} {'Recall':>10} {'F1':>10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for model_name, model in classification_models.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train_binary)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        acc = accuracy_score(y_val_binary, y_pred)\n",
    "        prec = precision_score(y_val_binary, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_val_binary, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val_binary, y_pred, zero_division=0)\n",
    "        \n",
    "        clf_results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Time': train_time,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1': f1,\n",
    "            'model_object': model\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name:<40} {train_time:>9.2f}s {acc:>10.4f} {prec:>12.4f} {rec:>10.4f} {f1:>10.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{model_name:<40} FAILED: {str(e)[:30]}\")\n",
    "\n",
    "# Find best classification model\n",
    "clf_results_df = pd.DataFrame(clf_results)\n",
    "best_clf_idx = clf_results_df['F1'].idxmax()\n",
    "best_clf_name = clf_results_df.loc[best_clf_idx, 'Model']\n",
    "best_clf_model = clf_results_df.loc[best_clf_idx, 'model_object']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(f\"BEST CLASSIFICATION MODEL: {best_clf_name}\")\n",
    "print(f\"  Validation Accuracy:  {clf_results_df.loc[best_clf_idx, 'Accuracy']:.4f}\")\n",
    "print(f\"  Validation Precision: {clf_results_df.loc[best_clf_idx, 'Precision']:.4f}\")\n",
    "print(f\"  Validation Recall:    {clf_results_df.loc[best_clf_idx, 'Recall']:.4f}\")\n",
    "print(f\"  Validation F1-Score:  {clf_results_df.loc[best_clf_idx, 'F1']:.4f}\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "# ============================================================================\n",
    "# Compare Classification Approach on TEST set (NO LIST)\n",
    "# ============================================================================\n",
    "print(\"\\n[PART C] Evaluating the best classification model on TEST set...\")\n",
    "\n",
    "# Predict on test set using the best classifier\n",
    "y_test_pred_clf = best_clf_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "acc = accuracy_score(y_test_binary, y_test_pred_clf)\n",
    "prec = precision_score(y_test_binary, y_test_pred_clf, zero_division=0)\n",
    "rec = recall_score(y_test_binary, y_test_pred_clf, zero_division=0)\n",
    "f1 = f1_score(y_test_binary, y_test_pred_clf, zero_division=0)\n",
    "\n",
    "print(f\"\\n{'Approach':<45} {'Accuracy':>10} {'Precision':>12} {'Recall':>10} {'F1':>10}\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{('Classification (' + best_clf_name + ')'):<45} \"\n",
    "      f\"{acc:>10.4f} {prec:>12.4f} {rec:>10.4f} {f1:>10.4f}\")\n",
    "\n",
    "# Store results in a single-row DataFrame\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Approach': f'Classification ({best_clf_name})',\n",
    "    'Accuracy': acc,\n",
    "    'Precision': prec,\n",
    "    'Recall': rec,\n",
    "    'F1': f1\n",
    "}])\n",
    "\n",
    "# This is the only approach, so it's automatically the best\n",
    "best_overall_approach = comparison_df.loc[0, 'Approach']\n",
    "best_overall_idx = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"üèÜ BEST OVERALL APPROACH: {best_overall_approach}\")\n",
    "print(f\"  Test F1-Score: {f1:.4f}\")\n",
    "print(f\"  Test Recall:   {rec:.4f}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# ============================================================================\n",
    "# Visualizations\n",
    "# ============================================================================\n",
    "print(\"\\n[PART D] Creating visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Plot 1: Classification Models Comparison (F1 Score)\n",
    "# ----------------------------------------------------------------------------\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "clf_sorted = clf_results_df.sort_values('F1', ascending=True)\n",
    "\n",
    "colors_clf = [\n",
    "    'darkgreen' if x == clf_results_df['F1'].max() else 'steelblue'\n",
    "    for x in clf_sorted['F1']\n",
    "]\n",
    "\n",
    "ax1.barh(\n",
    "    range(len(clf_sorted)),\n",
    "    clf_sorted['F1'],\n",
    "    color=colors_clf,\n",
    "    alpha=0.7,\n",
    "    edgecolor='black'\n",
    ")\n",
    "ax1.set_yticks(range(len(clf_sorted)))\n",
    "ax1.set_yticklabels(clf_sorted['Model'], fontsize=9)\n",
    "ax1.set_xlabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Classification Models - F1 Score', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Plot 2: Confusion Matrix for best classification model\n",
    "# ----------------------------------------------------------------------------\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_test_pred_clf)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues', cbar=False, square=True,\n",
    "    xticklabels=['Low', 'High'], yticklabels=['Low', 'High'],\n",
    "    ax=ax2, annot_kws={'size': 12, 'weight': 'bold'}\n",
    ")\n",
    "\n",
    "rec = recall_score(y_test_binary, y_test_pred_clf)\n",
    "f1 = f1_score(y_test_binary, y_test_pred_clf)\n",
    "\n",
    "ax2.set_title(\n",
    "    f'Confusion Matrix\\nBest: {best_clf_name}\\nRecall={rec:.3f}, F1={f1:.3f}',\n",
    "    fontsize=12, fontweight='bold'\n",
    ")\n",
    "ax2.set_xlabel('Predicted', fontsize=10, fontweight='bold')\n",
    "ax2.set_ylabel('Actual', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Figure title + layout\n",
    "# ----------------------------------------------------------------------------\n",
    "plt.suptitle('Classification Analysis ‚Äî Best Model Performance', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/best_classification_model_performance.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualizations created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ecd12b-4508-4a29-a54e-7b364169c8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create directory\n",
    "output_dir = \"/Workspace/Users/peter.ducati@gmail.com/grid_stress_classification\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open(f\"{output_dir}/xgboost_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_clf_model, f)\n",
    "\n",
    "# Save feature names\n",
    "with open(f\"{output_dir}/feature_names.pkl\", 'wb') as f:\n",
    "    pickle.dump(list(X_train.columns), f)\n",
    "\n",
    "# Save sample data\n",
    "spark_df = spark.createDataFrame(X_test)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"x_test_imputed_with_features\")\n",
    "\n",
    "# Save country stats\n",
    "stats = train_df.groupby('country').agg({\n",
    "    'Actual_Load': 'mean',\n",
    "    'net_imports': 'mean',\n",
    "    'mean_temperature_c': 'mean',\n",
    "    'grid_stress_score': 'mean'\n",
    "}).to_csv(f\"{output_dir}/country_stats.csv\")\n",
    "\n",
    "print(f\"‚úì Saved to: {output_dir}\")\n",
    "print(\"Files: xgboost_model.pkl, feature_names.pkl, sample_data.csv, country_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "grid_stress_classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
