{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdab83ee-2601-4abc-b6c5-2a23c3d859b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EUROPEAN POWER GRID STRESS PREDICTION - 6-TARGET SYSTEM\n",
    "# Cell 1: Environment Setup and Library Imports\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "# XGBoost is not included in Databricks by default, must install explicitly\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost --quiet\n",
    "\n",
    "# Core Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# File Management\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Settings\n",
    "# ============================================================================\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configure pandas display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "# ============================================================================\n",
    "# Verification\n",
    "# ============================================================================\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENVIRONMENT READY - PROCEED TO DATA LOADING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ac255a-4a4d-44fe-b0ae-3fa4dbb87802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Data Loading and Initial Exploration\n",
    "# ============================================================================\n",
    "\n",
    "# Load the three datasets from Databricks tables\n",
    "# These splits were created previously with temporal separation to prevent leakage\n",
    "print(\"\\nLoading datasets from Databricks tables...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "train_df = spark.table(\"workspace.default.train_set\").toPandas()\n",
    "val_df = spark.table(\"workspace.default.validation_set\").toPandas()\n",
    "test_df = spark.table(\"workspace.default.test_set\").toPandas()\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training set:   {len(train_df):>8,} records\")\n",
    "print(f\"  Validation set: {len(val_df):>8,} records\")\n",
    "print(f\"  Test set:       {len(test_df):>8,} records\")\n",
    "print(f\"  Total:          {len(train_df) + len(val_df) + len(test_df):>8,} records\")\n",
    "\n",
    "# Display column information\n",
    "print(f\"\\nNumber of columns: {len(train_df.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Display basic info about the training set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "missing_counts = train_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(train_df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\nColumns with missing values:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Check temporal coverage\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPORAL COVERAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert datetime columns if they exist\n",
    "if 'datetime' in train_df.columns:\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    val_df['datetime'] = pd.to_datetime(val_df['datetime'])\n",
    "    test_df['datetime'] = pd.to_datetime(test_df['datetime'])\n",
    "    \n",
    "    print(f\"\\nTraining set:   {train_df['datetime'].min()} to {train_df['datetime'].max()}\")\n",
    "    print(f\"Validation set: {val_df['datetime'].min()} to {val_df['datetime'].max()}\")\n",
    "    print(f\"Test set:       {test_df['datetime'].min()} to {test_df['datetime'].max()}\")\n",
    "\n",
    "# Check country distribution\n",
    "if 'country' in train_df.columns:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COUNTRY DISTRIBUTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    country_counts = train_df['country'].value_counts().sort_index()\n",
    "    print(f\"\\nNumber of countries: {len(country_counts)}\")\n",
    "    print(f\"\\nCountries: {sorted(train_df['country'].unique())}\")\n",
    "    print(f\"\\nRecords per country (training set):\")\n",
    "    print(country_counts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d605575a-5fe8-4ca2-986d-81a641c911dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Correlation Analysis and Key Feature Exploration\n",
    "# ============================================================================\n",
    "\n",
    "# Focus on the key columns we'll use for our 6-target system\n",
    "# We'll analyze correlations with grid_stress_score and between key features\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS - KEY FEATURES FOR 6-TARGET SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define key features for our analysis\n",
    "# These are the complete, non-missing columns most relevant to grid stress\n",
    "key_features = [\n",
    "    'Actual_Load',\n",
    "    'Forecasted_Load', \n",
    "    'net_imports',\n",
    "    'mean_temperature_c',\n",
    "    'mean_wind_speed',\n",
    "    'mean_ssrd',\n",
    "    'reserve_margin_ml',\n",
    "    'forecast_load_error',\n",
    "    'load_rel_error',\n",
    "    'grid_stress_score'\n",
    "]\n",
    "\n",
    "# Create subset with key features\n",
    "analysis_df = train_df[key_features].copy()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(key_features)} key features\")\n",
    "print(f\"Records: {len(analysis_df):,}\")\n",
    "print(\"\\nFeatures selected:\")\n",
    "for i, feat in enumerate(key_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CORRELATION MATRIX\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "correlation_matrix = analysis_df.corr()\n",
    "print(\"\\nFull correlation matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Focus on correlations with grid_stress_score\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CORRELATIONS WITH EXISTING GRID_STRESS_SCORE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "stress_correlations = correlation_matrix['grid_stress_score'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with grid_stress_score (sorted):\")\n",
    "for feature, corr in stress_correlations.items():\n",
    "    if feature != 'grid_stress_score':\n",
    "        print(f\"  {feature:<25} {corr:>7.4f}\")\n",
    "\n",
    "# Analyze the existing targets in the dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXISTING TARGET ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nExisting grid_stress_score distribution:\")\n",
    "print(train_df['grid_stress_score'].describe())\n",
    "\n",
    "print(\"\\nUnique stress score values:\")\n",
    "unique_scores = sorted(train_df['grid_stress_score'].unique())\n",
    "print(unique_scores)\n",
    "\n",
    "print(\"\\nDistribution of stress scores:\")\n",
    "score_dist = train_df['grid_stress_score'].value_counts().sort_index()\n",
    "for score, count in score_dist.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    print(f\"  Score {score:>5.1f}: {count:>7,} records ({pct:>5.2f}%)\")\n",
    "\n",
    "# Analyze existing binary targets\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXISTING BINARY TARGETS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nT7 (High Exports) distribution:\")\n",
    "print(f\"  T7 = 0: {(train_df['T7_high_exports']==0).sum():>7,} records ({(train_df['T7_high_exports']==0).mean()*100:>5.2f}%)\")\n",
    "print(f\"  T7 = 1: {(train_df['T7_high_exports']==1).sum():>7,} records ({(train_df['T7_high_exports']==1).mean()*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nT8 (High Imports) distribution:\")\n",
    "print(f\"  T8 = 0: {(train_df['T8_high_imports']==0).sum():>7,} records ({(train_df['T8_high_imports']==0).mean()*100:>5.2f}%)\")\n",
    "print(f\"  T8 = 1: {(train_df['T8_high_imports']==1).sum():>7,} records ({(train_df['T8_high_imports']==1).mean()*100:>5.2f}%)\")\n",
    "\n",
    "# Check P10/P90 thresholds for import/export\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"IMPORT/EXPORT THRESHOLDS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\nP10 (high exports threshold): {train_df['P10_net'].iloc[0]:,.2f} MW\")\n",
    "print(f\"P90 (high imports threshold): {train_df['P90_net'].iloc[0]:,.2f} MW\")\n",
    "\n",
    "print(\"\\nNet imports distribution:\")\n",
    "print(train_df['net_imports'].describe())\n",
    "\n",
    "# Visualize key relationships\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZING KEY RELATIONSHIPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap of all correlations\n",
    "import seaborn as sns\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Correlation Matrix - Key Features', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar plot of correlations with grid_stress_score\n",
    "stress_corr_plot = stress_correlations.drop('grid_stress_score')\n",
    "axes[1].barh(range(len(stress_corr_plot)), stress_corr_plot.values, color='steelblue')\n",
    "axes[1].set_yticks(range(len(stress_corr_plot)))\n",
    "axes[1].set_yticklabels(stress_corr_plot.index)\n",
    "axes[1].set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "axes[1].set_title('Correlation with Grid Stress Score', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d692821-f41f-4b89-ba6b-55e388e8173e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3B: Missing Values Analysis and Handling Strategy\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES ANALYSIS FOR 6-TARGET SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check missing values in key columns we'll use for our 6-target system\n",
    "key_columns_for_targets = [\n",
    "    'Actual_Load',\n",
    "    'Forecasted_Load',\n",
    "    'net_imports',\n",
    "    'mean_temperature_c',\n",
    "    'mean_wind_speed',\n",
    "    'mean_ssrd'\n",
    "]\n",
    "\n",
    "print(\"\\nMissing values in KEY columns for 6-target system:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for col in key_columns_for_targets:\n",
    "    missing_count = train_df[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(train_df)) * 100\n",
    "    print(f\"  {col:<30} {missing_count:>8,} ({missing_pct:>6.2f}%)\")\n",
    "\n",
    "# Check if we have complete records for target calculation\n",
    "complete_mask = train_df[key_columns_for_targets].notna().all(axis=1)\n",
    "complete_count = complete_mask.sum()\n",
    "complete_pct = (complete_count / len(train_df)) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Records with ALL key features complete: {complete_count:,} ({complete_pct:.2f}%)\")\n",
    "print(f\"Records with ANY missing values:        {(~complete_mask).sum():,} ({(~complete_mask).mean()*100:.2f}%)\")\n",
    "\n",
    "# Check generation columns (many have missing values)\n",
    "generation_cols = [col for col in train_df.columns if '__Actual_Aggregated' in col]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GENERATION COLUMNS ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nTotal generation columns: {len(generation_cols)}\")\n",
    "\n",
    "gen_missing = []\n",
    "for col in generation_cols[:10]:  # Show first 10\n",
    "    missing_count = train_df[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(train_df)) * 100\n",
    "    gen_missing.append((col, missing_count, missing_pct))\n",
    "    print(f\"  {col:<50} {missing_pct:>6.2f}% missing\")\n",
    "\n",
    "print(\"\\n  ... (showing first 10 of {}) ...\".format(len(generation_cols)))\n",
    "\n",
    "# STRATEGY DECISION\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MISSING VALUES HANDLING STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFor our 6-target stress score system:\")\n",
    "print(\"\\n1. REQUIRED COLUMNS (must be complete):\")\n",
    "print(\"   - Actual_Load, Forecasted_Load, net_imports\")\n",
    "print(\"   - These are ESSENTIAL for calculating all 6 targets\")\n",
    "print(\"   - Action: DROP rows with missing values in these columns\")\n",
    "\n",
    "print(\"\\n2. WEATHER COLUMNS (useful but not critical):\")\n",
    "print(\"   - mean_temperature_c, mean_wind_speed, mean_ssrd\")\n",
    "print(\"   - Used for feature engineering but not for targets\")\n",
    "print(\"   - Action: FILL missing values with median/mean per country\")\n",
    "\n",
    "print(\"\\n3. GENERATION COLUMNS (44-100% missing):\")\n",
    "print(\"   - Many generation columns have extensive missing data\")\n",
    "print(\"   - NOT required for our 6-target system\")\n",
    "print(\"   - Action: DO NOT USE for initial modeling\")\n",
    "\n",
    "# Apply cleaning to training data\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"APPLYING CLEANING STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check critical columns for targets\n",
    "critical_cols = ['Actual_Load', 'Forecasted_Load', 'net_imports']\n",
    "\n",
    "print(f\"\\nBefore cleaning: {len(train_df):,} records\")\n",
    "\n",
    "# Check missing in critical columns\n",
    "for col in critical_cols:\n",
    "    missing = train_df[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing:,} missing\")\n",
    "\n",
    "if train_df[critical_cols].isnull().any().any():\n",
    "    print(\"\\nWARNING: Found missing values in critical columns!\")\n",
    "    print(\"Action: Will drop these rows before target calculation\")\n",
    "    \n",
    "    clean_df = train_df.dropna(subset=critical_cols)\n",
    "    print(f\"\\nAfter dropping rows with missing critical values: {len(clean_df):,} records\")\n",
    "    print(f\"Dropped: {len(train_df) - len(clean_df):,} records ({(len(train_df) - len(clean_df))/len(train_df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nGOOD: No missing values in critical columns!\")\n",
    "    print(\"All records can be used for target calculation\")\n",
    "    clean_df = train_df.copy()\n",
    "\n",
    "# Check weather columns\n",
    "weather_cols = ['mean_temperature_c', 'mean_wind_speed', 'mean_ssrd']\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"WEATHER COLUMNS CHECK\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for col in weather_cols:\n",
    "    missing = clean_df[col].isnull().sum()\n",
    "    print(f\"  {col}: {missing:,} missing ({missing/len(clean_df)*100:.2f}%)\")\n",
    "    \n",
    "if clean_df[weather_cols].isnull().any().any():\n",
    "    print(\"\\nAction: Will fill weather missing values with country medians\")\n",
    "    print(\"        (This preserves country-specific patterns)\")\n",
    "else:\n",
    "    print(\"\\nGOOD: No missing values in weather columns!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFinal usable records: {len(clean_df):,}\")\n",
    "print(f\"Percentage retained: {len(clean_df)/len(train_df)*100:.2f}%\")\n",
    "print(\"\\nReady to proceed with 6-target system creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a635069-eb3a-46ba-9e36-7f2b6a4a9188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Build 6-Target Stress Score System\n",
    "# ============================================================================\n",
    "\n",
    "# Based on our project requirements, we'll create 6 operational targets\n",
    "# Each target represents a specific grid stress condition with weighted impact\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING 6-TARGET STRESS SCORE SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Work with a copy of training data\n",
    "df_targets = train_df.copy()\n",
    "\n",
    "# Calculate additional features needed for targets\n",
    "df_targets['forecast_error_pct'] = (\n",
    "    (df_targets['Actual_Load'] - df_targets['Forecasted_Load']).abs() / \n",
    "    df_targets['Forecasted_Load']\n",
    ")\n",
    "\n",
    "# Calculate percentiles for import/export thresholds\n",
    "p10_imports = df_targets['net_imports'].quantile(0.10)\n",
    "p90_imports = df_targets['net_imports'].quantile(0.90)\n",
    "p90_magnitude = df_targets['net_imports'].abs().quantile(0.90)\n",
    "\n",
    "print(\"\\nImport/Export Thresholds (from training data):\")\n",
    "print(f\"  P10 (high exports):      {p10_imports:>10,.2f} MW (negative = exporting)\")\n",
    "print(f\"  P90 (high imports):      {p90_imports:>10,.2f} MW (positive = importing)\")\n",
    "print(f\"  P90 magnitude (extreme): {p90_magnitude:>10,.2f} MW\")\n",
    "\n",
    "# ============================================================================\n",
    "# Define the 6 Targets\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"TARGET DEFINITIONS\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# TARGET 1: Large Forecast Error (>10%) - CRITICAL\n",
    "# When actual demand differs from forecast by more than 10%\n",
    "df_targets['T1_large_error'] = (df_targets['forecast_error_pct'] > 0.10).astype(int)\n",
    "\n",
    "print(\"\\nT1 - Large Forecast Error (>10%)\")\n",
    "print(f\"  Weight: 25 points\")\n",
    "print(f\"  Logic: |Actual_Load - Forecasted_Load| / Forecasted_Load > 0.10\")\n",
    "print(f\"  Triggered: {df_targets['T1_large_error'].sum():,} records ({df_targets['T1_large_error'].mean()*100:.2f}%)\")\n",
    "\n",
    "# TARGET 2: Medium Forecast Error (5-10%) - MODERATE\n",
    "# When forecast error is between 5% and 10%\n",
    "df_targets['T2_medium_error'] = (\n",
    "    (df_targets['forecast_error_pct'] > 0.05) & \n",
    "    (df_targets['forecast_error_pct'] <= 0.10)\n",
    ").astype(int)\n",
    "\n",
    "print(\"\\nT2 - Medium Forecast Error (5-10%)\")\n",
    "print(f\"  Weight: 10 points\")\n",
    "print(f\"  Logic: 0.05 < |Actual_Load - Forecasted_Load| / Forecasted_Load ≤ 0.10\")\n",
    "print(f\"  Triggered: {df_targets['T2_medium_error'].sum():,} records ({df_targets['T2_medium_error'].mean()*100:.2f}%)\")\n",
    "\n",
    "# TARGET 3: Underestimated Demand (>5% underforecast) - HIGH\n",
    "# When actual demand exceeds forecast by more than 5%\n",
    "df_targets['T3_underestimated'] = (\n",
    "    ((df_targets['Actual_Load'] - df_targets['Forecasted_Load']) / df_targets['Forecasted_Load'] > 0.05) &\n",
    "    (df_targets['Forecasted_Load'] < df_targets['Actual_Load'])\n",
    ").astype(int)\n",
    "\n",
    "print(\"\\nT3 - Underestimated Demand (>5% underforecast)\")\n",
    "print(f\"  Weight: 20 points\")\n",
    "print(f\"  Logic: (Actual_Load - Forecasted_Load) / Forecasted_Load > 0.05 AND Actual > Forecast\")\n",
    "print(f\"  Triggered: {df_targets['T3_underestimated'].sum():,} records ({df_targets['T3_underestimated'].mean()*100:.2f}%)\")\n",
    "\n",
    "# TARGET 7: High Exports (<P10) - MODERATE\n",
    "# When country is exporting heavily (negative net_imports below 10th percentile)\n",
    "df_targets['T7_high_exports_new'] = (df_targets['net_imports'] < p10_imports).astype(int)\n",
    "\n",
    "print(\"\\nT7 - High Exports (<P10)\")\n",
    "print(f\"  Weight: 10 points\")\n",
    "print(f\"  Logic: net_imports < P10 ({p10_imports:.2f} MW)\")\n",
    "print(f\"  Triggered: {df_targets['T7_high_exports_new'].sum():,} records ({df_targets['T7_high_exports_new'].mean()*100:.2f}%)\")\n",
    "\n",
    "# TARGET 8: High Imports (>P90) - HIGH\n",
    "# When country is importing heavily (positive net_imports above 90th percentile)\n",
    "df_targets['T8_high_imports_new'] = (df_targets['net_imports'] > p90_imports).astype(int)\n",
    "\n",
    "print(\"\\nT8 - High Imports (>P90)\")\n",
    "print(f\"  Weight: 20 points\")\n",
    "print(f\"  Logic: net_imports > P90 ({p90_imports:.2f} MW)\")\n",
    "print(f\"  Triggered: {df_targets['T8_high_imports_new'].sum():,} records ({df_targets['T8_high_imports_new'].mean()*100:.2f}%)\")\n",
    "\n",
    "# TARGET 9: Extreme Import/Export Flow (|imports| >P90) - HIGH\n",
    "# When absolute value of imports/exports exceeds 90th percentile (stress in either direction)\n",
    "df_targets['T9_extreme_flow'] = (df_targets['net_imports'].abs() > p90_magnitude).astype(int)\n",
    "\n",
    "print(\"\\nT9 - Extreme Import/Export Flow (|imports| >P90)\")\n",
    "print(f\"  Weight: 15 points\")\n",
    "print(f\"  Logic: |net_imports| > P90 ({p90_magnitude:.2f} MW)\")\n",
    "print(f\"  Triggered: {df_targets['T9_extreme_flow'].sum():,} records ({df_targets['T9_extreme_flow'].mean()*100:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate Our Stress Score\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CALCULATING 6-TARGET STRESS SCORE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_targets['stress_score_6target'] = (\n",
    "    df_targets['T1_large_error'] * 25 +\n",
    "    df_targets['T2_medium_error'] * 10 +\n",
    "    df_targets['T3_underestimated'] * 20 +\n",
    "    df_targets['T7_high_exports_new'] * 10 +\n",
    "    df_targets['T8_high_imports_new'] * 20 +\n",
    "    df_targets['T9_extreme_flow'] * 15\n",
    ")\n",
    "\n",
    "print(\"\\nFormula: stress_score = T1×25 + T2×10 + T3×20 + T7×10 + T8×20 + T9×15\")\n",
    "print(f\"Range: 0 to 100 points\")\n",
    "\n",
    "print(\"\\n6-Target Stress Score Distribution:\")\n",
    "print(df_targets['stress_score_6target'].describe())\n",
    "\n",
    "print(\"\\nUnique stress score values:\")\n",
    "unique_new_scores = sorted(df_targets['stress_score_6target'].unique())\n",
    "print(unique_new_scores[:20])  # Show first 20 values\n",
    "\n",
    "print(\"\\nDistribution of stress scores:\")\n",
    "new_score_dist = df_targets['stress_score_6target'].value_counts().sort_index()\n",
    "for score, count in new_score_dist.head(15).items():\n",
    "    pct = (count / len(df_targets)) * 100\n",
    "    print(f\"  Score {score:>3.0f}: {count:>7,} records ({pct:>5.2f}%)\")\n",
    "\n",
    "# Calculate blackout risk threshold\n",
    "# Recommend threshold at 30 points\n",
    "df_targets['blackout_risk_6target'] = (df_targets['stress_score_6target'] >= 30).astype(int)\n",
    "\n",
    "print(f\"\\nBlackout Risk Classification (threshold = 30 points):\")\n",
    "print(f\"  Normal (0-29):   {(df_targets['stress_score_6target'] < 30).sum():,} records ({(df_targets['stress_score_6target'] < 30).mean()*100:.2f}%)\")\n",
    "print(f\"  At Risk (≥30):   {(df_targets['stress_score_6target'] >= 30).sum():,} records ({(df_targets['stress_score_6target'] >= 30).mean()*100:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compare with Existing Grid Stress Score\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: OUR 6-TARGET vs EXISTING GRID_STRESS_SCORE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate correlation between the two systems\n",
    "correlation = df_targets['stress_score_6target'].corr(df_targets['grid_stress_score'])\n",
    "\n",
    "print(f\"\\nCorrelation between systems: {correlation:.4f}\")\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(f\"{'Metric':<30} {'Our 6-Target':<20} {'Existing Score':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Mean':<30} {df_targets['stress_score_6target'].mean():<20.2f} {df_targets['grid_stress_score'].mean():<20.2f}\")\n",
    "print(f\"{'Median':<30} {df_targets['stress_score_6target'].median():<20.2f} {df_targets['grid_stress_score'].median():<20.2f}\")\n",
    "print(f\"{'Std Dev':<30} {df_targets['stress_score_6target'].std():<20.2f} {df_targets['grid_stress_score'].std():<20.2f}\")\n",
    "print(f\"{'Min':<30} {df_targets['stress_score_6target'].min():<20.0f} {df_targets['grid_stress_score'].min():<20.0f}\")\n",
    "print(f\"{'Max':<30} {df_targets['stress_score_6target'].max():<20.0f} {df_targets['grid_stress_score'].max():<20.0f}\")\n",
    "print(f\"{'% Zero Stress':<30} {(df_targets['stress_score_6target']==0).mean()*100:<20.2f} {(df_targets['grid_stress_score']==0).mean()*100:<20.2f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Distribution comparison\n",
    "axes[0, 0].hist(df_targets['stress_score_6target'], bins=50, alpha=0.6, \n",
    "                label='Our 6-Target Score', color='blue', edgecolor='black')\n",
    "axes[0, 0].hist(df_targets['grid_stress_score'], bins=50, alpha=0.6, \n",
    "                label='Existing Score', color='red', edgecolor='black')\n",
    "axes[0, 0].axvline(30, color='orange', linestyle='--', linewidth=2, label='Blackout Threshold (30)')\n",
    "axes[0, 0].set_xlabel('Stress Score', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Score Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Our score vs Existing score\n",
    "axes[0, 1].scatter(df_targets['grid_stress_score'], df_targets['stress_score_6target'], \n",
    "                   alpha=0.1, s=1, color='steelblue')\n",
    "axes[0, 1].plot([0, 75], [0, 75], 'r--', linewidth=2, label='Perfect Agreement')\n",
    "axes[0, 1].set_xlabel('Existing Grid Stress Score', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Our 6-Target Stress Score', fontsize=12)\n",
    "axes[0, 1].set_title(f'Score Comparison (r={correlation:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Target contribution breakdown\n",
    "target_contributions = {\n",
    "    'T1 (Large Error)': df_targets['T1_large_error'].sum(),\n",
    "    'T2 (Medium Error)': df_targets['T2_medium_error'].sum(),\n",
    "    'T3 (Underestimated)': df_targets['T3_underestimated'].sum(),\n",
    "    'T7 (High Exports)': df_targets['T7_high_exports_new'].sum(),\n",
    "    'T8 (High Imports)': df_targets['T8_high_imports_new'].sum(),\n",
    "    'T9 (Extreme Flow)': df_targets['T9_extreme_flow'].sum()\n",
    "}\n",
    "\n",
    "axes[1, 0].barh(list(target_contributions.keys()), \n",
    "                [v/len(df_targets)*100 for v in target_contributions.values()],\n",
    "                color='steelblue', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Percentage of Records Triggered (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Target Trigger Rates', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Box plots comparison\n",
    "box_data = [df_targets['stress_score_6target'], df_targets['grid_stress_score']]\n",
    "axes[1, 1].boxplot(box_data, labels=['Our 6-Target', 'Existing Score'])\n",
    "axes[1, 1].set_ylabel('Stress Score', fontsize=12)\n",
    "axes[1, 1].set_title('Score Distribution (Box Plot)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"6-TARGET SYSTEM COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  1. Our system has correlation of {correlation:.4f} with existing score\")\n",
    "print(f\"  2. Our system is more selective: {(df_targets['stress_score_6target']==0).mean()*100:.1f}% normal vs {(df_targets['grid_stress_score']==0).mean()*100:.1f}%\")\n",
    "print(f\"  3. Blackout risk rate at threshold 30: {(df_targets['stress_score_6target'] >= 30).mean()*100:.1f}%\")\n",
    "print(\"\\nConclusion: Our 6-target system differs significantly from existing score.\")\n",
    "print(\"We will proceed with OUR system for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7ceaca-83f7-43ae-9d13-9478b7ad590b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Feature Engineering - Base Features and Lag Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING FOR 6-TARGET STRESS PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We'll engineer features on all three datasets\n",
    "# Start with training data to demonstrate, then apply to val and test\n",
    "\n",
    "def engineer_features(df, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set including base features and lag features.\n",
    "    Lag features capture temporal persistence in grid stress patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Initial records: {len(df):,}\")\n",
    "    \n",
    "    # Create working copy\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # Ensure datetime column exists and is properly formatted\n",
    "    if 'index' in df_feat.columns:\n",
    "        df_feat['datetime'] = pd.to_datetime(df_feat['index'])\n",
    "    elif 'datetime' not in df_feat.columns:\n",
    "        print(\"ERROR: No datetime column found!\")\n",
    "        return df_feat\n",
    "    \n",
    "    # Sort by country and datetime for proper lag calculation\n",
    "    df_feat = df_feat.sort_values(['country', 'datetime']).reset_index(drop=True)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BASE FEATURES (25 features)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nCreating base features...\")\n",
    "    \n",
    "    # Load-related features\n",
    "    df_feat['load_difference'] = df_feat['Actual_Load'] - df_feat['Forecasted_Load']\n",
    "    df_feat['load_forecast_ratio'] = df_feat['Actual_Load'] / df_feat['Forecasted_Load']\n",
    "    \n",
    "    # Import/export features\n",
    "    df_feat['import_magnitude'] = df_feat['net_imports'].abs()\n",
    "    df_feat['import_dependency_ratio'] = df_feat['net_imports'] / df_feat['Actual_Load']\n",
    "    df_feat['is_importing'] = (df_feat['net_imports'] > 0).astype(int)\n",
    "    df_feat['is_exporting'] = (df_feat['net_imports'] < 0).astype(int)\n",
    "    \n",
    "    # Weather interaction features\n",
    "    df_feat['temp_load_interaction'] = df_feat['mean_temperature_c'] * df_feat['Actual_Load']\n",
    "    df_feat['wind_load_interaction'] = df_feat['mean_wind_speed'] * df_feat['Actual_Load']\n",
    "    df_feat['solar_load_interaction'] = df_feat['mean_ssrd'] * df_feat['Actual_Load']\n",
    "    \n",
    "    # Temporal features - Cyclical encoding\n",
    "    df_feat['hour'] = df_feat['datetime'].dt.hour\n",
    "    df_feat['month'] = df_feat['datetime'].dt.month\n",
    "    df_feat['day_of_week'] = df_feat['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Convert to cyclical features (preserves circular nature of time)\n",
    "    df_feat['hour_sin'] = np.sin(2 * np.pi * df_feat['hour'] / 24)\n",
    "    df_feat['hour_cos'] = np.cos(2 * np.pi * df_feat['hour'] / 24)\n",
    "    df_feat['month_sin'] = np.sin(2 * np.pi * df_feat['month'] / 12)\n",
    "    df_feat['month_cos'] = np.cos(2 * np.pi * df_feat['month'] / 12)\n",
    "    df_feat['dow_sin'] = np.sin(2 * np.pi * df_feat['day_of_week'] / 7)\n",
    "    df_feat['dow_cos'] = np.cos(2 * np.pi * df_feat['day_of_week'] / 7)\n",
    "    \n",
    "    # Temporal features - Binary indicators\n",
    "    df_feat['is_weekend'] = (df_feat['day_of_week'] >= 5).astype(int)\n",
    "    df_feat['is_peak_hour'] = df_feat['hour'].isin([8, 9, 10, 18, 19, 20]).astype(int)\n",
    "    \n",
    "    print(f\"  Created 25 base features\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LAG FEATURES (18 features)\n",
    "    # Critical for capturing temporal persistence in grid stress\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nCreating lag features (per country to prevent leakage)...\")\n",
    "    \n",
    "    # Initialize lag feature columns\n",
    "    lag_features = [\n",
    "        'load_lag_1h', 'load_lag_3h', 'load_lag_6h', 'load_lag_24h',\n",
    "        'import_lag_1h', 'import_lag_3h', 'import_lag_24h',\n",
    "        'load_rolling_mean_6h', 'load_rolling_std_6h', 'load_rolling_mean_24h',\n",
    "        'import_rolling_mean_24h',\n",
    "        'load_change_1h', 'load_change_24h', 'import_change_1h'\n",
    "    ]\n",
    "    \n",
    "    for feat in lag_features:\n",
    "        df_feat[feat] = np.nan\n",
    "    \n",
    "    # Create lag features per country (prevents cross-country leakage)\n",
    "    for country in df_feat['country'].unique():\n",
    "        mask = df_feat['country'] == country\n",
    "        \n",
    "        # Load lags (past values)\n",
    "        df_feat.loc[mask, 'load_lag_1h'] = df_feat.loc[mask, 'Actual_Load'].shift(1)\n",
    "        df_feat.loc[mask, 'load_lag_3h'] = df_feat.loc[mask, 'Actual_Load'].shift(3)\n",
    "        df_feat.loc[mask, 'load_lag_6h'] = df_feat.loc[mask, 'Actual_Load'].shift(6)\n",
    "        df_feat.loc[mask, 'load_lag_24h'] = df_feat.loc[mask, 'Actual_Load'].shift(24)\n",
    "        \n",
    "        # Import lags\n",
    "        df_feat.loc[mask, 'import_lag_1h'] = df_feat.loc[mask, 'net_imports'].shift(1)\n",
    "        df_feat.loc[mask, 'import_lag_3h'] = df_feat.loc[mask, 'net_imports'].shift(3)\n",
    "        df_feat.loc[mask, 'import_lag_24h'] = df_feat.loc[mask, 'net_imports'].shift(24)\n",
    "        \n",
    "        # Rolling statistics (moving averages and std dev)\n",
    "        df_feat.loc[mask, 'load_rolling_mean_6h'] = (\n",
    "            df_feat.loc[mask, 'Actual_Load'].shift(1).rolling(6, min_periods=1).mean()\n",
    "        )\n",
    "        df_feat.loc[mask, 'load_rolling_std_6h'] = (\n",
    "            df_feat.loc[mask, 'Actual_Load'].shift(1).rolling(6, min_periods=1).std()\n",
    "        )\n",
    "        df_feat.loc[mask, 'load_rolling_mean_24h'] = (\n",
    "            df_feat.loc[mask, 'Actual_Load'].shift(1).rolling(24, min_periods=1).mean()\n",
    "        )\n",
    "        df_feat.loc[mask, 'import_rolling_mean_24h'] = (\n",
    "            df_feat.loc[mask, 'net_imports'].shift(1).rolling(24, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Change features (derivatives)\n",
    "        df_feat.loc[mask, 'load_change_1h'] = df_feat.loc[mask, 'Actual_Load'].diff(1)\n",
    "        df_feat.loc[mask, 'load_change_24h'] = df_feat.loc[mask, 'Actual_Load'].diff(24)\n",
    "        df_feat.loc[mask, 'import_change_1h'] = df_feat.loc[mask, 'net_imports'].diff(1)\n",
    "    \n",
    "    # Note: Stress lag features will be added after target creation\n",
    "    print(f\"  Created 14 lag features (4 more after targets)\")\n",
    "    \n",
    "    # Drop first 24 hours per country (insufficient history for lags)\n",
    "    initial_count = len(df_feat)\n",
    "    df_feat = df_feat.groupby('country').apply(lambda x: x.iloc[24:]).reset_index(drop=True)\n",
    "    dropped = initial_count - len(df_feat)\n",
    "    \n",
    "    print(f\"  Dropped {dropped:,} records (first 24h per country)\")\n",
    "    print(f\"  Final: {len(df_feat):,} records with complete lag features\")\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# ========================================================================\n",
    "# Apply feature engineering to all datasets\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_engineered = engineer_features(train_df, \"Training Set\")\n",
    "val_engineered = engineer_features(val_df, \"Validation Set\")\n",
    "test_engineered = engineer_features(test_df, \"Test Set\")\n",
    "\n",
    "# ========================================================================\n",
    "# Verify feature creation\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count features by category\n",
    "base_features = [\n",
    "    'Actual_Load', 'Forecasted_Load', 'load_rel_error', 'load_difference', 'load_forecast_ratio',\n",
    "    'net_imports', 'import_magnitude', 'import_dependency_ratio', 'is_importing', 'is_exporting',\n",
    "    'mean_ssrd', 'mean_wind_speed', 'mean_temperature_c',\n",
    "    'temp_load_interaction', 'wind_load_interaction', 'solar_load_interaction',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos',\n",
    "    'is_weekend', 'is_peak_hour',\n",
    "    'reserve_margin_ml'\n",
    "]\n",
    "\n",
    "lag_features = [\n",
    "    'load_lag_1h', 'load_lag_3h', 'load_lag_6h', 'load_lag_24h',\n",
    "    'import_lag_1h', 'import_lag_3h', 'import_lag_24h',\n",
    "    'load_rolling_mean_6h', 'load_rolling_std_6h', 'load_rolling_mean_24h',\n",
    "    'import_rolling_mean_24h',\n",
    "    'load_change_1h', 'load_change_24h', 'import_change_1h'\n",
    "]\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Base features:  {len(base_features)}\")\n",
    "print(f\"  Lag features:   {len(lag_features)} (4 stress lags to be added)\")\n",
    "print(f\"  Total so far:   {len(base_features) + len(lag_features)}\")\n",
    "\n",
    "# Check for missing values in engineered features\n",
    "print(f\"\\nMissing values in engineered features:\")\n",
    "missing_in_engineered = train_engineered[base_features + lag_features].isnull().sum()\n",
    "if missing_in_engineered.sum() > 0:\n",
    "    print(\"\\nFeatures with missing values:\")\n",
    "    for feat in missing_in_engineered[missing_in_engineered > 0].index:\n",
    "        count = missing_in_engineered[feat]\n",
    "        pct = (count / len(train_engineered)) * 100\n",
    "        print(f\"  {feat:<30} {count:>7,} ({pct:>5.2f}%)\")\n",
    "    \n",
    "    print(\"\\nAction: Filling missing values with 0 (safe for lag features)\")\n",
    "    for df in [train_engineered, val_engineered, test_engineered]:\n",
    "        df[base_features + lag_features] = df[base_features + lag_features].fillna(0)\n",
    "else:\n",
    "    print(\"  No missing values in engineered features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset sizes after feature engineering:\")\n",
    "print(f\"  Training:   {len(train_engineered):>8,} records\")\n",
    "print(f\"  Validation: {len(val_engineered):>8,} records\")\n",
    "print(f\"  Test:       {len(test_engineered):>8,} records\")\n",
    "print(f\"\\nNext step: Add 6-target system to engineered datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd75a8c-4636-416e-a06d-b5cf38317608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Add 6-Target System to Engineered Data + Stress Lag Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADDING 6-TARGET SYSTEM TO ENGINEERED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def add_6target_system(df, p10_threshold, p90_threshold, p90_magnitude_threshold, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Add 6-target stress score system to engineered dataset.\n",
    "    Then create stress-based lag features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Initial records: {len(df):,}\")\n",
    "    \n",
    "    # Create working copy\n",
    "    df_targets = df.copy()\n",
    "    \n",
    "    # Calculate forecast error percentage for targets\n",
    "    df_targets['forecast_error_pct'] = (\n",
    "        (df_targets['Actual_Load'] - df_targets['Forecasted_Load']).abs() / \n",
    "        df_targets['Forecasted_Load']\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE 6 TARGETS\n",
    "    # ========================================================================\n",
    "    \n",
    "    # T1: Large Forecast Error (>10%) - 25 points\n",
    "    df_targets['target_T1_large_error'] = (\n",
    "        df_targets['forecast_error_pct'] > 0.10\n",
    "    ).astype(int)\n",
    "    \n",
    "    # T2: Medium Forecast Error (5-10%) - 10 points\n",
    "    df_targets['target_T2_medium_error'] = (\n",
    "        (df_targets['forecast_error_pct'] > 0.05) & \n",
    "        (df_targets['forecast_error_pct'] <= 0.10)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # T3: Underestimated Demand (>5% underforecast) - 20 points\n",
    "    df_targets['target_T3_underestimated'] = (\n",
    "        ((df_targets['Actual_Load'] - df_targets['Forecasted_Load']) / \n",
    "         df_targets['Forecasted_Load'] > 0.05) &\n",
    "        (df_targets['Forecasted_Load'] < df_targets['Actual_Load'])\n",
    "    ).astype(int)\n",
    "    \n",
    "    # T7: High Exports (<P10) - 10 points\n",
    "    df_targets['target_T7_high_exports'] = (\n",
    "        df_targets['net_imports'] < p10_threshold\n",
    "    ).astype(int)\n",
    "    \n",
    "    # T8: High Imports (>P90) - 20 points\n",
    "    df_targets['target_T8_high_imports'] = (\n",
    "        df_targets['net_imports'] > p90_threshold\n",
    "    ).astype(int)\n",
    "    \n",
    "    # T9: Extreme Import/Export (|imports| >P90) - 15 points\n",
    "    df_targets['target_T9_extreme_flow'] = (\n",
    "        df_targets['net_imports'].abs() > p90_magnitude_threshold\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Calculate stress score\n",
    "    df_targets['stress_score'] = (\n",
    "        df_targets['target_T1_large_error'] * 25 +\n",
    "        df_targets['target_T2_medium_error'] * 10 +\n",
    "        df_targets['target_T3_underestimated'] * 20 +\n",
    "        df_targets['target_T7_high_exports'] * 10 +\n",
    "        df_targets['target_T8_high_imports'] * 20 +\n",
    "        df_targets['target_T9_extreme_flow'] * 15\n",
    "    )\n",
    "    \n",
    "    # Create blackout risk binary target (threshold = 30)\n",
    "    df_targets['blackout_risk'] = (df_targets['stress_score'] >= 30).astype(int)\n",
    "    \n",
    "    print(f\"\\n  Target occurrence rates:\")\n",
    "    print(f\"    T1 (Large Error):      {df_targets['target_T1_large_error'].mean()*100:5.2f}%\")\n",
    "    print(f\"    T2 (Medium Error):     {df_targets['target_T2_medium_error'].mean()*100:5.2f}%\")\n",
    "    print(f\"    T3 (Underestimated):   {df_targets['target_T3_underestimated'].mean()*100:5.2f}%\")\n",
    "    print(f\"    T7 (High Exports):     {df_targets['target_T7_high_exports'].mean()*100:5.2f}%\")\n",
    "    print(f\"    T8 (High Imports):     {df_targets['target_T8_high_imports'].mean()*100:5.2f}%\")\n",
    "    print(f\"    T9 (Extreme Flow):     {df_targets['target_T9_extreme_flow'].mean()*100:5.2f}%\")\n",
    "    \n",
    "    print(f\"\\n  Stress score statistics:\")\n",
    "    print(f\"    Mean:   {df_targets['stress_score'].mean():6.2f}\")\n",
    "    print(f\"    Median: {df_targets['stress_score'].median():6.2f}\")\n",
    "    print(f\"    Std:    {df_targets['stress_score'].std():6.2f}\")\n",
    "    print(f\"    Min:    {df_targets['stress_score'].min():6.0f}\")\n",
    "    print(f\"    Max:    {df_targets['stress_score'].max():6.0f}\")\n",
    "    \n",
    "    print(f\"\\n  Blackout risk (≥30 points): {df_targets['blackout_risk'].mean()*100:5.2f}%\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE STRESS LAG FEATURES (4 additional lag features)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n  Creating stress lag features...\")\n",
    "    \n",
    "    # Initialize stress lag columns\n",
    "    df_targets['stress_lag_1h'] = np.nan\n",
    "    df_targets['stress_lag_24h'] = np.nan\n",
    "    df_targets['stress_momentum'] = np.nan\n",
    "    df_targets['stress_trend_24h'] = np.nan\n",
    "    \n",
    "    # Create per country to prevent leakage\n",
    "    for country in df_targets['country'].unique():\n",
    "        mask = df_targets['country'] == country\n",
    "        \n",
    "        # Stress lags\n",
    "        df_targets.loc[mask, 'stress_lag_1h'] = df_targets.loc[mask, 'stress_score'].shift(1)\n",
    "        df_targets.loc[mask, 'stress_lag_24h'] = df_targets.loc[mask, 'stress_score'].shift(24)\n",
    "        \n",
    "        # Stress momentum (change from previous hour)\n",
    "        df_targets.loc[mask, 'stress_momentum'] = df_targets.loc[mask, 'stress_score'].diff(1)\n",
    "        \n",
    "        # Stress trend (change from 24h ago)\n",
    "        df_targets.loc[mask, 'stress_trend_24h'] = df_targets.loc[mask, 'stress_score'].diff(24)\n",
    "    \n",
    "    # Drop first row per country (stress lags create NaN in first row)\n",
    "    initial_count = len(df_targets)\n",
    "    df_targets = df_targets.groupby('country').apply(lambda x: x.iloc[1:]).reset_index(drop=True)\n",
    "    dropped = initial_count - len(df_targets)\n",
    "    \n",
    "    print(f\"  Dropped {dropped:,} records (first row per country for stress lags)\")\n",
    "    print(f\"  Final: {len(df_targets):,} records\")\n",
    "    \n",
    "    # Fill any remaining NaN in stress lag features with 0\n",
    "    stress_lag_cols = ['stress_lag_1h', 'stress_lag_24h', 'stress_momentum', 'stress_trend_24h']\n",
    "    df_targets[stress_lag_cols] = df_targets[stress_lag_cols].fillna(0)\n",
    "    \n",
    "    return df_targets\n",
    "\n",
    "# ========================================================================\n",
    "# Calculate thresholds from TRAINING data\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nCalculating import/export thresholds from training data...\")\n",
    "\n",
    "p10_imports = train_engineered['net_imports'].quantile(0.10)\n",
    "p90_imports = train_engineered['net_imports'].quantile(0.90)\n",
    "p90_magnitude = train_engineered['net_imports'].abs().quantile(0.90)\n",
    "\n",
    "print(f\"  P10 (high exports):      {p10_imports:>10,.2f} MW\")\n",
    "print(f\"  P90 (high imports):      {p90_imports:>10,.2f} MW\")\n",
    "print(f\"  P90 magnitude (extreme): {p90_magnitude:>10,.2f} MW\")\n",
    "\n",
    "print(\"\\nNote: Using training thresholds for all datasets to prevent data leakage\")\n",
    "\n",
    "# ========================================================================\n",
    "# Apply 6-target system to all datasets\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADDING TARGETS TO ALL DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_final = add_6target_system(\n",
    "    train_engineered, p10_imports, p90_imports, p90_magnitude, \"Training Set\"\n",
    ")\n",
    "\n",
    "val_final = add_6target_system(\n",
    "    val_engineered, p10_imports, p90_imports, p90_magnitude, \"Validation Set\"\n",
    ")\n",
    "\n",
    "test_final = add_6target_system(\n",
    "    test_engineered, p10_imports, p90_imports, p90_magnitude, \"Test Set\"\n",
    ")\n",
    "\n",
    "# ========================================================================\n",
    "# Final summary\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL FEATURE COUNT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define complete feature list\n",
    "FEATURE_COLUMNS = [\n",
    "    # Core load features (5)\n",
    "    'Actual_Load', 'Forecasted_Load', 'load_rel_error', 'load_difference', 'load_forecast_ratio',\n",
    "    \n",
    "    # Import/export features (5)\n",
    "    'net_imports', 'import_magnitude', 'import_dependency_ratio', 'is_importing', 'is_exporting',\n",
    "    \n",
    "    # Weather features (3)\n",
    "    'mean_ssrd', 'mean_wind_speed', 'mean_temperature_c',\n",
    "    \n",
    "    # Weather interactions (3)\n",
    "    'temp_load_interaction', 'wind_load_interaction', 'solar_load_interaction',\n",
    "    \n",
    "    # Temporal cyclical (6)\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos',\n",
    "    \n",
    "    # Temporal binary (2)\n",
    "    'is_weekend', 'is_peak_hour',\n",
    "    \n",
    "    # Reserve margin (1)\n",
    "    'reserve_margin_ml',\n",
    "    \n",
    "    # Lag features - Load (4)\n",
    "    'load_lag_1h', 'load_lag_3h', 'load_lag_6h', 'load_lag_24h',\n",
    "    \n",
    "    # Lag features - Imports (3)\n",
    "    'import_lag_1h', 'import_lag_3h', 'import_lag_24h',\n",
    "    \n",
    "    # Lag features - Stress (2)\n",
    "    'stress_lag_1h', 'stress_lag_24h',\n",
    "    \n",
    "    # Rolling statistics (4)\n",
    "    'load_rolling_mean_6h', 'load_rolling_std_6h', 'load_rolling_mean_24h', 'import_rolling_mean_24h',\n",
    "    \n",
    "    # Change features (5)\n",
    "    'load_change_1h', 'load_change_24h', 'import_change_1h', 'stress_momentum', 'stress_trend_24h'\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal feature count: {len(FEATURE_COLUMNS)}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Core load:          5\")\n",
    "print(f\"  Import/export:      5\")\n",
    "print(f\"  Weather:            3\")\n",
    "print(f\"  Weather interact:   3\")\n",
    "print(f\"  Temporal cyclical:  6\")\n",
    "print(f\"  Temporal binary:    2\")\n",
    "print(f\"  Reserve margin:     1\")\n",
    "print(f\"  Lag - Load:         4\")\n",
    "print(f\"  Lag - Imports:      3\")\n",
    "print(f\"  Lag - Stress:       2\")\n",
    "print(f\"  Rolling stats:      4\")\n",
    "print(f\"  Change features:    5\")\n",
    "print(f\"  \" + \"-\" * 30)\n",
    "print(f\"  TOTAL:             43\")\n",
    "\n",
    "print(f\"\\nTarget column: stress_score (range: 0-100)\")\n",
    "print(f\"Binary target: blackout_risk (threshold: ≥30)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET SIZES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Training:   {len(train_final):>8,} records\")\n",
    "print(f\"  Validation: {len(val_final):>8,} records\")\n",
    "print(f\"  Test:       {len(test_final):>8,} records\")\n",
    "print(f\"  Total:      {len(train_final) + len(val_final) + len(test_final):>8,} records\")\n",
    "\n",
    "# Verify no missing values in feature columns\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_train = train_final[FEATURE_COLUMNS].isnull().sum().sum()\n",
    "missing_val = val_final[FEATURE_COLUMNS].isnull().sum().sum()\n",
    "missing_test = test_final[FEATURE_COLUMNS].isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nMissing values in feature columns:\")\n",
    "print(f\"  Training:   {missing_train:,}\")\n",
    "print(f\"  Validation: {missing_val:,}\")\n",
    "print(f\"  Test:       {missing_test:,}\")\n",
    "\n",
    "if missing_train + missing_val + missing_test == 0:\n",
    "    print(\"\\nEXCELLENT: All feature columns are complete!\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Found missing values - will need to handle before modeling\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION COMPLETE - READY FOR MODELING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc4f7ec-dd71-49f9-9e8a-bb0fbe40076f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Data Cleaning and XGBoost Model Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING DATA AND TRAINING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# Step 1: Define features and prepare datasets\n",
    "# ========================================================================\n",
    "\n",
    "# Define the 43 feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    # Core load features (5)\n",
    "    'Actual_Load', 'Forecasted_Load', 'load_rel_error', 'load_difference', 'load_forecast_ratio',\n",
    "    # Import/export features (5)\n",
    "    'net_imports', 'import_magnitude', 'import_dependency_ratio', 'is_importing', 'is_exporting',\n",
    "    # Weather features (3)\n",
    "    'mean_ssrd', 'mean_wind_speed', 'mean_temperature_c',\n",
    "    # Weather interactions (3)\n",
    "    'temp_load_interaction', 'wind_load_interaction', 'solar_load_interaction',\n",
    "    # Temporal cyclical (6)\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos',\n",
    "    # Temporal binary (2)\n",
    "    'is_weekend', 'is_peak_hour',\n",
    "    # Reserve margin (1)\n",
    "    'reserve_margin_ml',\n",
    "    # Lag features - Load (4)\n",
    "    'load_lag_1h', 'load_lag_3h', 'load_lag_6h', 'load_lag_24h',\n",
    "    # Lag features - Imports (3)\n",
    "    'import_lag_1h', 'import_lag_3h', 'import_lag_24h',\n",
    "    # Lag features - Stress (2)\n",
    "    'stress_lag_1h', 'stress_lag_24h',\n",
    "    # Rolling statistics (4)\n",
    "    'load_rolling_mean_6h', 'load_rolling_std_6h', 'load_rolling_mean_24h', 'import_rolling_mean_24h',\n",
    "    # Change features (5)\n",
    "    'load_change_1h', 'load_change_24h', 'import_change_1h', 'stress_momentum', 'stress_trend_24h'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'stress_score'\n",
    "\n",
    "print(\"\\nPreparing feature matrices...\")\n",
    "print(f\"  Features: {len(FEATURE_COLUMNS)}\")\n",
    "print(f\"  Target: {TARGET_COLUMN}\")\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_final[FEATURE_COLUMNS].copy()\n",
    "y_train = train_final[TARGET_COLUMN].copy()\n",
    "\n",
    "X_val = val_final[FEATURE_COLUMNS].copy()\n",
    "y_val = val_final[TARGET_COLUMN].copy()\n",
    "\n",
    "X_test = test_final[FEATURE_COLUMNS].copy()\n",
    "y_test = test_final[TARGET_COLUMN].copy()\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 2: Data cleaning - Handle infinity and extreme values\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Data Cleaning\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def clean_features(X, dataset_name):\n",
    "    \"\"\"\n",
    "    Clean feature data by replacing infinity and extreme values.\n",
    "    This prevents XGBoost errors during training.\n",
    "    \"\"\"\n",
    "    X_clean = X.copy()\n",
    "    \n",
    "    # Replace infinity with NaN\n",
    "    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Count and fill NaN values with column median\n",
    "    nan_count = X_clean.isnull().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"  {dataset_name}: Filling {nan_count} NaN/inf values with column medians\")\n",
    "        for col in X_clean.columns:\n",
    "            if X_clean[col].isnull().any():\n",
    "                median_val = X_clean[col].median()\n",
    "                X_clean[col] = X_clean[col].fillna(median_val)\n",
    "    \n",
    "    return X_clean\n",
    "\n",
    "print(\"\\nCleaning datasets...\")\n",
    "X_train_clean = clean_features(X_train, \"Training\")\n",
    "X_val_clean = clean_features(X_val, \"Validation\")\n",
    "X_test_clean = clean_features(X_test, \"Test\")\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nVerification:\")\n",
    "for name, X in [(\"Train\", X_train_clean), (\"Val\", X_val_clean), (\"Test\", X_test_clean)]:\n",
    "    inf_count = np.isinf(X.values).sum()\n",
    "    nan_count = X.isnull().sum().sum()\n",
    "    print(f\"  {name}: Inf={inf_count}, NaN={nan_count}\")\n",
    "\n",
    "print(\"Data cleaning complete\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 3: Train XGBoost Regressor\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING XGBOOST REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configure model with hyperparameters to prevent overfitting\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,           # Number of boosting rounds\n",
    "    max_depth=6,                # Maximum tree depth\n",
    "    learning_rate=0.1,          # Step size shrinkage\n",
    "    subsample=0.8,              # Row sampling per tree\n",
    "    colsample_bytree=0.8,       # Column sampling per tree\n",
    "    min_child_weight=3,         # Minimum sum of weights in child\n",
    "    random_state=42,            # Reproducibility\n",
    "    n_jobs=-1,                  # Use all CPU cores\n",
    "    tree_method='hist',         # Fast histogram algorithm\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "print(\"\\nModel configuration:\")\n",
    "print(f\"  Algorithm:        XGBoost Regressor\")\n",
    "print(f\"  n_estimators:     {model.n_estimators}\")\n",
    "print(f\"  max_depth:        {model.max_depth}\")\n",
    "print(f\"  learning_rate:    {model.learning_rate}\")\n",
    "print(f\"  subsample:        {model.subsample}\")\n",
    "print(f\"  colsample_bytree: {model.colsample_bytree}\")\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with validation monitoring\n",
    "model.fit(\n",
    "    X_train_clean, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_clean, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training complete in {training_time:.1f} seconds\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 4: Generate predictions\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Generating Predictions\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "y_train_pred = model.predict(X_train_clean)\n",
    "y_val_pred = model.predict(X_val_clean)\n",
    "y_test_pred = model.predict(X_test_clean)\n",
    "\n",
    "print(\"Predictions generated for all datasets\")\n",
    "\n",
    "# ========================================================================\n",
    "# Step 5: Evaluate model performance\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Calculate regression metrics\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "# Evaluate all datasets\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"Training\")\n",
    "val_metrics = evaluate_model(y_val, y_val_pred, \"Validation\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"Test\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Dataset':<15} {'MAE':<12} {'RMSE':<12} {'R²':<12}\")\n",
    "print(\"-\" * 52)\n",
    "for m in [train_metrics, val_metrics, test_metrics]:\n",
    "    print(f\"{m['dataset']:<15} {m['mae']:<12.4f} {m['rmse']:<12.4f} {m['r2']:<12.6f}\")\n",
    "\n",
    "# Analyze generalization\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Overfitting Analysis\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "mae_diff = abs(train_metrics['mae'] - test_metrics['mae'])\n",
    "r2_diff = abs(train_metrics['r2'] - test_metrics['r2'])\n",
    "\n",
    "print(f\"\\nTrain vs Test:\")\n",
    "print(f\"  MAE difference:  {mae_diff:.4f} points\")\n",
    "print(f\"  R² difference:   {r2_diff:.6f}\")\n",
    "\n",
    "if mae_diff < 1.0 and r2_diff < 0.05:\n",
    "    print(\"\\nConclusion: Excellent generalization - no overfitting detected\")\n",
    "elif mae_diff < 2.0 and r2_diff < 0.10:\n",
    "    print(\"\\nConclusion: Good generalization - minimal overfitting\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Possible overfitting - significant train/test gap\")\n",
    "\n",
    "# ========================================================================\n",
    "# Summary\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel Details:\")\n",
    "print(f\"  Training samples:  {len(X_train_clean):,}\")\n",
    "print(f\"  Feature count:     {len(FEATURE_COLUMNS)}\")\n",
    "print(f\"  Target:            {TARGET_COLUMN} (0-100 range)\")\n",
    "print(f\"  Training time:     {training_time:.1f} seconds\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  MAE:  {test_metrics['mae']:.4f} points\")\n",
    "print(f\"  RMSE: {test_metrics['rmse']:.4f} points\")\n",
    "print(f\"  R²:   {test_metrics['r2']:.6f}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Feature importance analysis\")\n",
    "print(\"  - Visualization of predictions\")\n",
    "print(\"  - Model export for deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97fa420-4985-4e0f-9182-08910debc9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Feature Importance Analysis and Visualizations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================================================================\n",
    "# Extract and analyze feature importance\n",
    "# ========================================================================\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importance_values = model.feature_importances_\n",
    "\n",
    "# Create dataframe for analysis\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': FEATURE_COLUMNS,\n",
    "    'importance': feature_importance_values\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Rank':<6} {'Feature':<35} {'Importance':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in feature_importance_df.head(20).iterrows():\n",
    "    rank = feature_importance_df.index.get_loc(idx) + 1\n",
    "    print(f\"{rank:<6} {row['feature']:<35} {row['importance']:<12.6f}\")\n",
    "\n",
    "# Categorize features by type\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Feature Importance by Category\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "feature_categories = {\n",
    "    'Stress Lag': ['stress_lag_1h', 'stress_lag_24h', 'stress_momentum', 'stress_trend_24h'],\n",
    "    'Load Lag': ['load_lag_1h', 'load_lag_3h', 'load_lag_6h', 'load_lag_24h'],\n",
    "    'Import Lag': ['import_lag_1h', 'import_lag_3h', 'import_lag_24h'],\n",
    "    'Core Load': ['Actual_Load', 'Forecasted_Load', 'load_rel_error', 'load_difference', 'load_forecast_ratio'],\n",
    "    'Import/Export': ['net_imports', 'import_magnitude', 'import_dependency_ratio', 'is_importing', 'is_exporting'],\n",
    "    'Rolling Stats': ['load_rolling_mean_6h', 'load_rolling_std_6h', 'load_rolling_mean_24h', 'import_rolling_mean_24h'],\n",
    "    'Temporal': ['hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'is_weekend', 'is_peak_hour'],\n",
    "    'Weather': ['mean_ssrd', 'mean_wind_speed', 'mean_temperature_c', 'temp_load_interaction', 'wind_load_interaction', 'solar_load_interaction'],\n",
    "    'Other': ['reserve_margin_ml', 'load_change_1h', 'load_change_24h', 'import_change_1h']\n",
    "}\n",
    "\n",
    "category_importance = {}\n",
    "for category, features in feature_categories.items():\n",
    "    total_importance = feature_importance_df[feature_importance_df['feature'].isin(features)]['importance'].sum()\n",
    "    category_importance[category] = total_importance\n",
    "\n",
    "# Sort and display\n",
    "category_df = pd.DataFrame(list(category_importance.items()), \n",
    "                          columns=['Category', 'Total_Importance']).sort_values('Total_Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Category':<20} {'Total Importance':<20} {'Percentage':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for _, row in category_df.iterrows():\n",
    "    pct = (row['Total_Importance'] / feature_importance_values.sum()) * 100\n",
    "    print(f\"{row['Category']:<20} {row['Total_Importance']:<20.6f} {pct:<15.2f}%\")\n",
    "\n",
    "# ========================================================================\n",
    "# Visualizations\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Top 15 Feature Importances\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "top_15 = feature_importance_df.head(15)\n",
    "ax1.barh(range(len(top_15)), top_15['importance'], color='steelblue', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15['feature'])\n",
    "ax1.set_xlabel('Importance Score', fontsize=11)\n",
    "ax1.set_title('Top 15 Most Important Features', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Feature Importance by Category\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax2.barh(range(len(category_df)), category_df['Total_Importance'], color='coral', edgecolor='black')\n",
    "ax2.set_yticks(range(len(category_df)))\n",
    "ax2.set_yticklabels(category_df['Category'])\n",
    "ax2.set_xlabel('Total Importance', fontsize=11)\n",
    "ax2.set_title('Importance by Category', fontsize=13, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Actual vs Predicted (Test Set)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.scatter(y_test, y_test_pred, alpha=0.3, s=1, color='steelblue')\n",
    "ax3.plot([0, 80], [0, 80], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax3.set_xlabel('Actual Stress Score', fontsize=11)\n",
    "ax3.set_ylabel('Predicted Stress Score', fontsize=11)\n",
    "ax3.set_title(f'Actual vs Predicted - Test Set\\n(R²={test_metrics[\"r2\"]:.6f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "residuals_test = y_test - y_test_pred\n",
    "ax4.hist(residuals_test, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax4.set_xlabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "ax4.set_ylabel('Frequency', fontsize=11)\n",
    "ax4.set_title(f'Residuals Distribution - Test Set\\nMean={residuals_test.mean():.4f}, Std={residuals_test.std():.4f}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residuals vs Predicted\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax5.scatter(y_test_pred, residuals_test, alpha=0.3, s=1, color='steelblue')\n",
    "ax5.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax5.set_xlabel('Predicted Stress Score', fontsize=11)\n",
    "ax5.set_ylabel('Residual', fontsize=11)\n",
    "ax5.set_title('Residual Plot - Test Set', fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Prediction Distribution Comparison\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "ax6.hist(y_test, bins=30, alpha=0.6, label='Actual', color='blue', edgecolor='black')\n",
    "ax6.hist(y_test_pred, bins=30, alpha=0.6, label='Predicted', color='red', edgecolor='black')\n",
    "ax6.axvline(30, color='orange', linestyle='--', linewidth=2, label='Blackout Threshold')\n",
    "ax6.set_xlabel('Stress Score', fontsize=11)\n",
    "ax6.set_ylabel('Frequency', fontsize=11)\n",
    "ax6.set_title('Distribution: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Performance Across Datasets\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "datasets = ['Train', 'Val', 'Test']\n",
    "mae_values = [train_metrics['mae'], val_metrics['mae'], test_metrics['mae']]\n",
    "rmse_values = [train_metrics['rmse'], val_metrics['rmse'], test_metrics['rmse']]\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "\n",
    "ax7.bar(x - width/2, mae_values, width, label='MAE', color='steelblue', edgecolor='black')\n",
    "ax7.bar(x + width/2, rmse_values, width, label='RMSE', color='coral', edgecolor='black')\n",
    "ax7.set_ylabel('Error (points)', fontsize=11)\n",
    "ax7.set_title('Performance Across Datasets', fontsize=12, fontweight='bold')\n",
    "ax7.set_xticks(x)\n",
    "ax7.set_xticklabels(datasets)\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 8. R² Comparison\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "r2_values = [train_metrics['r2'], val_metrics['r2'], test_metrics['r2']]\n",
    "bars = ax8.bar(datasets, r2_values, color='green', edgecolor='black', alpha=0.7)\n",
    "ax8.set_ylabel('R² Score', fontsize=11)\n",
    "ax8.set_title('R² Score Comparison', fontsize=12, fontweight='bold')\n",
    "ax8.set_ylim([0.999, 1.0])\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, r2_values):\n",
    "    height = bar.get_height()\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.6f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('European Power Grid Stress Prediction - Model Analysis', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualizations complete\")\n",
    "\n",
    "# ========================================================================\n",
    "# Key Insights\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Model Performance:\")\n",
    "print(f\"   - Achieved R² of {test_metrics['r2']:.6f} on test set\")\n",
    "print(f\"   - Mean prediction error: {test_metrics['mae']:.4f} points\")\n",
    "print(f\"   - Excellent generalization (no overfitting)\")\n",
    "\n",
    "print(\"\\n2. Most Important Features:\")\n",
    "top_5 = feature_importance_df.head(5)\n",
    "for idx, row in top_5.iterrows():\n",
    "    rank = feature_importance_df.index.get_loc(idx) + 1\n",
    "    print(f\"   {rank}. {row['feature']} ({row['importance']:.6f})\")\n",
    "\n",
    "print(\"\\n3. Feature Categories:\")\n",
    "print(f\"   - Most important: {category_df.iloc[0]['Category']} ({category_df.iloc[0]['Total_Importance']:.4f})\")\n",
    "print(f\"   - Second: {category_df.iloc[1]['Category']} ({category_df.iloc[1]['Total_Importance']:.4f})\")\n",
    "\n",
    "print(\"\\n4. Prediction Quality:\")\n",
    "print(f\"   - Residuals centered at {residuals_test.mean():.4f} (near zero)\")\n",
    "print(f\"   - Residual std: {residuals_test.std():.4f} points\")\n",
    "print(f\"   - Model captures both low and high stress events accurately\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902c147b-e708-41c7-beab-e8461c859ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "CELL 9: SAVE MODEL AND ARTIFACTS FOR STREAMLIT\n",
    "==============================================================================\n",
    "Save the trained model and all necessary files for deployment\n",
    "==============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING MODEL FOR STREAMLIT DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# 9.1: CREATE SAVE DIRECTORY\n",
    "# ============================================================\n",
    "print(\"\\n[9.1 CREATING SAVE DIRECTORY]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "save_dir = '/tmp/grid_stress_final_model'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Directory created: {save_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9.2: SAVE THE TRAINED MODEL\n",
    "# ============================================================\n",
    "print(\"\\n[9.2 SAVING TRAINED MODEL]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# The model should be named something like xgb_regressor or xgb_regressor_6target\n",
    "# Let's check what model variables exist\n",
    "model_names = ['xgb_regressor_6target', 'xgb_regressor', 'model', 'final_model']\n",
    "model_to_save = None\n",
    "\n",
    "for name in model_names:\n",
    "    try:\n",
    "        model_to_save = eval(name)\n",
    "        print(f\"Found model: {name}\")\n",
    "        break\n",
    "    except NameError:\n",
    "        continue\n",
    "\n",
    "if model_to_save is not None:\n",
    "    model_path = f'{save_dir}/grid_stress_model.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model_to_save, f)\n",
    "    \n",
    "    file_size = os.path.getsize(model_path) / (1024*1024)\n",
    "    print(f\"✓ Model saved: {model_path}\")\n",
    "    print(f\"  Size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"✗ Model not found in memory\")\n",
    "    print(\"Available variables:\", [v for v in dir() if 'model' in v.lower()])\n",
    "\n",
    "# ============================================================\n",
    "# 9.3: SAVE FEATURE CONFIGURATION\n",
    "# ============================================================\n",
    "print(\"\\n[9.3 SAVING FEATURE CONFIGURATION]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Check if available_features exists\n",
    "try:\n",
    "    feature_list = available_features\n",
    "    print(f\"Found {len(feature_list)} features\")\n",
    "except NameError:\n",
    "    # If not, try to get from X_train columns\n",
    "    try:\n",
    "        feature_list = X_train.columns.tolist()\n",
    "        print(f\"Retrieved {len(feature_list)} features from X_train\")\n",
    "    except:\n",
    "        print(\"✗ Could not find feature list\")\n",
    "        feature_list = []\n",
    "\n",
    "if feature_list:\n",
    "    feature_config = {\n",
    "        'feature_names': feature_list,\n",
    "        'n_features': len(feature_list),\n",
    "        'model_type': 'XGBoost Regressor',\n",
    "        'target': 'stress_score (0-100)',\n",
    "        'blackout_threshold': 30,\n",
    "        'training_date': '2025-11-25'\n",
    "    }\n",
    "    \n",
    "    config_path = f'{save_dir}/feature_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(feature_config, f, indent=2)\n",
    "    print(f\"✓ Feature config saved: {config_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9.4: SAVE TARGET DEFINITIONS\n",
    "# ============================================================\n",
    "print(\"\\n[9.4 SAVING TARGET DEFINITIONS]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "target_definitions = {\n",
    "    'targets': [\n",
    "        {'id': 'T1', 'name': 'Large Forecast Error', 'weight': 25, 'threshold': '>10%'},\n",
    "        {'id': 'T2', 'name': 'Medium Forecast Error', 'weight': 10, 'threshold': '5-10%'},\n",
    "        {'id': 'T3', 'name': 'Underestimated Demand', 'weight': 20, 'threshold': '>5%'},\n",
    "        {'id': 'T7', 'name': 'High Exports', 'weight': 10, 'threshold': '<P10'},\n",
    "        {'id': 'T8', 'name': 'High Imports', 'weight': 20, 'threshold': '>P90'},\n",
    "        {'id': 'T9', 'name': 'Extreme Import/Export', 'weight': 15, 'threshold': '>P90 abs'}\n",
    "    ],\n",
    "    'max_score': 100,\n",
    "    'blackout_threshold': 30,\n",
    "    'categories': {\n",
    "        'NORMAL': '0-29',\n",
    "        'WARNING': '30-59',\n",
    "        'CRITICAL': '60-100'\n",
    "    }\n",
    "}\n",
    "\n",
    "targets_path = f'{save_dir}/target_definitions.json'\n",
    "with open(targets_path, 'w') as f:\n",
    "    json.dump(target_definitions, f, indent=2)\n",
    "print(f\"✓ Target definitions saved: {targets_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9.5: SAVE PERFORMANCE METRICS\n",
    "# ============================================================\n",
    "print(\"\\n[9.5 SAVING PERFORMANCE METRICS]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "performance = {\n",
    "    'test_set': {\n",
    "        'r2_score': 0.999878,\n",
    "        'mae': 0.0247,\n",
    "        'rmse': 0.1670\n",
    "    },\n",
    "    'training_info': {\n",
    "        'training_samples': 385973,\n",
    "        'validation_samples': 111142,\n",
    "        'test_samples': 53047,\n",
    "        'features_used': len(feature_list) if feature_list else 43\n",
    "    }\n",
    "}\n",
    "\n",
    "perf_path = f'{save_dir}/model_performance.json'\n",
    "with open(perf_path, 'w') as f:\n",
    "    json.dump(performance, f, indent=2)\n",
    "print(f\"✓ Performance metrics saved: {perf_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9.6: PRINT JSON FILES FOR COPY-PASTE\n",
    "# ============================================================\n",
    "print(\"\\n[9.6 FILE CONTENTS FOR STREAMLIT]\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- target_definitions.json ---\")\n",
    "print(json.dumps(target_definitions, indent=2))\n",
    "\n",
    "print(\"\\n--- model_performance.json ---\")\n",
    "print(json.dumps(performance, indent=2))\n",
    "\n",
    "# ============================================================\n",
    "# 9.7: SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n[9.7 FILES SAVED]\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAll files saved to: {save_dir}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  1. grid_stress_model.pkl\")\n",
    "print(\"  2. feature_config.json\")\n",
    "print(\"  3. target_definitions.json\")\n",
    "print(\"  4. model_performance.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SAVED - READY FOR STREAMLIT!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "NEXT STEP: Start NEW conversation for Streamlit with this info:\n",
    "\n",
    "\"I have a trained European power grid stress predictor:\n",
    "- Model: XGBoost Regressor (R² = 0.9999, MAE = 0.02)\n",
    "- Input: 43 features (load, imports, weather, lag features)\n",
    "- Output: Stress score 0-100\n",
    "- 6-target system: T1-T9 with weights 25/20/15/10\n",
    "\n",
    "Need Streamlit dashboard with interactive sliders.\n",
    "Help me build it!\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9381c8e-13d1-40cf-ae60-0d5ed88ad854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "CELL 10: SAVE MODEL FILES TO WORKSPACE (VISIBLE IN FILE BROWSER)\n",
    "==============================================================================\n",
    "Save model and configs to your workspace directory for easy access\n",
    "==============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING MODEL TO WORKSPACE DIRECTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# 10.1: DETERMINE CURRENT NOTEBOOK DIRECTORY\n",
    "# ============================================================\n",
    "print(\"\\n[10.1 FINDING NOTEBOOK DIRECTORY]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get current notebook path\n",
    "try:\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    notebook_dir = '/Workspace' + '/'.join(notebook_path.split('/')[:-1])\n",
    "    print(f\"Notebook directory: {notebook_dir}\")\n",
    "except:\n",
    "    # Fallback to manual path based on your screenshot\n",
    "    notebook_dir = '/Workspace/Repos/peterbranch/energy-grid-load-processing/energy_grid_load_processing'\n",
    "    print(f\"Using manual path: {notebook_dir}\")\n",
    "\n",
    "# Create model_files subdirectory\n",
    "save_dir = f'{notebook_dir}/model_files'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"✓ Save directory created: {save_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10.2: SAVE MODEL\n",
    "# ============================================================\n",
    "print(\"\\n[10.2 SAVING MODEL]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "model_path = f'{save_dir}/grid_stress_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "file_size = os.path.getsize(model_path) / (1024*1024)\n",
    "print(f\"✓ Model saved: grid_stress_model.pkl\")\n",
    "print(f\"  Size: {file_size:.2f} MB\")\n",
    "print(f\"  Location: {model_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10.3: SAVE FEATURE NAMES\n",
    "# ============================================================\n",
    "print(\"\\n[10.3 SAVING FEATURE CONFIGURATION]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "feature_config = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'model_type': 'XGBoost Regressor',\n",
    "    'target': 'stress_score (0-100)',\n",
    "    'blackout_threshold': 30\n",
    "}\n",
    "\n",
    "config_path = f'{save_dir}/feature_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(f\"✓ Feature config saved: feature_config.json\")\n",
    "\n",
    "# ============================================================\n",
    "# 10.4: SAVE TARGET DEFINITIONS\n",
    "# ============================================================\n",
    "print(\"\\n[10.4 SAVING TARGET DEFINITIONS]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "target_definitions = {\n",
    "    'targets': [\n",
    "        {'id': 'T1', 'name': 'Large Forecast Error', 'weight': 25, 'threshold': '>10%'},\n",
    "        {'id': 'T2', 'name': 'Medium Forecast Error', 'weight': 10, 'threshold': '5-10%'},\n",
    "        {'id': 'T3', 'name': 'Underestimated Demand', 'weight': 20, 'threshold': '>5%'},\n",
    "        {'id': 'T7', 'name': 'High Exports', 'weight': 10, 'threshold': '<P10'},\n",
    "        {'id': 'T8', 'name': 'High Imports', 'weight': 20, 'threshold': '>P90'},\n",
    "        {'id': 'T9', 'name': 'Extreme Import/Export', 'weight': 15, 'threshold': '>P90 abs'}\n",
    "    ],\n",
    "    'max_score': 100,\n",
    "    'blackout_threshold': 30,\n",
    "    'categories': {\n",
    "        'NORMAL': '0-29',\n",
    "        'WARNING': '30-59',\n",
    "        'CRITICAL': '60-100'\n",
    "    }\n",
    "}\n",
    "\n",
    "targets_path = f'{save_dir}/target_definitions.json'\n",
    "with open(targets_path, 'w') as f:\n",
    "    json.dump(target_definitions, f, indent=2)\n",
    "print(f\"✓ Target definitions saved: target_definitions.json\")\n",
    "\n",
    "# ============================================================\n",
    "# 10.5: SAVE PERFORMANCE METRICS\n",
    "# ============================================================\n",
    "print(\"\\n[10.5 SAVING PERFORMANCE METRICS]\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "performance = {\n",
    "    'test_set': {\n",
    "        'r2_score': 0.999878,\n",
    "        'mae': 0.0247,\n",
    "        'rmse': 0.1670\n",
    "    },\n",
    "    'training_info': {\n",
    "        'training_samples': 385973,\n",
    "        'validation_samples': 111142,\n",
    "        'test_samples': 53047,\n",
    "        'features_used': 43\n",
    "    }\n",
    "}\n",
    "\n",
    "perf_path = f'{save_dir}/model_performance.json'\n",
    "with open(perf_path, 'w') as f:\n",
    "    json.dump(performance, f, indent=2)\n",
    "print(f\"✓ Performance metrics saved: model_performance.json\")\n",
    "\n",
    "# ============================================================\n",
    "# 10.6: LIST ALL SAVED FILES\n",
    "# ============================================================\n",
    "print(\"\\n[10.6 FILES SAVED]\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ All files saved to: {save_dir}\")\n",
    "print(\"\\nFiles created (visible in file browser):\")\n",
    "print(\"  1. grid_stress_model.pkl        (0.84 MB)\")\n",
    "print(\"  2. feature_config.json\")\n",
    "print(\"  3. target_definitions.json\")\n",
    "print(\"  4. model_performance.json\")\n",
    "\n",
    "print(\"\\nTO VIEW FILES:\")\n",
    "print(f\"1. In left sidebar, navigate to: {save_dir}\")\n",
    "print(\"2. You'll see a 'model_files' folder\")\n",
    "print(\"3. Right-click any file → Download\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILES SAVED TO WORKSPACE - VISIBLE IN FILE BROWSER!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify files exist\n",
    "print(\"\\nVerifying files...\")\n",
    "for filename in ['grid_stress_model.pkl', 'feature_config.json', 'target_definitions.json', 'model_performance.json']:\n",
    "    filepath = f'{save_dir}/{filename}'\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  ✓ {filename}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {filename} - NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR STREAMLIT!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "grid_stress_6target_final_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
