{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "effa382b-3301-4a5a-a2c7-1e1c6943ffa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "CATALOG = \"curlybyte_solutions_rawdata_europe_grid_load\"\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"Catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9c5f7a-c7b3-4b39-88a4-0ebb354d3e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Discover all tables in each schema\n",
    "schemas = [\"european_grid_raw\", \"european_grid_raw__v2\", \"european_weather_raw\"]\n",
    "\n",
    "for schema in schemas:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Schema: {schema}\")\n",
    "    print('='*60)\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {CATALOG}.{schema}\").collect()\n",
    "    for table in tables:\n",
    "        print(f\"  - {table.tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c559f7d-864d-4515-9292-12aefe352505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect column names and types for each table in v2 schema\n",
    "tables_to_inspect = [\n",
    "    \"load_actual\", \n",
    "    \"load_forecast\", \n",
    "    \"generation\", \n",
    "    \"installed_capacity\", \n",
    "    \"crossborder_flows\",\n",
    "    \"solar_forecast\",\n",
    "    \"wind_forecast\"\n",
    "]\n",
    "\n",
    "for table_name in tables_to_inspect:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print('='*60)\n",
    "    df = spark.table(f\"{CATALOG}.european_grid_raw__v2.{table_name}\")\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(f\"Row count: {df.count():,}\")\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd25f10-9227-4da2-af60-f25aa8db6e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check temporal coverage and countries\n",
    "load_actual = spark.table(f\"{CATALOG}.european_grid_raw__v2.load_actual\")\n",
    "\n",
    "# Date range\n",
    "date_stats = load_actual.agg(\n",
    "    F.min(\"index\").alias(\"earliest\"),\n",
    "    F.max(\"index\").alias(\"latest\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"TEMPORAL COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Earliest: {date_stats['earliest']}\")\n",
    "print(f\"Latest:   {date_stats['latest']}\")\n",
    "\n",
    "# Check if April 28, 2025 (Spain/Portugal blackout) is in range\n",
    "blackout_date = \"2025-04-28\"\n",
    "print(f\"\\nSpain/Portugal blackout date ({blackout_date}): \", end=\"\")\n",
    "if date_stats['latest'] and str(date_stats['latest']) >= blackout_date:\n",
    "    print(\"IN RANGE - can validate against real event\")\n",
    "else:\n",
    "    print(\"NOT IN RANGE\")\n",
    "\n",
    "# Countries\n",
    "print(\"\\n\\nCOUNTRIES COVERED\")\n",
    "print(\"=\"*60)\n",
    "countries = load_actual.select(\"country\").distinct().orderBy(\"country\").collect()\n",
    "print(f\"Total countries: {len(countries)}\")\n",
    "for c in countries:\n",
    "    print(f\"  - {c['country']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f501a8-7772-42fa-bdae-0e684521a995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load core tables into DataFrames\n",
    "load_actual = spark.table(f\"{CATALOG}.european_grid_raw__v2.load_actual\")\n",
    "load_forecast = spark.table(f\"{CATALOG}.european_grid_raw__v2.load_forecast\")\n",
    "generation = spark.table(f\"{CATALOG}.european_grid_raw__v2.generation\")\n",
    "crossborder = spark.table(f\"{CATALOG}.european_grid_raw__v2.crossborder_flows\")\n",
    "\n",
    "# Also check weather data structure\n",
    "weather = spark.table(f\"{CATALOG}.european_weather_raw.weather_hourly\")\n",
    "print(\"Weather table columns:\")\n",
    "print(weather.columns)\n",
    "print(f\"\\nWeather row count: {weather.count():,}\")\n",
    "weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b264cf49-8d29-4c80-809c-1e8583a16c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive NaN Analysis for all core tables\n",
    "\n",
    "def analyze_nulls(df, table_name):\n",
    "    \"\"\"Analyze null values for each column in a dataframe\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NULL ANALYSIS: {table_name}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    print(f\"Total rows: {total_rows:,}\\n\")\n",
    "    \n",
    "    # Calculate null counts and percentages\n",
    "    null_stats = []\n",
    "    for col in df.columns:\n",
    "        null_count = df.filter(F.col(col).isNull()).count()\n",
    "        null_pct = (null_count / total_rows) * 100\n",
    "        if null_pct > 0:\n",
    "            null_stats.append((col, null_count, null_pct))\n",
    "    \n",
    "    if null_stats:\n",
    "        # Sort by null percentage descending\n",
    "        null_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "        print(f\"{'Column':<55} {'Nulls':>12} {'Pct':>8}\")\n",
    "        print(\"-\"*75)\n",
    "        for col, count, pct in null_stats:\n",
    "            print(f\"{col:<55} {count:>12,} {pct:>7.2f}%\")\n",
    "    else:\n",
    "        print(\"No null values found\")\n",
    "    \n",
    "    return null_stats\n",
    "\n",
    "# Analyze each table\n",
    "null_load_actual = analyze_nulls(load_actual, \"load_actual\")\n",
    "null_load_forecast = analyze_nulls(load_forecast, \"load_forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0edc30-a564-4ffb-bef0-3c335062bab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generation table null analysis - this one has many columns\n",
    "print(\"=\"*70)\n",
    "print(\"NULL ANALYSIS: generation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_rows = generation.count()\n",
    "print(f\"Total rows: {total_rows:,}\\n\")\n",
    "\n",
    "# Get only the Actual_Aggregated columns (main generation sources)\n",
    "gen_cols = [c for c in generation.columns if 'Actual_Aggregated' in c]\n",
    "\n",
    "null_stats = []\n",
    "for col in gen_cols:\n",
    "    null_count = generation.filter(F.col(col).isNull()).count()\n",
    "    null_pct = (null_count / total_rows) * 100\n",
    "    null_stats.append((col.replace('__Actual_Aggregated', ''), null_count, null_pct))\n",
    "\n",
    "# Sort by null percentage descending\n",
    "null_stats.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"{'Generation Source':<45} {'Nulls':>12} {'Pct':>8}\")\n",
    "print(\"-\"*70)\n",
    "for col, count, pct in null_stats:\n",
    "    print(f\"{col:<45} {count:>12,} {pct:>7.2f}%\")\n",
    "\n",
    "# Also check crossborder\n",
    "print(\"\\n\")\n",
    "null_crossborder = analyze_nulls(crossborder, \"crossborder_flows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7c7685-88a1-4613-b240-2900b4d064ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check which countries report generation data well\n",
    "gen_by_country = generation.groupBy(\"country\").agg(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.count(\"Wind_Onshore__Actual_Aggregated\").alias(\"wind_onshore_count\"),\n",
    "    F.count(\"Solar__Actual_Aggregated\").alias(\"solar_count\"),\n",
    "    F.count(\"Fossil_Gas__Actual_Aggregated\").alias(\"gas_count\"),\n",
    "    F.count(\"Nuclear__Actual_Aggregated\").alias(\"nuclear_count\")\n",
    ").orderBy(\"country\")\n",
    "\n",
    "gen_by_country_pd = gen_by_country.toPandas()\n",
    "gen_by_country_pd['wind_pct'] = (gen_by_country_pd['wind_onshore_count'] / gen_by_country_pd['total_rows'] * 100).round(1)\n",
    "gen_by_country_pd['solar_pct'] = (gen_by_country_pd['solar_count'] / gen_by_country_pd['total_rows'] * 100).round(1)\n",
    "gen_by_country_pd['gas_pct'] = (gen_by_country_pd['gas_count'] / gen_by_country_pd['total_rows'] * 100).round(1)\n",
    "gen_by_country_pd['nuclear_pct'] = (gen_by_country_pd['nuclear_count'] / gen_by_country_pd['total_rows'] * 100).round(1)\n",
    "\n",
    "print(\"GENERATION DATA COVERAGE BY COUNTRY (% non-null)\")\n",
    "print(\"=\"*70)\n",
    "print(gen_by_country_pd[['country', 'total_rows', 'wind_pct', 'solar_pct', 'gas_pct', 'nuclear_pct']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d51cb96-9410-4fdf-85ae-54b239cb9c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare total generation vs actual load to understand the gap\n",
    "# First, create total generation column\n",
    "\n",
    "# Get generation aggregated columns only\n",
    "gen_agg_cols = [c for c in generation.columns if c.endswith('__Actual_Aggregated')]\n",
    "\n",
    "# Calculate total generation per row\n",
    "generation_with_total = generation.withColumn(\n",
    "    \"total_generation\",\n",
    "    sum([F.coalesce(F.col(c), F.lit(0)) for c in gen_agg_cols])\n",
    ")\n",
    "\n",
    "# Join with load_actual\n",
    "load_gen_compare = load_actual.alias(\"l\").join(\n",
    "    generation_with_total.select(\"index\", \"country\", \"total_generation\").alias(\"g\"),\n",
    "    (F.col(\"l.index\") == F.col(\"g.index\")) & (F.col(\"l.country\") == F.col(\"g.country\")),\n",
    "    \"left\"\n",
    ").select(\n",
    "    F.col(\"l.index\").alias(\"timestamp\"),\n",
    "    F.col(\"l.country\"),\n",
    "    F.col(\"l.Actual_Load\"),\n",
    "    F.col(\"g.total_generation\")\n",
    ")\n",
    "\n",
    "# Aggregate by country\n",
    "gap_by_country = load_gen_compare.groupBy(\"country\").agg(\n",
    "    F.avg(\"Actual_Load\").alias(\"avg_load\"),\n",
    "    F.avg(\"total_generation\").alias(\"avg_generation\"),\n",
    "    F.count(\"*\").alias(\"rows\")\n",
    ").withColumn(\n",
    "    \"generation_pct_of_load\", \n",
    "    F.round(F.col(\"avg_generation\") / F.col(\"avg_load\") * 100, 1)\n",
    ").orderBy(\"country\")\n",
    "\n",
    "print(\"LOAD vs GENERATION BY COUNTRY\")\n",
    "print(\"=\"*70)\n",
    "print(\"(generation_pct < 100 means incomplete generation reporting)\\n\")\n",
    "display(gap_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbeeddc-70df-4457-8d2b-ab8bcabcab86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Investigate the April 28, 2025 Spain/Portugal blackout\n",
    "# The blackout occurred at 12:33 PM CEST\n",
    "\n",
    "blackout_start = \"2025-04-28 10:00:00\"  # A few hours before\n",
    "blackout_end = \"2025-04-28 18:00:00\"    # A few hours after\n",
    "\n",
    "# Filter for ES and PT around the blackout\n",
    "es_pt_blackout = load_actual.filter(\n",
    "    (F.col(\"country\").isin([\"ES\", \"PT\"])) &\n",
    "    (F.col(\"index\") >= blackout_start) &\n",
    "    (F.col(\"index\") <= blackout_end)\n",
    ").orderBy(\"index\", \"country\")\n",
    "\n",
    "print(\"SPAIN & PORTUGAL - APRIL 28, 2025 (BLACKOUT DAY)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Blackout reported at 12:33 PM CEST\\n\")\n",
    "\n",
    "display(es_pt_blackout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ac8de0-8262-4884-acdf-c9f674fcc0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare blackout day to a normal day (one week before)\n",
    "normal_day = \"2025-04-21\"  # Monday, one week before\n",
    "\n",
    "# Get both days for ES and PT\n",
    "comparison = load_actual.filter(\n",
    "    (F.col(\"country\").isin([\"ES\", \"PT\"])) &\n",
    "    (\n",
    "        (F.to_date(\"index\") == \"2025-04-28\") |  # Blackout day\n",
    "        (F.to_date(\"index\") == normal_day)       # Normal day\n",
    "    )\n",
    ").withColumn(\n",
    "    \"date\", F.to_date(\"index\")\n",
    ").withColumn(\n",
    "    \"hour\", F.hour(\"index\")\n",
    ")\n",
    "\n",
    "# Aggregate by date, hour, country\n",
    "hourly_comparison = comparison.groupBy(\"date\", \"hour\", \"country\").agg(\n",
    "    F.avg(\"Actual_Load\").alias(\"avg_load\")\n",
    ").orderBy(\"country\", \"date\", \"hour\")\n",
    "\n",
    "# Pivot for easier comparison\n",
    "comparison_pd = hourly_comparison.toPandas()\n",
    "\n",
    "print(\"BLACKOUT DAY vs NORMAL DAY (Hourly Average Load in MW)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for country in [\"ES\", \"PT\"]:\n",
    "    print(f\"\\n{country}:\")\n",
    "    country_data = comparison_pd[comparison_pd['country'] == country].pivot(\n",
    "        index='hour', columns='date', values='avg_load'\n",
    "    )\n",
    "    print(country_data.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83363391-47be-43f1-bc79-53424ad5fd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build unified dataset joining load_actual with load_forecast\n",
    "# This gives us the forecast error - a key feature\n",
    "\n",
    "unified = load_actual.alias(\"la\").join(\n",
    "    load_forecast.alias(\"lf\"),\n",
    "    (F.col(\"la.index\") == F.col(\"lf.index\")) & \n",
    "    (F.col(\"la.country\") == F.col(\"lf.country\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"la.index\").alias(\"timestamp\"),\n",
    "    F.col(\"la.country\"),\n",
    "    F.col(\"la.Actual_Load\").alias(\"actual_load\"),\n",
    "    F.col(\"lf.Forecasted_Load\").alias(\"forecast_load\")\n",
    ").withColumn(\n",
    "    \"forecast_error\", F.col(\"actual_load\") - F.col(\"forecast_load\")\n",
    ").withColumn(\n",
    "    \"forecast_error_pct\", \n",
    "    (F.col(\"forecast_error\") / F.col(\"forecast_load\")) * 100\n",
    ")\n",
    "\n",
    "print(f\"Unified dataset rows: {unified.count():,}\")\n",
    "print(\"\\nSample:\")\n",
    "display(unified.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ab29b8-712a-434f-9ff5-f2cffa4fa679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate net imports for each country at each timestamp\n",
    "# Net imports = total incoming flows - total outgoing flows\n",
    "\n",
    "# Incoming flows (where country is the destination)\n",
    "imports_df = crossborder.groupBy(\"index\", \"to_country\").agg(\n",
    "    F.sum(\"Value\").alias(\"total_imports\")\n",
    ").withColumnRenamed(\"to_country\", \"country\")\n",
    "\n",
    "# Outgoing flows (where country is the source)\n",
    "exports_df = crossborder.groupBy(\"index\", \"from_country\").agg(\n",
    "    F.sum(\"Value\").alias(\"total_exports\")\n",
    ").withColumnRenamed(\"from_country\", \"country\")\n",
    "\n",
    "# Join imports and exports\n",
    "net_flows = imports_df.alias(\"i\").join(\n",
    "    exports_df.alias(\"e\"),\n",
    "    (F.col(\"i.index\") == F.col(\"e.index\")) & \n",
    "    (F.col(\"i.country\") == F.col(\"e.country\")),\n",
    "    \"outer\"\n",
    ").select(\n",
    "    F.coalesce(F.col(\"i.index\"), F.col(\"e.index\")).alias(\"index\"),\n",
    "    F.coalesce(F.col(\"i.country\"), F.col(\"e.country\")).alias(\"country\"),\n",
    "    F.coalesce(F.col(\"i.total_imports\"), F.lit(0)).alias(\"total_imports\"),\n",
    "    F.coalesce(F.col(\"e.total_exports\"), F.lit(0)).alias(\"total_exports\")\n",
    ").withColumn(\n",
    "    \"net_imports\", F.col(\"total_imports\") - F.col(\"total_exports\")\n",
    ")\n",
    "\n",
    "print(f\"Net flows dataset rows: {net_flows.count():,}\")\n",
    "print(\"\\nSample:\")\n",
    "display(net_flows.orderBy(\"index\", \"country\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b46c8e6-7e5f-4755-832b-c3e75b4b9c08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rebuild unified with safe division\n",
    "unified_safe = load_actual.alias(\"la\").join(\n",
    "    load_forecast.alias(\"lf\"),\n",
    "    (F.col(\"la.index\") == F.col(\"lf.index\")) & \n",
    "    (F.col(\"la.country\") == F.col(\"lf.country\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"la.index\").alias(\"timestamp\"),\n",
    "    F.col(\"la.country\"),\n",
    "    F.col(\"la.Actual_Load\").alias(\"actual_load\"),\n",
    "    F.col(\"lf.Forecasted_Load\").alias(\"forecast_load\")\n",
    ").withColumn(\n",
    "    \"forecast_error\", F.col(\"actual_load\") - F.col(\"forecast_load\")\n",
    ").withColumn(\n",
    "    \"forecast_error_pct\", \n",
    "    F.coalesce(F.try_divide(F.col(\"forecast_error\"), F.col(\"forecast_load\")) * 100, F.lit(0))\n",
    ")\n",
    "\n",
    "# Join with net_flows\n",
    "complete_df = unified_safe.alias(\"u\").join(\n",
    "    net_flows.alias(\"n\"),\n",
    "    (F.col(\"u.timestamp\") == F.col(\"n.index\")) & \n",
    "    (F.col(\"u.country\") == F.col(\"n.country\")),\n",
    "    \"left\"\n",
    ").select(\n",
    "    F.col(\"u.timestamp\"),\n",
    "    F.col(\"u.country\"),\n",
    "    F.col(\"u.actual_load\"),\n",
    "    F.col(\"u.forecast_load\"),\n",
    "    F.col(\"u.forecast_error\"),\n",
    "    F.col(\"u.forecast_error_pct\"),\n",
    "    F.coalesce(F.col(\"n.total_imports\"), F.lit(0)).alias(\"total_imports\"),\n",
    "    F.coalesce(F.col(\"n.total_exports\"), F.lit(0)).alias(\"total_exports\"),\n",
    "    F.coalesce(F.col(\"n.net_imports\"), F.lit(0)).alias(\"net_imports\")\n",
    ")\n",
    "\n",
    "# Add time features\n",
    "complete_df = complete_df.withColumn(\"hour\", F.hour(\"timestamp\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\")) \\\n",
    "    .withColumn(\"month\", F.month(\"timestamp\")) \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.dayofweek(\"timestamp\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# Calculate import dependency ratio\n",
    "complete_df = complete_df.withColumn(\n",
    "    \"import_ratio\", \n",
    "    F.coalesce(F.try_divide(F.col(\"net_imports\"), F.col(\"actual_load\")), F.lit(0))\n",
    ")\n",
    "\n",
    "print(f\"Complete dataset rows: {complete_df.count():,}\")\n",
    "print(\"\\nSchema:\")\n",
    "complete_df.printSchema()\n",
    "print(\"\\nSample:\")\n",
    "display(complete_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3b54e9-1f6b-4129-950d-04ab4d52106b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for detailed EDA (sample if needed for performance)\n",
    "# Take a stratified sample to keep it manageable\n",
    "complete_pd = complete_df.sample(fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "print(f\"Sample size for EDA: {len(complete_pd):,} rows\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Key numeric columns\n",
    "numeric_cols = ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct', \n",
    "                'total_imports', 'total_exports', 'net_imports', 'import_ratio']\n",
    "\n",
    "print(complete_pd[numeric_cols].describe().round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0041de-2ad0-46c9-a41b-632da74a8662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Distribution plots for key variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Actual Load distribution\n",
    "axes[0, 0].hist(complete_pd['actual_load'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Actual Load Distribution')\n",
    "axes[0, 0].set_xlabel('MW')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Forecast Error distribution\n",
    "axes[0, 1].hist(complete_pd['forecast_error'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Forecast Error Distribution')\n",
    "axes[0, 1].set_xlabel('MW')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Forecast Error Percentage (clipped for visibility)\n",
    "error_pct_clipped = complete_pd['forecast_error_pct'].clip(-50, 50)\n",
    "axes[0, 2].hist(error_pct_clipped, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].set_title('Forecast Error % (clipped to +/-50%)')\n",
    "axes[0, 2].set_xlabel('%')\n",
    "axes[0, 2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Net Imports distribution\n",
    "axes[1, 0].hist(complete_pd['net_imports'], bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 0].set_title('Net Imports Distribution')\n",
    "axes[1, 0].set_xlabel('MW (positive = importing)')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Import Ratio (clipped)\n",
    "import_ratio_clipped = complete_pd['import_ratio'].clip(-1, 1)\n",
    "axes[1, 1].hist(import_ratio_clipped, bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
    "axes[1, 1].set_title('Import Ratio (clipped to +/-1)')\n",
    "axes[1, 1].set_xlabel('Ratio (net imports / load)')\n",
    "\n",
    "# Load by hour of day\n",
    "hourly_load = complete_pd.groupby('hour')['actual_load'].mean()\n",
    "axes[1, 2].bar(hourly_load.index, hourly_load.values, color='steelblue', edgecolor='black')\n",
    "axes[1, 2].set_title('Average Load by Hour')\n",
    "axes[1, 2].set_xlabel('Hour of Day')\n",
    "axes[1, 2].set_ylabel('MW')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba3efe2-48f1-4915-8c14-9be85435b54c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric features\n",
    "numeric_cols = ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct', \n",
    "                'total_imports', 'total_exports', 'net_imports', 'import_ratio',\n",
    "                'hour', 'day_of_week', 'month', 'is_weekend']\n",
    "\n",
    "corr_matrix = complete_pd[numeric_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Upper triangle mask\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, vmin=-1, vmax=1, square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "print(\"\\nSTRONGEST CORRELATIONS (absolute value > 0.3):\")\n",
    "print(\"=\"*60)\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.3:\n",
    "            print(f\"{corr_matrix.columns[i]:20} <-> {corr_matrix.columns[j]:20}: {corr_val:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcaf1c5-a307-492b-9a8e-f506f435e1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze patterns by country\n",
    "country_stats = complete_pd.groupby('country').agg({\n",
    "    'actual_load': ['mean', 'std'],\n",
    "    'forecast_error': ['mean', 'std'],\n",
    "    'forecast_error_pct': ['mean', 'std'],\n",
    "    'net_imports': ['mean', 'std'],\n",
    "    'import_ratio': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "country_stats.columns = ['_'.join(col) for col in country_stats.columns]\n",
    "country_stats = country_stats.reset_index()\n",
    "\n",
    "print(\"COUNTRY-LEVEL STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "print(country_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3384c2-e714-43a3-92c2-30d59d4a0bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check Netherlands data - the 22% average forecast error is suspicious\n",
    "nl_data = complete_pd[complete_pd['country'] == 'NL']\n",
    "\n",
    "print(\"NETHERLANDS FORECAST ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(nl_data):,}\")\n",
    "print(f\"\\nForecast Error % Distribution:\")\n",
    "print(nl_data['forecast_error_pct'].describe())\n",
    "\n",
    "# Check for extreme values\n",
    "print(f\"\\nExtreme values (>100% or <-50%):\")\n",
    "extreme_nl = nl_data[(nl_data['forecast_error_pct'] > 100) | (nl_data['forecast_error_pct'] < -50)]\n",
    "print(f\"Count: {len(extreme_nl)}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(nl_data['forecast_error_pct'].clip(-100, 200), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Netherlands: Forecast Error % Distribution')\n",
    "axes[0].set_xlabel('Forecast Error %')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# Time series of monthly average error\n",
    "nl_data['year_month'] = pd.to_datetime(nl_data['timestamp']).dt.to_period('M')\n",
    "monthly_error = nl_data.groupby('year_month')['forecast_error_pct'].mean()\n",
    "axes[1].plot(monthly_error.index.astype(str), monthly_error.values, marker='o', markersize=3)\n",
    "axes[1].set_title('Netherlands: Monthly Average Forecast Error %')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Avg Error %')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2c43bc-0ecc-45ae-a5ee-6402bf9ef527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Target Variable Definition Strategy\n",
    "#\n",
    "# Based on blackout analysis, vulnerable conditions include:\n",
    "# 1. Supply-demand imbalance (forecast error as proxy)\n",
    "# 2. High import dependency (relying on external power)\n",
    "# 3. Unusual conditions for that country/time (z-scores)\n",
    "# 4. Rapid changes (will add lag features later)\n",
    "#\n",
    "# We'll create:\n",
    "# - Continuous stress score (for regression)\n",
    "# - Binary high-stress flag (for classification)\n",
    "# - Multi-class risk levels (Low/Medium/High/Critical)\n",
    "\n",
    "# Calculate country-specific baselines for z-score normalization\n",
    "country_baselines = complete_pd.groupby('country').agg({\n",
    "    'forecast_error': ['mean', 'std'],\n",
    "    'forecast_error_pct': ['mean', 'std'],\n",
    "    'import_ratio': ['mean', 'std'],\n",
    "    'actual_load': ['mean', 'std']\n",
    "})\n",
    "\n",
    "country_baselines.columns = ['_'.join(col) for col in country_baselines.columns]\n",
    "country_baselines = country_baselines.reset_index()\n",
    "\n",
    "# Merge baselines back to main data\n",
    "analysis_df = complete_pd.merge(country_baselines, on='country', how='left')\n",
    "\n",
    "# Calculate z-scores (how unusual is this observation for this country?)\n",
    "# Use small epsilon to avoid division by zero\n",
    "eps = 1e-6\n",
    "\n",
    "analysis_df['forecast_error_zscore'] = (\n",
    "    (analysis_df['forecast_error'] - analysis_df['forecast_error_mean']) / \n",
    "    (analysis_df['forecast_error_std'] + eps)\n",
    ")\n",
    "\n",
    "analysis_df['import_ratio_zscore'] = (\n",
    "    (analysis_df['import_ratio'] - analysis_df['import_ratio_mean']) / \n",
    "    (analysis_df['import_ratio_std'] + eps)\n",
    ")\n",
    "\n",
    "analysis_df['load_zscore'] = (\n",
    "    (analysis_df['actual_load'] - analysis_df['actual_load_mean']) / \n",
    "    (analysis_df['actual_load_std'] + eps)\n",
    ")\n",
    "\n",
    "print(\"Z-SCORE DISTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(analysis_df[['forecast_error_zscore', 'import_ratio_zscore', 'load_zscore']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d2e374-d860-4002-a1d3-137f82928525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create composite stress score\n",
    "# Higher absolute z-scores = more unusual/stressed conditions\n",
    "# I focus on:\n",
    "# 1. Absolute forecast error (supply-demand mismatch) - PRIMARY indicator\n",
    "# 2. High import dependency (vulnerability) - SECONDARY indicator\n",
    "# 3. Unusual load patterns - TERTIARY indicator\n",
    "\n",
    "# Stress components (using absolute values for deviations)\n",
    "analysis_df['stress_forecast_error'] = analysis_df['forecast_error_zscore'].abs()\n",
    "analysis_df['stress_import_dependency'] = analysis_df['import_ratio_zscore'].clip(lower=0)  # Only high imports are risky\n",
    "analysis_df['stress_load_unusual'] = analysis_df['load_zscore'].abs()\n",
    "\n",
    "# Composite stress score (weighted sum)\n",
    "# Forecast error is most important based on blackout analysis\n",
    "analysis_df['stress_score'] = (\n",
    "    0.50 * analysis_df['stress_forecast_error'] +      # 50% weight - supply/demand mismatch\n",
    "    0.30 * analysis_df['stress_import_dependency'] +   # 30% weight - import vulnerability  \n",
    "    0.20 * analysis_df['stress_load_unusual']          # 20% weight - unusual demand\n",
    ")\n",
    "\n",
    "# Define risk levels based on stress score percentiles\n",
    "stress_percentiles = analysis_df['stress_score'].quantile([0.50, 0.85, 0.95, 0.99])\n",
    "print(\"STRESS SCORE PERCENTILES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"50th percentile: {stress_percentiles[0.50]:.3f}\")\n",
    "print(f\"85th percentile: {stress_percentiles[0.85]:.3f}\")\n",
    "print(f\"95th percentile: {stress_percentiles[0.95]:.3f}\")\n",
    "print(f\"99th percentile: {stress_percentiles[0.99]:.3f}\")\n",
    "\n",
    "# Create risk level categories\n",
    "def assign_risk_level(score, p50, p85, p95, p99):\n",
    "    if score < p50:\n",
    "        return 0  # Low\n",
    "    elif score < p85:\n",
    "        return 1  # Medium\n",
    "    elif score < p95:\n",
    "        return 2  # High\n",
    "    else:\n",
    "        return 3  # Critical\n",
    "\n",
    "analysis_df['risk_level'] = analysis_df['stress_score'].apply(\n",
    "    lambda x: assign_risk_level(x, stress_percentiles[0.50], stress_percentiles[0.85], \n",
    "                                 stress_percentiles[0.95], stress_percentiles[0.99])\n",
    ")\n",
    "\n",
    "# Binary target: High stress = risk level 2 or 3 (top 15%)\n",
    "analysis_df['high_stress'] = (analysis_df['risk_level'] >= 2).astype(int)\n",
    "\n",
    "# Distribution of targets\n",
    "print(\"\\nRISK LEVEL DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "risk_counts = analysis_df['risk_level'].value_counts().sort_index()\n",
    "risk_labels = {0: 'Low', 1: 'Medium', 2: 'High', 3: 'Critical'}\n",
    "for level, count in risk_counts.items():\n",
    "    pct = count / len(analysis_df) * 100\n",
    "    print(f\"{risk_labels[level]:10}: {count:>8,} ({pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nBinary high_stress (1): {analysis_df['high_stress'].sum():,} ({analysis_df['high_stress'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5e8dc8-6fa5-4885-b605-e924fdd925e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if my stress score detected the April 28, 2025 blackout\n",
    "# I need to look at the hours BEFORE the blackout (10:45 is when it started)\n",
    "\n",
    "# Filter for ES and PT around the blackout date\n",
    "blackout_validation = analysis_df[\n",
    "    (analysis_df['country'].isin(['ES', 'PT'])) &\n",
    "    (pd.to_datetime(analysis_df['timestamp']).dt.date == pd.to_datetime('2025-04-28').date())\n",
    "].copy()\n",
    "\n",
    "blackout_validation['hour'] = pd.to_datetime(blackout_validation['timestamp']).dt.hour\n",
    "blackout_validation = blackout_validation.sort_values(['country', 'timestamp'])\n",
    "\n",
    "print(\"BLACKOUT DAY STRESS ANALYSIS - April 28, 2025\")\n",
    "print(\"=\"*70)\n",
    "print(\"(Blackout started at ~10:45 local time)\")\n",
    "print()\n",
    "\n",
    "# Show key metrics around the blackout\n",
    "cols_to_show = ['timestamp', 'country', 'actual_load', 'forecast_error_pct', \n",
    "                'import_ratio', 'stress_score', 'risk_level']\n",
    "\n",
    "print(\"SPAIN (ES):\")\n",
    "es_data = blackout_validation[blackout_validation['country'] == 'ES'][cols_to_show]\n",
    "print(es_data.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPORTUGAL (PT):\")\n",
    "pt_data = blackout_validation[blackout_validation['country'] == 'PT'][cols_to_show]\n",
    "print(pt_data.to_string(index=False))\n",
    "\n",
    "# Check what risk levels were assigned\n",
    "print(\"\\n\\nRISK LEVEL SUMMARY ON BLACKOUT DAY:\")\n",
    "print(\"=\"*70)\n",
    "for country in ['ES', 'PT']:\n",
    "    country_data = blackout_validation[blackout_validation['country'] == country]\n",
    "    print(f\"\\n{country}:\")\n",
    "    print(f\"  Max stress score: {country_data['stress_score'].max():.3f}\")\n",
    "    print(f\"  Records at Critical (3): {(country_data['risk_level'] == 3).sum()}\")\n",
    "    print(f\"  Records at High (2): {(country_data['risk_level'] == 2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91be95d5-5200-4585-8fee-b6bf2a9801bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at the days BEFORE the blackout to find early warning signs\n",
    "# Check April 25-28 for ES and PT\n",
    "\n",
    "pre_blackout = analysis_df[\n",
    "    (analysis_df['country'].isin(['ES', 'PT'])) &\n",
    "    (pd.to_datetime(analysis_df['timestamp']).dt.date >= pd.to_datetime('2025-04-25').date()) &\n",
    "    (pd.to_datetime(analysis_df['timestamp']).dt.date <= pd.to_datetime('2025-04-28').date())\n",
    "].copy()\n",
    "\n",
    "pre_blackout['date'] = pd.to_datetime(pre_blackout['timestamp']).dt.date\n",
    "pre_blackout['hour'] = pd.to_datetime(pre_blackout['timestamp']).dt.hour\n",
    "\n",
    "# Daily statistics\n",
    "daily_stats = pre_blackout.groupby(['country', 'date']).agg({\n",
    "    'actual_load': 'mean',\n",
    "    'forecast_error': 'mean',\n",
    "    'forecast_error_pct': ['mean', 'std', 'min', 'max'],\n",
    "    'import_ratio': 'mean',\n",
    "    'stress_score': ['mean', 'max'],\n",
    "    'risk_level': 'max'\n",
    "}).round(2)\n",
    "\n",
    "daily_stats.columns = ['_'.join(col) if col[1] else col[0] for col in daily_stats.columns]\n",
    "daily_stats = daily_stats.reset_index()\n",
    "\n",
    "print(\"DAILY STATISTICS - DAYS BEFORE BLACKOUT\")\n",
    "print(\"=\"*80)\n",
    "print(\"(April 28 is the blackout day)\")\n",
    "print()\n",
    "print(daily_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60357bd-a4d8-4de2-9829-7fbc339476d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Pipeline\n",
    "# I need to build this on the full Spark dataset, not the sample\n",
    "\n",
    "# First, let's define all the features I want to create\n",
    "print(\"FEATURE ENGINEERING PLAN\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Features to create:\n",
    "1. TEMPORAL FEATURES (already have some)\n",
    "   - hour, day_of_week, month, is_weekend\n",
    "   - hour_sin, hour_cos (cyclical encoding)\n",
    "   \n",
    "2. LAG FEATURES (past values)\n",
    "   - forecast_error at t-1, t-2, t-4, t-24 hours\n",
    "   - import_ratio at t-1, t-4, t-24 hours\n",
    "   - actual_load at t-1, t-24 hours\n",
    "   \n",
    "3. ROLLING STATISTICS (trends)\n",
    "   - forecast_error rolling mean/std (4h, 24h windows)\n",
    "   - import_ratio rolling mean (4h, 24h windows)\n",
    "   \n",
    "4. RATE OF CHANGE\n",
    "   - forecast_error change from t-1\n",
    "   - load change from t-1\n",
    "   \n",
    "5. COUNTRY-NORMALIZED FEATURES\n",
    "   - z-scores for key metrics (already created)\n",
    "   \n",
    "6. CROSS-BORDER STRESS INDICATORS\n",
    "   - absolute import/export levels\n",
    "   - import dependency ratio\n",
    "\"\"\")\n",
    "\n",
    "# Build on full Spark dataset\n",
    "# Add temporal encoding\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "\n",
    "# Cyclical encoding for hour\n",
    "complete_features = complete_df.withColumn(\n",
    "    \"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)\n",
    ").withColumn(\n",
    "    \"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24)\n",
    ").withColumn(\n",
    "    \"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)\n",
    ").withColumn(\n",
    "    \"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12)\n",
    ")\n",
    "\n",
    "print(\"\\nCyclical encoding added\")\n",
    "print(f\"Dataset rows: {complete_features.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e990694-30d5-4486-83e7-b4ae93dbd6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window for lag features (partitioned by country, ordered by timestamp)\n",
    "country_time_window = Window.partitionBy(\"country\").orderBy(\"timestamp\")\n",
    "\n",
    "# Lag features - past values\n",
    "# Note: Data is at 15-min intervals, so lag 4 = 1 hour, lag 96 = 24 hours\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"forecast_error_lag_1\", F.lag(\"forecast_error\", 1).over(country_time_window)\n",
    ").withColumn(\n",
    "    \"forecast_error_lag_4\", F.lag(\"forecast_error\", 4).over(country_time_window)  # 1 hour ago\n",
    ").withColumn(\n",
    "    \"forecast_error_lag_16\", F.lag(\"forecast_error\", 16).over(country_time_window)  # 4 hours ago\n",
    ").withColumn(\n",
    "    \"forecast_error_lag_96\", F.lag(\"forecast_error\", 96).over(country_time_window)  # 24 hours ago\n",
    ")\n",
    "\n",
    "# Import ratio lags\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"import_ratio_lag_1\", F.lag(\"import_ratio\", 1).over(country_time_window)\n",
    ").withColumn(\n",
    "    \"import_ratio_lag_4\", F.lag(\"import_ratio\", 4).over(country_time_window)\n",
    ").withColumn(\n",
    "    \"import_ratio_lag_96\", F.lag(\"import_ratio\", 96).over(country_time_window)\n",
    ")\n",
    "\n",
    "# Load lags\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"actual_load_lag_1\", F.lag(\"actual_load\", 1).over(country_time_window)\n",
    ").withColumn(\n",
    "    \"actual_load_lag_96\", F.lag(\"actual_load\", 96).over(country_time_window)\n",
    ")\n",
    "\n",
    "# Rate of change\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"forecast_error_change\", F.col(\"forecast_error\") - F.col(\"forecast_error_lag_1\")\n",
    ").withColumn(\n",
    "    \"load_change\", F.col(\"actual_load\") - F.col(\"actual_load_lag_1\")\n",
    ").withColumn(\n",
    "    \"load_change_pct\", \n",
    "    F.coalesce(\n",
    "        F.try_divide(F.col(\"load_change\"), F.col(\"actual_load_lag_1\")) * 100, \n",
    "        F.lit(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Lag features added\")\n",
    "complete_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425edc1f-6cb9-4c4c-ab43-586cf8663d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rolling statistics using window functions\n",
    "# 4-hour window = 16 rows, 24-hour window = 96 rows\n",
    "\n",
    "# Rolling window definitions\n",
    "roll_4h = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(-15, 0)  # 4 hours\n",
    "roll_24h = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(-95, 0)  # 24 hours\n",
    "\n",
    "# Forecast error rolling stats\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"forecast_error_roll_4h_mean\", F.avg(\"forecast_error\").over(roll_4h)\n",
    ").withColumn(\n",
    "    \"forecast_error_roll_4h_std\", F.stddev(\"forecast_error\").over(roll_4h)\n",
    ").withColumn(\n",
    "    \"forecast_error_roll_24h_mean\", F.avg(\"forecast_error\").over(roll_24h)\n",
    ").withColumn(\n",
    "    \"forecast_error_roll_24h_std\", F.stddev(\"forecast_error\").over(roll_24h)\n",
    ")\n",
    "\n",
    "# Import ratio rolling stats\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"import_ratio_roll_4h_mean\", F.avg(\"import_ratio\").over(roll_4h)\n",
    ").withColumn(\n",
    "    \"import_ratio_roll_24h_mean\", F.avg(\"import_ratio\").over(roll_24h)\n",
    ")\n",
    "\n",
    "# Load rolling stats\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"load_roll_24h_mean\", F.avg(\"actual_load\").over(roll_24h)\n",
    ").withColumn(\n",
    "    \"load_roll_24h_std\", F.stddev(\"actual_load\").over(roll_24h)\n",
    ")\n",
    "\n",
    "# Volatility indicator: current error vs rolling average\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"forecast_error_vs_roll_4h\",\n",
    "    F.col(\"forecast_error\") - F.col(\"forecast_error_roll_4h_mean\")\n",
    ")\n",
    "\n",
    "print(\"Rolling statistics added\")\n",
    "print(f\"Total features now: {len(complete_features.columns)}\")\n",
    "print(\"\\nNew columns:\")\n",
    "for col in complete_features.columns[-9:]:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f4f7d-47ec-4b52-b136-0befe6171edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate country baselines from the full dataset\n",
    "country_stats = complete_features.groupBy(\"country\").agg(\n",
    "    F.avg(\"forecast_error\").alias(\"country_forecast_error_mean\"),\n",
    "    F.stddev(\"forecast_error\").alias(\"country_forecast_error_std\"),\n",
    "    F.avg(\"import_ratio\").alias(\"country_import_ratio_mean\"),\n",
    "    F.stddev(\"import_ratio\").alias(\"country_import_ratio_std\"),\n",
    "    F.avg(\"actual_load\").alias(\"country_load_mean\"),\n",
    "    F.stddev(\"actual_load\").alias(\"country_load_std\")\n",
    ")\n",
    "\n",
    "# Join country stats back to main dataframe\n",
    "complete_features = complete_features.join(country_stats, on=\"country\", how=\"left\")\n",
    "\n",
    "# Calculate z-scores\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"forecast_error_zscore\",\n",
    "    (F.col(\"forecast_error\") - F.col(\"country_forecast_error_mean\")) / \n",
    "    (F.col(\"country_forecast_error_std\") + 1e-6)\n",
    ").withColumn(\n",
    "    \"import_ratio_zscore\",\n",
    "    (F.col(\"import_ratio\") - F.col(\"country_import_ratio_mean\")) / \n",
    "    (F.col(\"country_import_ratio_std\") + 1e-6)\n",
    ").withColumn(\n",
    "    \"load_zscore\",\n",
    "    (F.col(\"actual_load\") - F.col(\"country_load_mean\")) / \n",
    "    (F.col(\"country_load_std\") + 1e-6)\n",
    ")\n",
    "\n",
    "# Create stress score components\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"stress_forecast_error\", F.abs(F.col(\"forecast_error_zscore\"))\n",
    ").withColumn(\n",
    "    \"stress_import_dependency\", \n",
    "    F.when(F.col(\"import_ratio_zscore\") > 0, F.col(\"import_ratio_zscore\")).otherwise(0)\n",
    ").withColumn(\n",
    "    \"stress_load_unusual\", F.abs(F.col(\"load_zscore\"))\n",
    ")\n",
    "\n",
    "# Composite stress score\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"stress_score\",\n",
    "    0.50 * F.col(\"stress_forecast_error\") +\n",
    "    0.30 * F.col(\"stress_import_dependency\") +\n",
    "    0.20 * F.col(\"stress_load_unusual\")\n",
    ")\n",
    "\n",
    "print(\"Country baselines and stress score added\")\n",
    "print(f\"Total features: {len(complete_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc095e44-f04c-4c83-8964-3ab37d9027c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate stress score percentiles for threshold setting\n",
    "stress_percentiles = complete_features.approxQuantile(\n",
    "    \"stress_score\", [0.50, 0.85, 0.95, 0.99], 0.01\n",
    ")\n",
    "\n",
    "print(\"STRESS SCORE PERCENTILES (Full Dataset)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"50th percentile: {stress_percentiles[0]:.3f}\")\n",
    "print(f\"85th percentile: {stress_percentiles[1]:.3f}\")\n",
    "print(f\"95th percentile: {stress_percentiles[2]:.3f}\")\n",
    "print(f\"99th percentile: {stress_percentiles[3]:.3f}\")\n",
    "\n",
    "# Create risk levels and binary target\n",
    "p50, p85, p95, p99 = stress_percentiles\n",
    "\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"risk_level\",\n",
    "    F.when(F.col(\"stress_score\") < p50, 0)\n",
    "     .when(F.col(\"stress_score\") < p85, 1)\n",
    "     .when(F.col(\"stress_score\") < p95, 2)\n",
    "     .otherwise(3)\n",
    ")\n",
    "\n",
    "# Binary target: high stress = risk level 2 or 3\n",
    "complete_features = complete_features.withColumn(\n",
    "    \"high_stress\", \n",
    "    F.when(F.col(\"risk_level\") >= 2, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "print(\"\\nTARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "risk_dist = complete_features.groupBy(\"risk_level\").count().orderBy(\"risk_level\").collect()\n",
    "total = complete_features.count()\n",
    "risk_labels = {0: 'Low', 1: 'Medium', 2: 'High', 3: 'Critical'}\n",
    "for row in risk_dist:\n",
    "    level = row['risk_level']\n",
    "    count = row['count']\n",
    "    pct = count / total * 100\n",
    "    print(f\"{risk_labels[level]:10}: {count:>10,} ({pct:>5.1f}%)\")\n",
    "\n",
    "high_stress_count = complete_features.filter(F.col(\"high_stress\") == 1).count()\n",
    "print(f\"\\nBinary high_stress (1): {high_stress_count:,} ({high_stress_count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa00c48-2736-4628-a454-3c16f3aae262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check null counts after lag feature creation\n",
    "# Lag features will have nulls at the beginning of each country's time series\n",
    "\n",
    "null_check = complete_features.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in complete_features.columns\n",
    "]).collect()[0]\n",
    "\n",
    "print(\"NULL COUNTS BY COLUMN\")\n",
    "print(\"=\"*60)\n",
    "null_cols = [(col, null_check[col]) for col in complete_features.columns if null_check[col] > 0]\n",
    "for col, count in sorted(null_cols, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{col:40}: {count:>10,}\")\n",
    "\n",
    "# Drop rows with nulls in critical lag features (these are the first few rows per country)\n",
    "# I lose ~96 rows per country (24 hours of 15-min data)\n",
    "feature_cols = [\n",
    "    # Base features\n",
    "    'actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "    'total_imports', 'total_exports', 'net_imports', 'import_ratio',\n",
    "    # Temporal\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    # Lag features\n",
    "    'forecast_error_lag_1', 'forecast_error_lag_4', 'forecast_error_lag_16', 'forecast_error_lag_96',\n",
    "    'import_ratio_lag_1', 'import_ratio_lag_4', 'import_ratio_lag_96',\n",
    "    'actual_load_lag_1', 'actual_load_lag_96',\n",
    "    # Rate of change\n",
    "    'forecast_error_change', 'load_change', 'load_change_pct',\n",
    "    # Rolling stats\n",
    "    'forecast_error_roll_4h_mean', 'forecast_error_roll_4h_std',\n",
    "    'forecast_error_roll_24h_mean', 'forecast_error_roll_24h_std',\n",
    "    'import_ratio_roll_4h_mean', 'import_ratio_roll_24h_mean',\n",
    "    'load_roll_24h_mean', 'load_roll_24h_std',\n",
    "    'forecast_error_vs_roll_4h',\n",
    "    # Z-scores\n",
    "    'forecast_error_zscore', 'import_ratio_zscore', 'load_zscore'\n",
    "]\n",
    "\n",
    "# Target and identifiers\n",
    "id_cols = ['timestamp', 'country']\n",
    "target_cols = ['stress_score', 'risk_level', 'high_stress']\n",
    "\n",
    "# Filter to only needed columns and drop nulls\n",
    "model_df = complete_features.select(id_cols + feature_cols + target_cols)\n",
    "model_df_clean = model_df.dropna()\n",
    "\n",
    "print(f\"\\nDataset size before null removal: {complete_features.count():,}\")\n",
    "print(f\"Dataset size after null removal:  {model_df_clean.count():,}\")\n",
    "print(f\"Rows removed: {complete_features.count() - model_df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac2326d-a53b-457e-8b77-84d65a768613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporal split - critical for time series\n",
    "# I must not use future data to predict the past\n",
    "\n",
    "# Get date range\n",
    "date_range = model_df_clean.agg(\n",
    "    F.min(\"timestamp\").alias(\"min_date\"),\n",
    "    F.max(\"timestamp\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"DATA TEMPORAL RANGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start: {date_range['min_date']}\")\n",
    "print(f\"End:   {date_range['max_date']}\")\n",
    "\n",
    "# Split strategy:\n",
    "# Train: 2023-01-01 to 2024-12-31 (2 years)\n",
    "# Validation: 2025-01-01 to 2025-06-30 (6 months)\n",
    "# Test: 2025-07-01 to 2025-11-07 (includes post-blackout period for evaluation)\n",
    "\n",
    "train_end = \"2024-12-31 23:59:59\"\n",
    "val_end = \"2025-06-30 23:59:59\"\n",
    "\n",
    "train_df = model_df_clean.filter(F.col(\"timestamp\") <= train_end)\n",
    "val_df = model_df_clean.filter(\n",
    "    (F.col(\"timestamp\") > train_end) & (F.col(\"timestamp\") <= val_end)\n",
    ")\n",
    "test_df = model_df_clean.filter(F.col(\"timestamp\") > val_end)\n",
    "\n",
    "# Check sizes\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\nTRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train (2023-01 to 2024-12): {train_count:>10,} ({train_count/total*100:.1f}%)\")\n",
    "print(f\"Val   (2025-01 to 2025-06): {val_count:>10,} ({val_count/total*100:.1f}%)\")\n",
    "print(f\"Test  (2025-07 to 2025-11): {test_count:>10,} ({test_count/total*100:.1f}%)\")\n",
    "print(f\"Total:                      {total:>10,}\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(f\"\\nHIGH STRESS RATE BY SPLIT\")\n",
    "print(\"=\"*60)\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    high_stress_rate = df.filter(F.col(\"high_stress\") == 1).count() / df.count() * 100\n",
    "    print(f\"{name}: {high_stress_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabbcd47-56d8-4d1f-b210-5e3c0ac09477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrames for sklearn/xgboost\n",
    "# This is manageable since I have ~1.5M rows\n",
    "\n",
    "print(\"Converting to Pandas...\")\n",
    "\n",
    "train_pd = train_df.toPandas()\n",
    "val_pd = val_df.toPandas()\n",
    "test_pd = test_df.toPandas()\n",
    "\n",
    "print(f\"Train shape: {train_pd.shape}\")\n",
    "print(f\"Val shape:   {val_pd.shape}\")\n",
    "print(f\"Test shape:  {test_pd.shape}\")\n",
    "\n",
    "# Define feature columns (exclude identifiers and targets)\n",
    "feature_cols = [\n",
    "    # Base features\n",
    "    'actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "    'total_imports', 'total_exports', 'net_imports', 'import_ratio',\n",
    "    # Temporal\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    # Lag features\n",
    "    'forecast_error_lag_1', 'forecast_error_lag_4', 'forecast_error_lag_16', 'forecast_error_lag_96',\n",
    "    'import_ratio_lag_1', 'import_ratio_lag_4', 'import_ratio_lag_96',\n",
    "    'actual_load_lag_1', 'actual_load_lag_96',\n",
    "    # Rate of change\n",
    "    'forecast_error_change', 'load_change', 'load_change_pct',\n",
    "    # Rolling stats\n",
    "    'forecast_error_roll_4h_mean', 'forecast_error_roll_4h_std',\n",
    "    'forecast_error_roll_24h_mean', 'forecast_error_roll_24h_std',\n",
    "    'import_ratio_roll_4h_mean', 'import_ratio_roll_24h_mean',\n",
    "    'load_roll_24h_mean', 'load_roll_24h_std',\n",
    "    'forecast_error_vs_roll_4h',\n",
    "    # Z-scores\n",
    "    'forecast_error_zscore', 'import_ratio_zscore', 'load_zscore'\n",
    "]\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_pd[feature_cols]\n",
    "y_train_stress = train_pd['stress_score']\n",
    "y_train_binary = train_pd['high_stress']\n",
    "y_train_class = train_pd['risk_level']\n",
    "\n",
    "X_val = val_pd[feature_cols]\n",
    "y_val_stress = val_pd['stress_score']\n",
    "y_val_binary = val_pd['high_stress']\n",
    "y_val_class = val_pd['risk_level']\n",
    "\n",
    "X_test = test_pd[feature_cols]\n",
    "y_test_stress = test_pd['stress_score']\n",
    "y_test_binary = test_pd['high_stress']\n",
    "y_test_class = test_pd['risk_level']\n",
    "\n",
    "print(f\"\\nFeature count: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7244b7-17cd-472e-b938-b5c3f5596907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install xgboost lightgbm --quiet\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "\n",
    "# Baseline Model: Logistic Regression for binary classification\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL: Logistic Regression\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train baseline\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "baseline_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "# Predictions\n",
    "baseline_pred_train = baseline_model.predict(X_train_scaled)\n",
    "baseline_pred_val = baseline_model.predict(X_val_scaled)\n",
    "baseline_prob_val = baseline_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {accuracy_score(y_train_binary, baseline_pred_train):.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val_binary, baseline_pred_val):.4f}\")\n",
    "print(f\"Validation AUC-ROC: {roc_auc_score(y_val_binary, baseline_prob_val):.4f}\")\n",
    "print(f\"\\nValidation Classification Report:\")\n",
    "print(classification_report(y_val_binary, baseline_pred_val, target_names=['Normal', 'High Stress']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e015f5ae-cfd9-4b16-9734-2c07d924d858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Binary Classification\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 1: XGBoost Binary Classifier\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate class weight for imbalanced data\n",
    "scale_pos_weight = len(y_train_binary[y_train_binary == 0]) / len(y_train_binary[y_train_binary == 1])\n",
    "print(f\"Scale pos weight (for imbalance): {scale_pos_weight:.2f}\")\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train_binary,\n",
    "    eval_set=[(X_val, y_val_binary)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_pred_train = xgb_model.predict(X_train)\n",
    "xgb_pred_val = xgb_model.predict(X_val)\n",
    "xgb_prob_val = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {accuracy_score(y_train_binary, xgb_pred_train):.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val_binary, xgb_pred_val):.4f}\")\n",
    "print(f\"Validation AUC-ROC: {roc_auc_score(y_val_binary, xgb_prob_val):.4f}\")\n",
    "print(f\"Validation Precision: {precision_score(y_val_binary, xgb_pred_val):.4f}\")\n",
    "print(f\"Validation Recall: {recall_score(y_val_binary, xgb_pred_val):.4f}\")\n",
    "print(f\"Validation F1: {f1_score(y_val_binary, xgb_pred_val):.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Classification Report:\")\n",
    "print(classification_report(y_val_binary, xgb_pred_val, target_names=['Normal', 'High Stress']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba0606e-d34b-49f8-b64e-841b17f42bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance Analysis - Critical for detecting potential leakage\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "top_20 = importance_df.head(20)\n",
    "ax.barh(range(len(top_20)), top_20['importance'].values, color='steelblue')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('XGBoost Feature Importance (Top 20)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for potential leakage\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEAKAGE CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Potential leakage indicators to watch for:\n",
    "- forecast_error_zscore: This is used directly in stress_score calculation\n",
    "- import_ratio_zscore: Also used in stress_score calculation\n",
    "- load_zscore: Also used in stress_score calculation\n",
    "\n",
    "If these z-score features dominate, I have circular dependency (leakage).\n",
    "\"\"\")\n",
    "\n",
    "# Identify z-score features\n",
    "zscore_features = [f for f in feature_cols if 'zscore' in f]\n",
    "zscore_importance = importance_df[importance_df['feature'].isin(zscore_features)]['importance'].sum()\n",
    "total_importance = importance_df['importance'].sum()\n",
    "print(f\"Z-score features contribution: {zscore_importance/total_importance*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826e07ff-7995-49dd-93c7-df55ca8e05bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build clean model WITHOUT z-score features (which are used to create the target)\n",
    "print(\"=\"*70)\n",
    "print(\"CLEAN MODEL: XGBoost Without Z-Score Features (No Leakage)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Remove z-score features\n",
    "clean_feature_cols = [f for f in feature_cols if 'zscore' not in f]\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Clean features: {len(clean_feature_cols)}\")\n",
    "print(f\"Removed: {set(feature_cols) - set(clean_feature_cols)}\")\n",
    "\n",
    "# Prepare clean data\n",
    "X_train_clean = train_pd[clean_feature_cols]\n",
    "X_val_clean = val_pd[clean_feature_cols]\n",
    "X_test_clean = test_pd[clean_feature_cols]\n",
    "\n",
    "# Train clean XGBoost\n",
    "xgb_clean_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_clean = xgb.XGBClassifier(**xgb_clean_params)\n",
    "xgb_clean.fit(\n",
    "    X_train_clean, y_train_binary,\n",
    "    eval_set=[(X_val_clean, y_val_binary)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_clean_pred_val = xgb_clean.predict(X_val_clean)\n",
    "xgb_clean_prob_val = xgb_clean.predict_proba(X_val_clean)[:, 1]\n",
    "\n",
    "print(f\"\\nValidation Results (Clean Model):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val_binary, xgb_clean_pred_val):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_val_binary, xgb_clean_prob_val):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_val_binary, xgb_clean_pred_val):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_val_binary, xgb_clean_pred_val):.4f}\")\n",
    "print(f\"F1: {f1_score(y_val_binary, xgb_clean_pred_val):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val_binary, xgb_clean_pred_val, target_names=['Normal', 'High Stress']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045b5698-6278-40f8-b3ab-c2b524e6799a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LightGBM for comparison\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 2: LightGBM Binary Classifier (Clean Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_train_clean, y_train_binary,\n",
    "    eval_set=[(X_val_clean, y_val_binary)],\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "lgb_pred_val = lgb_model.predict(X_val_clean)\n",
    "lgb_prob_val = lgb_model.predict_proba(X_val_clean)[:, 1]\n",
    "\n",
    "print(f\"\\nValidation Results (LightGBM):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val_binary, lgb_pred_val):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_val_binary, lgb_prob_val):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_val_binary, lgb_pred_val):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_val_binary, lgb_pred_val):.4f}\")\n",
    "print(f\"F1: {f1_score(y_val_binary, lgb_pred_val):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val_binary, lgb_pred_val, target_names=['Normal', 'High Stress']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b12bac9-5e04-4de8-be81-c2237fd34411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance for clean XGBoost model\n",
    "print(\"=\"*70)\n",
    "print(\"CLEAN MODEL FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance_clean_df = pd.DataFrame({\n",
    "    'feature': clean_feature_cols,\n",
    "    'importance': xgb_clean.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Clean Model):\")\n",
    "print(importance_clean_df.head(20).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "top_20 = importance_clean_df.head(20)\n",
    "ax.barh(range(len(top_20)), top_20['importance'].values, color='forestgreen')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Clean XGBoost Feature Importance (Top 20) - No Leakage')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Categorize feature importance\n",
    "print(\"\\n\\nFEATURE CATEGORY CONTRIBUTIONS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "categories = {\n",
    "    'Forecast Error Features': [f for f in clean_feature_cols if 'forecast_error' in f],\n",
    "    'Import/Export Features': [f for f in clean_feature_cols if 'import' in f or 'export' in f],\n",
    "    'Load Features': [f for f in clean_feature_cols if 'load' in f and 'forecast' not in f],\n",
    "    'Temporal Features': ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
    "    'Lag Features': [f for f in clean_feature_cols if 'lag' in f],\n",
    "    'Rolling Features': [f for f in clean_feature_cols if 'roll' in f]\n",
    "}\n",
    "\n",
    "for cat_name, cat_features in categories.items():\n",
    "    cat_importance = importance_clean_df[importance_clean_df['feature'].isin(cat_features)]['importance'].sum()\n",
    "    print(f\"{cat_name:30}: {cat_importance*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bef4e9a-766c-4f6e-984a-1a0e797a2496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "print(\"=\"*70)\n",
    "print(\"CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Validation set confusion matrix\n",
    "cm_val = confusion_matrix(y_val_binary, xgb_clean_pred_val)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Validation confusion matrix\n",
    "sns.heatmap(cm_val, annot=True, fmt=',d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Normal', 'High Stress'],\n",
    "            yticklabels=['Normal', 'High Stress'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Validation Set Confusion Matrix')\n",
    "\n",
    "# Test set evaluation\n",
    "xgb_clean_pred_test = xgb_clean.predict(X_test_clean)\n",
    "xgb_clean_prob_test = xgb_clean.predict_proba(X_test_clean)[:, 1]\n",
    "\n",
    "cm_test = confusion_matrix(y_test_binary, xgb_clean_pred_test)\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, fmt=',d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Normal', 'High Stress'],\n",
    "            yticklabels=['Normal', 'High Stress'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Test Set Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test set metrics\n",
    "print(\"\\nTEST SET RESULTS (Final Evaluation)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, xgb_clean_pred_test):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test_binary, xgb_clean_prob_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, xgb_clean_pred_test):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, xgb_clean_pred_test):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test_binary, xgb_clean_pred_test):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_binary, xgb_clean_pred_test, target_names=['Normal', 'High Stress']))\n",
    "\n",
    "# Calculate operational metrics\n",
    "tn, fp, fn, tp = cm_test.ravel()\n",
    "print(f\"\\nOPERATIONAL METRICS:\")\n",
    "print(f\"True Positives (Correctly caught high stress): {tp:,}\")\n",
    "print(f\"False Negatives (Missed high stress events): {fn:,}\")\n",
    "print(f\"False Positives (False alarms): {fp:,}\")\n",
    "print(f\"True Negatives (Correctly identified normal): {tn:,}\")\n",
    "print(f\"\\nMissed Alert Rate: {fn/(tp+fn)*100:.2f}%\")\n",
    "print(f\"False Alarm Rate: {fp/(fp+tn)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418a5b77-b882-4cf1-a963-6382e51494c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ROC Curve and Threshold Analysis\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test_binary, xgb_clean_prob_test)\n",
    "auc_score = roc_auc_score(y_test_binary, xgb_clean_prob_test)\n",
    "\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'XGBoost (AUC = {auc_score:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve - Test Set')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test_binary, xgb_clean_prob_test)\n",
    "\n",
    "axes[1].plot(recall_curve, precision_curve, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve - Test Set')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=y_test_binary.mean(), color='r', linestyle='--', label=f'Baseline ({y_test_binary.mean():.3f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Threshold analysis for operational use\n",
    "print(\"\\nTHRESHOLD ANALYSIS FOR PRODUCTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Different thresholds trade off precision vs recall:\\n\")\n",
    "\n",
    "thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'False Alarms':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    pred_thresh = (xgb_clean_prob_test >= thresh).astype(int)\n",
    "    prec = precision_score(y_test_binary, pred_thresh)\n",
    "    rec = recall_score(y_test_binary, pred_thresh)\n",
    "    f1 = f1_score(y_test_binary, pred_thresh)\n",
    "    fp = ((pred_thresh == 1) & (y_test_binary == 0)).sum()\n",
    "    print(f\"{thresh:<12.1f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f} {fp:<15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73dd0181-3fa3-4253-aef1-2b64e2ccd1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate model against the known April 28, 2025 blackout\n",
    "print(\"=\"*70)\n",
    "print(\"BLACKOUT VALIDATION: April 28, 2025 - Spain & Portugal\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get validation data for ES and PT on blackout day\n",
    "# Note: Blackout is in validation set (Jan-Jun 2025)\n",
    "val_blackout = val_pd[\n",
    "    (val_pd['country'].isin(['ES', 'PT'])) &\n",
    "    (pd.to_datetime(val_pd['timestamp']).dt.date == pd.to_datetime('2025-04-28').date())\n",
    "].copy()\n",
    "\n",
    "val_blackout['hour'] = pd.to_datetime(val_blackout['timestamp']).dt.hour\n",
    "val_blackout = val_blackout.sort_values(['country', 'timestamp'])\n",
    "\n",
    "# Get predictions for blackout day\n",
    "X_blackout = val_blackout[clean_feature_cols]\n",
    "blackout_probs = xgb_clean.predict_proba(X_blackout)[:, 1]\n",
    "blackout_preds = xgb_clean.predict(X_blackout)\n",
    "\n",
    "val_blackout['predicted_prob'] = blackout_probs\n",
    "val_blackout['predicted_high_stress'] = blackout_preds\n",
    "\n",
    "# Show results\n",
    "print(\"\\nSPAIN (ES) - Blackout started ~10:45:\")\n",
    "es_blackout = val_blackout[val_blackout['country'] == 'ES'][\n",
    "    ['timestamp', 'actual_load', 'forecast_error_pct', 'high_stress', 'predicted_prob', 'predicted_high_stress']\n",
    "]\n",
    "print(es_blackout.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPORTUGAL (PT):\")\n",
    "pt_blackout = val_blackout[val_blackout['country'] == 'PT'][\n",
    "    ['timestamp', 'actual_load', 'forecast_error_pct', 'high_stress', 'predicted_prob', 'predicted_high_stress']\n",
    "]\n",
    "print(pt_blackout.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\nBLACKOUT DETECTION SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "for country in ['ES', 'PT']:\n",
    "    country_data = val_blackout[val_blackout['country'] == country]\n",
    "    actual_high = country_data['high_stress'].sum()\n",
    "    predicted_high = country_data['predicted_high_stress'].sum()\n",
    "    max_prob = country_data['predicted_prob'].max()\n",
    "    print(f\"\\n{country}:\")\n",
    "    print(f\"  Actual high stress periods: {actual_high}\")\n",
    "    print(f\"  Predicted high stress periods: {predicted_high}\")\n",
    "    print(f\"  Maximum predicted probability: {max_prob:.4f}\")\n",
    "    print(f\"  Detection rate: {predicted_high/max(actual_high, 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f01594-e77e-44f7-924f-73838298dd86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze model performance by country\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE BY COUNTRY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add predictions to test data\n",
    "test_pd_eval = test_pd.copy()\n",
    "test_pd_eval['predicted_prob'] = xgb_clean_prob_test\n",
    "test_pd_eval['predicted'] = xgb_clean_pred_test\n",
    "\n",
    "# Calculate metrics by country\n",
    "country_metrics = []\n",
    "\n",
    "for country in test_pd_eval['country'].unique():\n",
    "    country_data = test_pd_eval[test_pd_eval['country'] == country]\n",
    "    \n",
    "    if country_data['high_stress'].sum() > 0:  # Need positive cases\n",
    "        acc = accuracy_score(country_data['high_stress'], country_data['predicted'])\n",
    "        try:\n",
    "            auc = roc_auc_score(country_data['high_stress'], country_data['predicted_prob'])\n",
    "        except:\n",
    "            auc = np.nan\n",
    "        prec = precision_score(country_data['high_stress'], country_data['predicted'], zero_division=0)\n",
    "        rec = recall_score(country_data['high_stress'], country_data['predicted'], zero_division=0)\n",
    "        f1 = f1_score(country_data['high_stress'], country_data['predicted'], zero_division=0)\n",
    "        \n",
    "        country_metrics.append({\n",
    "            'country': country,\n",
    "            'samples': len(country_data),\n",
    "            'stress_rate': country_data['high_stress'].mean() * 100,\n",
    "            'accuracy': acc,\n",
    "            'auc': auc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "country_metrics_df = pd.DataFrame(country_metrics).sort_values('f1', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Country':<8} {'Samples':>8} {'Stress%':>8} {'Accuracy':>10} {'AUC':>8} {'Precision':>10} {'Recall':>8} {'F1':>8}\")\n",
    "print(\"-\"*78)\n",
    "for _, row in country_metrics_df.iterrows():\n",
    "    print(f\"{row['country']:<8} {row['samples']:>8,} {row['stress_rate']:>7.1f}% {row['accuracy']:>10.3f} {row['auc']:>8.3f} {row['precision']:>10.3f} {row['recall']:>8.3f} {row['f1']:>8.3f}\")\n",
    "\n",
    "# Identify weak performers\n",
    "print(\"\\n\\nCOUNTRIES NEEDING ATTENTION (F1 < 0.80):\")\n",
    "print(\"=\"*60)\n",
    "weak = country_metrics_df[country_metrics_df['f1'] < 0.80]\n",
    "if len(weak) > 0:\n",
    "    for _, row in weak.iterrows():\n",
    "        print(f\"{row['country']}: F1={row['f1']:.3f}, Recall={row['recall']:.3f}\")\n",
    "else:\n",
    "    print(\"All countries performing well!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3492e302-9ee7-40d0-9154-1bacf028028e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final Model Summary\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "MODEL: XGBoost Binary Classifier (Clean - No Leakage)\n",
    "TARGET: High Stress (risk_level >= 2, top 15% of stress scores)\n",
    "\n",
    "STRESS SCORE DEFINITION:\n",
    "  stress_score = 0.50 * |forecast_error_zscore| + \n",
    "                 0.30 * max(import_ratio_zscore, 0) + \n",
    "                 0.20 * |load_zscore|\n",
    "                 \n",
    "  High Stress = stress_score >= 95th percentile\n",
    "\n",
    "DATA:\n",
    "  - Source: ENTSOE Transparency Platform\n",
    "  - Period: Jan 2023 - Nov 2025\n",
    "  - Countries: 26 European nations\n",
    "  - Granularity: 15-minute intervals\n",
    "  - Total records: 1,484,991\n",
    "\n",
    "FEATURES (37 clean features, no target leakage):\n",
    "  - Forecast Error Features: forecast_error, forecast_error_pct, lags, rolling stats\n",
    "  - Import/Export Features: net_imports, import_ratio, total_imports/exports\n",
    "  - Load Features: actual_load, forecast_load, rolling stats\n",
    "  - Temporal Features: hour, day_of_week, month, cyclical encodings\n",
    "  - Lag Features: 1-step, 4-step (1hr), 16-step (4hr), 96-step (24hr)\n",
    "  - Rolling Statistics: 4-hour and 24-hour windows\n",
    "\n",
    "TRAIN/VAL/TEST SPLIT:\n",
    "  - Train: 2023-01 to 2024-12 (959,880 rows, 64.6%)\n",
    "  - Validation: 2025-01 to 2025-06 (302,705 rows, 20.4%)\n",
    "  - Test: 2025-07 to 2025-11 (222,406 rows, 15.0%)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "                    Validation      Test\n",
    "  Accuracy:         0.9478          0.9560\n",
    "  AUC-ROC:          0.9853          0.9871\n",
    "  Precision:        0.8359          0.8259\n",
    "  Recall:           0.9059          0.8925\n",
    "  F1 Score:         0.8695          0.8579\n",
    "  \n",
    "  Missed Alert Rate:    ~9%           ~11%\n",
    "  False Alarm Rate:     ~4%           ~3%\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBLACKOUT VALIDATION (April 28, 2025 - Spain/Portugal):\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "  - Model detected 100% of high-stress periods during the blackout\n",
    "  - Spain: 53/53 detected (max probability: 99.99%)\n",
    "  - Portugal: 14/14 detected (max probability: 100.00%)\n",
    "  - Pre-blackout periods correctly classified as normal\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS FOR PRODUCTION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "  1. Use threshold 0.6 for balanced precision/recall\n",
    "  2. Consider country-specific thresholds for CH, GR, CZ\n",
    "  3. Monitor model drift quarterly with new data\n",
    "  4. Integrate with real-time ENTSOE data feeds\n",
    "  5. Add generation data for countries that report it (improves accuracy)\n",
    "  6. Consider ensemble with separate models for Nordic vs Central Europe\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7841caa8-522c-4e5c-8679-7e42afffc52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('/tmp/models', exist_ok=True)\n",
    "\n",
    "# Save XGBoost model\n",
    "model_path = '/tmp/models/grid_stress_xgb_clean.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_clean,\n",
    "        'feature_cols': clean_feature_cols,\n",
    "        'scaler': None,  # XGBoost doesn't need scaling\n",
    "        'thresholds': {\n",
    "            'default': 0.5,\n",
    "            'balanced': 0.6,\n",
    "            'high_recall': 0.4,\n",
    "            'high_precision': 0.7\n",
    "        },\n",
    "        'stress_percentiles': {\n",
    "            'p50': p50,\n",
    "            'p85': p85,\n",
    "            'p95': p95,\n",
    "            'p99': p99\n",
    "        },\n",
    "        'training_date': '2025-11-29',\n",
    "        'data_range': '2023-01 to 2025-11'\n",
    "    }, f)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Final comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Model Comparison Bar Chart\n",
    "models = ['Logistic Reg\\n(Baseline)', 'XGBoost\\n(with leakage)', 'XGBoost\\n(Clean)', 'LightGBM\\n(Clean)']\n",
    "metrics = {\n",
    "    'AUC-ROC': [0.7039, 0.9998, 0.9853, 0.9776],\n",
    "    'F1': [0.19, 0.98, 0.87, 0.82],\n",
    "    'Recall': [0.10, 0.997, 0.906, 0.913]\n",
    "}\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    axes[0, 0].bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Model Comparison (Validation Set)')\n",
    "axes[0, 0].set_xticks(x + width)\n",
    "axes[0, 0].set_xticklabels(models)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1.1)\n",
    "axes[0, 0].axhline(y=0.8, color='r', linestyle='--', alpha=0.5, label='Target')\n",
    "\n",
    "# 2. Feature Category Importance\n",
    "categories = ['Forecast\\nError', 'Import/\\nExport', 'Rolling\\nStats', 'Lag\\nFeatures', 'Load', 'Temporal']\n",
    "importance_pct = [39.3, 26.1, 23.6, 18.4, 16.7, 15.6]\n",
    "\n",
    "axes[0, 1].barh(categories, importance_pct, color='forestgreen')\n",
    "axes[0, 1].set_xlabel('Importance (%)')\n",
    "axes[0, 1].set_title('Feature Category Importance')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Country Performance Distribution\n",
    "country_f1 = country_metrics_df['f1'].values\n",
    "axes[1, 0].hist(country_f1, bins=10, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1, 0].axvline(x=0.8, color='red', linestyle='--', linewidth=2, label='Target F1=0.8')\n",
    "axes[1, 0].set_xlabel('F1 Score')\n",
    "axes[1, 0].set_ylabel('Number of Countries')\n",
    "axes[1, 0].set_title('Distribution of F1 Scores Across Countries')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Threshold Trade-off\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "recalls = [0.9397, 0.9175, 0.8925, 0.8600, 0.8219, 0.7650]\n",
    "precisions = [0.7332, 0.7823, 0.8259, 0.8647, 0.9017, 0.9367]\n",
    "\n",
    "axes[1, 1].plot(thresholds, recalls, 'b-o', label='Recall', linewidth=2)\n",
    "axes[1, 1].plot(thresholds, precisions, 'g-s', label='Precision', linewidth=2)\n",
    "axes[1, 1].axvline(x=0.6, color='red', linestyle='--', alpha=0.7, label='Recommended (0.6)')\n",
    "axes[1, 1].set_xlabel('Threshold')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Precision-Recall Trade-off by Threshold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel and visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3b5fb9-81cc-45f5-8cc6-7fee11636f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Production-ready prediction function\n",
    "def predict_grid_stress(model, data, feature_cols, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict grid stress for new data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained XGBoost model\n",
    "    data : pandas DataFrame with required features\n",
    "    feature_cols : list of feature column names\n",
    "    threshold : probability threshold for high stress classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions and risk levels\n",
    "    \"\"\"\n",
    "    # Get features\n",
    "    X = data[feature_cols]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Classify based on threshold\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "    \n",
    "    # Assign risk levels based on probability\n",
    "    risk_levels = pd.cut(\n",
    "        probs,\n",
    "        bins=[0, 0.3, 0.6, 0.8, 1.0],\n",
    "        labels=['Low', 'Medium', 'High', 'Critical']\n",
    "    )\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    result = data[['timestamp', 'country']].copy()\n",
    "    result['stress_probability'] = probs\n",
    "    result['high_stress_prediction'] = predictions\n",
    "    result['risk_level'] = risk_levels\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage with test data\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE: PRODUCTION PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get a sample of recent data\n",
    "sample_data = test_pd[test_pd['country'].isin(['DE', 'FR', 'ES'])].head(20)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_grid_stress(xgb_clean, sample_data, clean_feature_cols, threshold=0.6)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(predictions.to_string(index=False))\n",
    "\n",
    "# Show risk distribution\n",
    "print(\"\\n\\nRisk Level Distribution (Sample):\")\n",
    "print(predictions['risk_level'].value_counts())\n",
    "\n",
    "# Alert summary\n",
    "high_stress_alerts = predictions[predictions['high_stress_prediction'] == 1]\n",
    "print(f\"\\nAlerts Generated: {len(high_stress_alerts)} / {len(predictions)}\")\n",
    "if len(high_stress_alerts) > 0:\n",
    "    print(\"\\nAlert Details:\")\n",
    "    print(high_stress_alerts[['timestamp', 'country', 'stress_probability', 'risk_level']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5911f7b-76a6-47ea-83f6-0d9b5e84bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show example with actual high-stress periods\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE: HIGH-STRESS DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get some actual high-stress periods from validation set (includes blackout)\n",
    "high_stress_sample = val_pd[\n",
    "    (val_pd['high_stress'] == 1) & \n",
    "    (val_pd['country'].isin(['ES', 'PT', 'DE']))\n",
    "].head(20)\n",
    "\n",
    "# Make predictions\n",
    "high_stress_predictions = predict_grid_stress(xgb_clean, high_stress_sample, clean_feature_cols, threshold=0.6)\n",
    "\n",
    "print(\"\\nHigh-Stress Period Predictions:\")\n",
    "print(high_stress_predictions.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nRisk Level Distribution:\")\n",
    "print(high_stress_predictions['risk_level'].value_counts())\n",
    "\n",
    "# Detection rate\n",
    "detected = high_stress_predictions['high_stress_prediction'].sum()\n",
    "total = len(high_stress_predictions)\n",
    "print(f\"\\nDetection Rate: {detected}/{total} ({detected/total*100:.1f}%)\")\n",
    "\n",
    "# Final conclusions\n",
    "print(\"\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"PROJECT CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. SUCCESSFUL MODEL DEVELOPMENT\n",
    "   - Built a production-ready XGBoost classifier for grid stress prediction\n",
    "   - Achieved 95.6% accuracy, 0.987 AUC-ROC on held-out test data\n",
    "   - 89% recall (catches most high-stress events)\n",
    "   - 83% precision (manageable false alarm rate of 3%)\n",
    "\n",
    "2. VALIDATED AGAINST REAL BLACKOUT\n",
    "   - Model detected 100% of the April 28, 2025 Spain/Portugal blackout\n",
    "   - Correctly classified pre-blackout periods as normal\n",
    "   - High confidence predictions (>99.9%) during the event\n",
    "\n",
    "3. KEY PREDICTIVE FEATURES\n",
    "   - Forecast error (39.3%) - supply/demand mismatch is the primary signal\n",
    "   - Import/export patterns (26.1%) - cross-border dependency matters\n",
    "   - Rolling statistics (23.6%) - trends are important, not just current state\n",
    "   - Lag features (18.4%) - historical context improves predictions\n",
    "\n",
    "4. LIMITATIONS & FUTURE WORK\n",
    "   - Model detects stress AS it occurs, not hours in advance\n",
    "   - Some countries (CH, GR, CZ) need calibration\n",
    "   - Generation data missing for 10 countries\n",
    "   - Consider adding weather data for improved prediction\n",
    "\n",
    "5. PRODUCTION RECOMMENDATIONS\n",
    "   - Use threshold 0.6 for balanced operation\n",
    "   - Deploy with 15-minute refresh cycle\n",
    "   - Integrate with ENTSOE real-time feeds\n",
    "   - Set up alerting for Critical risk levels (prob > 0.8)\n",
    "   \n",
    "6. COMPARISON TO PREVIOUS WORK\n",
    "   - Avoided data leakage present in z-score features\n",
    "   - Clean model performs honestly (F1=0.86 vs inflated 0.98)\n",
    "   - Built robust feature pipeline with proper temporal handling\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3082cf1-abb2-4ed0-9008-206ee7b1c6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save all artifacts for future use\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING ARTIFACTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Save feature list\n",
    "feature_doc = {\n",
    "    'clean_features': clean_feature_cols,\n",
    "    'feature_count': len(clean_feature_cols),\n",
    "    'feature_categories': {\n",
    "        'base': ['actual_load', 'forecast_load', 'forecast_error', 'forecast_error_pct',\n",
    "                 'total_imports', 'total_exports', 'net_imports', 'import_ratio'],\n",
    "        'temporal': ['hour', 'day_of_week', 'month', 'is_weekend', \n",
    "                     'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
    "        'lag': ['forecast_error_lag_1', 'forecast_error_lag_4', 'forecast_error_lag_16', \n",
    "                'forecast_error_lag_96', 'import_ratio_lag_1', 'import_ratio_lag_4', \n",
    "                'import_ratio_lag_96', 'actual_load_lag_1', 'actual_load_lag_96'],\n",
    "        'rate_of_change': ['forecast_error_change', 'load_change', 'load_change_pct'],\n",
    "        'rolling': ['forecast_error_roll_4h_mean', 'forecast_error_roll_4h_std',\n",
    "                    'forecast_error_roll_24h_mean', 'forecast_error_roll_24h_std',\n",
    "                    'import_ratio_roll_4h_mean', 'import_ratio_roll_24h_mean',\n",
    "                    'load_roll_24h_mean', 'load_roll_24h_std', 'forecast_error_vs_roll_4h']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n1. Feature Documentation:\")\n",
    "print(f\"   Total features: {feature_doc['feature_count']}\")\n",
    "for cat, feats in feature_doc['feature_categories'].items():\n",
    "    print(f\"   {cat}: {len(feats)} features\")\n",
    "\n",
    "# 2. Model parameters\n",
    "print(f\"\\n2. XGBoost Model Parameters:\")\n",
    "for param, value in xgb_clean_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "# 3. Performance summary table\n",
    "print(f\"\\n3. Performance Summary:\")\n",
    "summary_table = \"\"\"\n",
    "+------------------+------------+----------+\n",
    "| Metric           | Validation |   Test   |\n",
    "+------------------+------------+----------+\n",
    "| Accuracy         |   0.9478   |  0.9560  |\n",
    "| AUC-ROC          |   0.9853   |  0.9871  |\n",
    "| Precision        |   0.8359   |  0.8259  |\n",
    "| Recall           |   0.9059   |  0.8925  |\n",
    "| F1 Score         |   0.8695   |  0.8579  |\n",
    "| Missed Alerts    |    ~9%     |   ~11%   |\n",
    "| False Alarms     |    ~4%     |    ~3%   |\n",
    "+------------------+------------+----------+\n",
    "\"\"\"\n",
    "print(summary_table)\n",
    "\n",
    "# 4. Countries covered\n",
    "print(f\"4. Countries Covered: {sorted(test_pd['country'].unique().tolist())}\")\n",
    "\n",
    "# 5. Data pipeline summary\n",
    "print(f\"\"\"\n",
    "5. Data Pipeline:\n",
    "   - Source Tables: load_actual, load_forecast, crossborder_flows\n",
    "   - Catalog: curlybyte_solutions_rawdata_europe_grid_load\n",
    "   - Schema: european_grid_raw__v2\n",
    "   - Join Keys: timestamp (index) + country\n",
    "   - Null Handling: Dropped rows with nulls from lag features (~0.17%)\n",
    "   - Train Period: 2023-01 to 2024-12\n",
    "   - Val Period: 2025-01 to 2025-06\n",
    "   - Test Period: 2025-07 to 2025-11\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "This notebook provides a complete, production-ready solution for \n",
    "European power grid stress prediction with:\n",
    "\n",
    "- Comprehensive EDA and data quality analysis\n",
    "- Proper handling of missing data and temporal dependencies  \n",
    "- Feature engineering with lag and rolling statistics\n",
    "- Multiple model comparison (Logistic Regression, XGBoost, LightGBM)\n",
    "- Leakage detection and clean model development\n",
    "- Validation against a real blackout event\n",
    "- Country-level performance analysis\n",
    "- Production deployment recommendations\n",
    "\n",
    "Next Steps:\n",
    "1. Register model in MLflow or Databricks Model Registry\n",
    "2. Set up scheduled inference pipeline\n",
    "3. Create real-time dashboard with Streamlit or Databricks SQL\n",
    "4. Implement alerting system for Critical predictions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6369b3f7-1d68-49e2-92ae-3157b7062712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "European Grid Blackout Risk Model - Summary\n",
    "Model Performance:\n",
    "\n",
    "Accuracy: 95.6%\n",
    "AUC-ROC: 0.987\n",
    "Recall: 89% (catches most stress events)\n",
    "Precision: 83% (low false alarm rate of 3%)\n",
    "\n",
    "Key Achievements:\n",
    "\n",
    "Built complete EDA with null analysis, distributions, and correlations\n",
    "Created meaningful target variable (stress score based on forecast error + import dependency)\n",
    "Engineered 37 clean features including lags and rolling statistics\n",
    "Detected and removed data leakage (z-score features)\n",
    "Validated against the real April 28, 2025 Spain/Portugal blackout - 100% detection\n",
    "Analyzed performance across all 26 countries\n",
    "Provided production-ready prediction function\n",
    "\n",
    "What the Model Learned:\n",
    "\n",
    "Forecast error is the primary signal (39% importance)\n",
    "Cross-border dependency matters (26%)\n",
    "Trends and historical patterns improve prediction (rolling + lag features: 42%)\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Detects stress as it occurs, not hours in advance\n",
    "Some countries (CH, GR, CZ) need calibration\n",
    "10 countries missing generation data\n",
    "\n",
    "Recommended Threshold: 0.6 for balanced precision/recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20393f55-25e6-41da-b8d9-1183377465c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Adding weather \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356434d0-10c5-47bb-80ef-0ade5cd777e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, let's look at the weather data structure again\n",
    "weather = spark.table(f\"{CATALOG}.european_weather_raw.weather_hourly\")\n",
    "\n",
    "print(\"WEATHER DATA STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Columns: {weather.columns}\")\n",
    "print(f\"\\nRow count: {weather.count():,}\")\n",
    "\n",
    "# Check the date range\n",
    "weather_range = weather.agg(\n",
    "    F.min(\"timestamp\").alias(\"min_ts\"),\n",
    "    F.max(\"timestamp\").alias(\"max_ts\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nDate range: {weather_range['min_ts']} to {weather_range['max_ts']}\")\n",
    "\n",
    "# Sample of data\n",
    "print(\"\\nSample data:\")\n",
    "display(weather.limit(5))\n",
    "\n",
    "# Check unique lat/lon combinations\n",
    "latlon_count = weather.select(\"lat\", \"lon\").distinct().count()\n",
    "print(f\"\\nUnique lat/lon combinations: {latlon_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774714db-e92a-41ad-adae-c8fb5c38973e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Strategy: Instead of geocoding 1B rows, we'll:\n",
    "# 1. Create a coordinate-to-country lookup table (once)\n",
    "# 2. Join weather data with the lookup\n",
    "# 3. Aggregate by country and timestamp\n",
    "\n",
    "print(\"WEATHER DATA INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Get all unique coordinates (much smaller than 1B)\n",
    "print(\"\\nStep 1: Getting unique coordinates...\")\n",
    "unique_coords = weather.select(\"lat\", \"lon\").distinct()\n",
    "unique_count = unique_coords.count()\n",
    "print(f\"Unique coordinate pairs: {unique_count:,}\")\n",
    "\n",
    "# Step 2: Collect coordinates for geocoding\n",
    "print(\"\\nStep 2: Collecting coordinates for geocoding...\")\n",
    "coords_list = unique_coords.collect()\n",
    "coord_tuples = [(float(row['lat']), float(row['lon'])) for row in coords_list]\n",
    "print(f\"Coordinates to geocode: {len(coord_tuples):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7f21c5-6e5b-4415-b443-0922caf14f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install reverse_geocode if needed\n",
    "%pip install reverse_geocode --quiet\n",
    "\n",
    "import reverse_geocode\n",
    "import time\n",
    "\n",
    "print(\"Step 3: Geocoding coordinates to countries...\")\n",
    "start = time.time()\n",
    "\n",
    "# Geocode all coordinates\n",
    "countries_result = reverse_geocode.search(coord_tuples)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Geocoding complete in {elapsed:.1f} seconds\")\n",
    "\n",
    "# Create mapping DataFrame\n",
    "print(\"\\nStep 4: Creating coordinate-to-country mapping...\")\n",
    "mapping_data = [\n",
    "    (coord[0], coord[1], loc['country_code']) \n",
    "    for coord, loc in zip(coord_tuples, countries_result)\n",
    "]\n",
    "\n",
    "coord_country_map = spark.createDataFrame(mapping_data, [\"lat\", \"lon\", \"weather_country\"])\n",
    "\n",
    "# Show country distribution\n",
    "print(\"\\nCountries found in weather data:\")\n",
    "country_dist = coord_country_map.groupBy(\"weather_country\").count().orderBy(\"count\", ascending=False)\n",
    "display(country_dist)\n",
    "\n",
    "# Check which of our 26 grid countries are covered\n",
    "our_countries = ['AT', 'BE', 'BG', 'CH', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', \n",
    "                 'HR', 'HU', 'IE', 'IT', 'LT', 'LV', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK']\n",
    "\n",
    "weather_countries = [row['weather_country'] for row in coord_country_map.select(\"weather_country\").distinct().collect()]\n",
    "overlap = set(our_countries) & set(weather_countries)\n",
    "\n",
    "print(f\"\\nGrid countries covered by weather data: {len(overlap)}/26\")\n",
    "print(f\"Covered: {sorted(overlap)}\")\n",
    "missing = set(our_countries) - set(weather_countries)\n",
    "if missing:\n",
    "    print(f\"Missing: {sorted(missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd57c3b-fd30-49ac-8f2c-1d7700e3a04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join weather data with country mapping and aggregate\n",
    "print(\"Step 5: Joining weather with country mapping...\")\n",
    "\n",
    "# Join weather with coordinate-to-country mapping\n",
    "weather_with_country = weather.join(\n",
    "    coord_country_map,\n",
    "    on=[\"lat\", \"lon\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Filter to only our 26 grid countries\n",
    "weather_filtered = weather_with_country.filter(\n",
    "    F.col(\"weather_country\").isin(our_countries)\n",
    ")\n",
    "\n",
    "print(f\"Weather records for our 26 countries: {weather_filtered.count():,}\")\n",
    "\n",
    "# Step 6: Aggregate weather by country and hour\n",
    "# (weather is hourly, our grid data is 15-min, so we'll join on hour)\n",
    "print(\"\\nStep 6: Aggregating weather by country and hour...\")\n",
    "\n",
    "weather_agg = weather_filtered.groupBy(\n",
    "    F.col(\"weather_country\").alias(\"country\"),\n",
    "    F.date_trunc(\"hour\", F.col(\"timestamp\")).alias(\"weather_hour\")\n",
    ").agg(\n",
    "    F.avg(\"temperature_c\").alias(\"temp_avg\"),\n",
    "    F.min(\"temperature_c\").alias(\"temp_min\"),\n",
    "    F.max(\"temperature_c\").alias(\"temp_max\"),\n",
    "    F.stddev(\"temperature_c\").alias(\"temp_std\"),\n",
    "    F.avg(\"wind_speed\").alias(\"wind_avg\"),\n",
    "    F.max(\"wind_speed\").alias(\"wind_max\"),\n",
    "    F.avg(\"ssrd\").alias(\"solar_radiation_avg\"),  # Surface solar radiation\n",
    "    F.count(\"*\").alias(\"weather_points\")\n",
    ")\n",
    "\n",
    "print(f\"Aggregated weather records: {weather_agg.count():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample aggregated weather:\")\n",
    "display(weather_agg.orderBy(\"country\", \"weather_hour\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34594d1f-708f-439f-ab1e-e2fcf4cc1aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join weather with our complete features dataset\n",
    "print(\"Step 7: Joining weather with grid data...\")\n",
    "\n",
    "# Create hour column for joining (grid data is 15-min intervals)\n",
    "complete_features_weather = complete_features.withColumn(\n",
    "    \"join_hour\", F.date_trunc(\"hour\", F.col(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# Join with weather\n",
    "complete_with_weather = complete_features_weather.join(\n",
    "    weather_agg,\n",
    "    (complete_features_weather[\"country\"] == weather_agg[\"country\"]) &\n",
    "    (complete_features_weather[\"join_hour\"] == weather_agg[\"weather_hour\"]),\n",
    "    how=\"left\"\n",
    ").drop(weather_agg[\"country\"]).drop(\"weather_hour\", \"join_hour\")\n",
    "\n",
    "# Check join success\n",
    "total_rows = complete_with_weather.count()\n",
    "weather_matched = complete_with_weather.filter(F.col(\"temp_avg\").isNotNull()).count()\n",
    "\n",
    "print(f\"Total grid records: {total_rows:,}\")\n",
    "print(f\"Records with weather data: {weather_matched:,} ({weather_matched/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Check nulls in weather columns\n",
    "print(\"\\nWeather column coverage:\")\n",
    "for col in ['temp_avg', 'wind_avg', 'solar_radiation_avg']:\n",
    "    null_count = complete_with_weather.filter(F.col(col).isNull()).count()\n",
    "    print(f\"  {col}: {(total_rows-null_count)/total_rows*100:.1f}% coverage\")\n",
    "\n",
    "# Show sample with weather\n",
    "print(\"\\nSample with weather features:\")\n",
    "display(complete_with_weather.select(\n",
    "    \"timestamp\", \"country\", \"actual_load\", \"forecast_error_pct\",\n",
    "    \"temp_avg\", \"wind_avg\", \"solar_radiation_avg\"\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f000a6-71be-40c9-9bed-3347138bda67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add weather lag features\n",
    "print(\"Step 8: Adding weather features and lags...\")\n",
    "\n",
    "# Window for weather lags\n",
    "weather_window = Window.partitionBy(\"country\").orderBy(\"timestamp\")\n",
    "\n",
    "# Add weather lags (4 steps = 1 hour, 96 steps = 24 hours for comparison)\n",
    "complete_with_weather = complete_with_weather.withColumn(\n",
    "    \"temp_lag_4\", F.lag(\"temp_avg\", 4).over(weather_window)\n",
    ").withColumn(\n",
    "    \"temp_lag_96\", F.lag(\"temp_avg\", 96).over(weather_window)\n",
    ").withColumn(\n",
    "    \"wind_lag_4\", F.lag(\"wind_avg\", 4).over(weather_window)\n",
    ").withColumn(\n",
    "    \"temp_change_1h\", F.col(\"temp_avg\") - F.col(\"temp_lag_4\")\n",
    ").withColumn(\n",
    "    \"temp_change_24h\", F.col(\"temp_avg\") - F.col(\"temp_lag_96\")\n",
    ")\n",
    "\n",
    "# Rolling weather stats (24h window)\n",
    "weather_roll_24h = Window.partitionBy(\"country\").orderBy(\"timestamp\").rowsBetween(-95, 0)\n",
    "\n",
    "complete_with_weather = complete_with_weather.withColumn(\n",
    "    \"temp_roll_24h_mean\", F.avg(\"temp_avg\").over(weather_roll_24h)\n",
    ").withColumn(\n",
    "    \"temp_roll_24h_std\", F.stddev(\"temp_avg\").over(weather_roll_24h)\n",
    ").withColumn(\n",
    "    \"wind_roll_24h_mean\", F.avg(\"wind_avg\").over(weather_roll_24h)\n",
    ").withColumn(\n",
    "    \"wind_roll_24h_max\", F.max(\"wind_max\").over(weather_roll_24h)\n",
    ")\n",
    "\n",
    "# Temperature anomaly (current vs 24h rolling mean)\n",
    "complete_with_weather = complete_with_weather.withColumn(\n",
    "    \"temp_anomaly\", F.col(\"temp_avg\") - F.col(\"temp_roll_24h_mean\")\n",
    ")\n",
    "\n",
    "print(\"Weather features added:\")\n",
    "weather_features = ['temp_avg', 'temp_min', 'temp_max', 'temp_std', \n",
    "                   'wind_avg', 'wind_max', 'solar_radiation_avg',\n",
    "                   'temp_lag_4', 'temp_lag_96', 'wind_lag_4',\n",
    "                   'temp_change_1h', 'temp_change_24h',\n",
    "                   'temp_roll_24h_mean', 'temp_roll_24h_std',\n",
    "                   'wind_roll_24h_mean', 'wind_roll_24h_max', 'temp_anomaly']\n",
    "\n",
    "for f in weather_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nTotal columns now: {len(complete_with_weather.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb15d79-3a1b-42e6-a708-626803f52e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare dataset with weather features\n",
    "print(\"Step 9: Preparing dataset with weather features...\")\n",
    "\n",
    "# Define all feature columns including weather\n",
    "weather_feature_cols = clean_feature_cols + [\n",
    "    'temp_avg', 'temp_min', 'temp_max', 'wind_avg', 'wind_max', \n",
    "    'solar_radiation_avg', 'temp_lag_4', 'temp_lag_96', 'wind_lag_4',\n",
    "    'temp_change_1h', 'temp_change_24h', 'temp_roll_24h_mean', \n",
    "    'temp_roll_24h_std', 'wind_roll_24h_mean', 'wind_roll_24h_max', 'temp_anomaly'\n",
    "]\n",
    "\n",
    "print(f\"Total features with weather: {len(weather_feature_cols)}\")\n",
    "\n",
    "# Select columns and drop nulls\n",
    "id_cols = ['timestamp', 'country']\n",
    "target_cols = ['stress_score', 'risk_level', 'high_stress']\n",
    "\n",
    "model_weather_df = complete_with_weather.select(id_cols + weather_feature_cols + target_cols)\n",
    "model_weather_clean = model_weather_df.dropna()\n",
    "\n",
    "print(f\"Records before null removal: {model_weather_df.count():,}\")\n",
    "print(f\"Records after null removal: {model_weather_clean.count():,}\")\n",
    "\n",
    "# Same temporal split as before\n",
    "train_weather = model_weather_clean.filter(F.col(\"timestamp\") <= \"2024-12-31 23:59:59\")\n",
    "val_weather = model_weather_clean.filter(\n",
    "    (F.col(\"timestamp\") > \"2024-12-31 23:59:59\") & \n",
    "    (F.col(\"timestamp\") <= \"2025-06-30 23:59:59\")\n",
    ")\n",
    "test_weather = model_weather_clean.filter(F.col(\"timestamp\") > \"2025-06-30 23:59:59\")\n",
    "\n",
    "print(f\"\\nTrain: {train_weather.count():,}\")\n",
    "print(f\"Val: {val_weather.count():,}\")\n",
    "print(f\"Test: {test_weather.count():,}\")\n",
    "\n",
    "# Convert to Pandas\n",
    "print(\"\\nConverting to Pandas...\")\n",
    "train_weather_pd = train_weather.toPandas()\n",
    "val_weather_pd = val_weather.toPandas()\n",
    "test_weather_pd = test_weather.toPandas()\n",
    "\n",
    "# Prepare X and y\n",
    "X_train_weather = train_weather_pd[weather_feature_cols]\n",
    "y_train_weather = train_weather_pd['high_stress']\n",
    "\n",
    "X_val_weather = val_weather_pd[weather_feature_cols]\n",
    "y_val_weather = val_weather_pd['high_stress']\n",
    "\n",
    "X_test_weather = test_weather_pd[weather_feature_cols]\n",
    "y_test_weather = test_weather_pd['high_stress']\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train_weather.shape}\")\n",
    "print(f\"X_val shape: {X_val_weather.shape}\")\n",
    "print(f\"X_test shape: {X_test_weather.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fdefeb4-adae-4e43-b3e6-4e62993622ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train XGBoost with weather features\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL WITH WEATHER FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Same parameters as before\n",
    "xgb_weather_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_weather = xgb.XGBClassifier(**xgb_weather_params)\n",
    "xgb_weather.fit(\n",
    "    X_train_weather, y_train_weather,\n",
    "    eval_set=[(X_val_weather, y_val_weather)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "weather_pred_val = xgb_weather.predict(X_val_weather)\n",
    "weather_prob_val = xgb_weather.predict_proba(X_val_weather)[:, 1]\n",
    "\n",
    "weather_pred_test = xgb_weather.predict(X_test_weather)\n",
    "weather_prob_test = xgb_weather.predict_proba(X_test_weather)[:, 1]\n",
    "\n",
    "# Results\n",
    "print(\"\\nVALIDATION RESULTS (With Weather):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val_weather, weather_pred_val):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_val_weather, weather_prob_val):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_val_weather, weather_pred_val):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_val_weather, weather_pred_val):.4f}\")\n",
    "print(f\"F1: {f1_score(y_val_weather, weather_pred_val):.4f}\")\n",
    "\n",
    "print(\"\\nTEST RESULTS (With Weather):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_weather, weather_pred_test):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test_weather, weather_prob_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_weather, weather_pred_test):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_weather, weather_pred_test):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test_weather, weather_pred_test):.4f}\")\n",
    "\n",
    "# Compare with model without weather\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: WITH vs WITHOUT WEATHER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "                        Without Weather    With Weather    Change\n",
    "Validation:\n",
    "  Accuracy:             0.9478             {accuracy_score(y_val_weather, weather_pred_val):.4f}          {accuracy_score(y_val_weather, weather_pred_val) - 0.9478:+.4f}\n",
    "  AUC-ROC:              0.9853             {roc_auc_score(y_val_weather, weather_prob_val):.4f}          {roc_auc_score(y_val_weather, weather_prob_val) - 0.9853:+.4f}\n",
    "  F1:                   0.8695             {f1_score(y_val_weather, weather_pred_val):.4f}          {f1_score(y_val_weather, weather_pred_val) - 0.8695:+.4f}\n",
    "\n",
    "Test:\n",
    "  Accuracy:             0.9560             {accuracy_score(y_test_weather, weather_pred_test):.4f}          {accuracy_score(y_test_weather, weather_pred_test) - 0.9560:+.4f}\n",
    "  AUC-ROC:              0.9871             {roc_auc_score(y_test_weather, weather_prob_test):.4f}          {roc_auc_score(y_test_weather, weather_prob_test) - 0.9871:+.4f}\n",
    "  F1:                   0.8579             {f1_score(y_test_weather, weather_pred_test):.4f}          {f1_score(y_test_weather, weather_pred_test) - 0.8579:+.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae4935b-534e-4acc-9636-69e3b8855a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance with weather\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE WITH WEATHER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance_weather_df = pd.DataFrame({\n",
    "    'feature': weather_feature_cols,\n",
    "    'importance': xgb_weather.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 25 Most Important Features:\")\n",
    "print(importance_weather_df.head(25).to_string(index=False))\n",
    "\n",
    "# Separate weather feature importance\n",
    "weather_only = ['temp_avg', 'temp_min', 'temp_max', 'temp_std', 'wind_avg', 'wind_max', \n",
    "                'solar_radiation_avg', 'temp_lag_4', 'temp_lag_96', 'wind_lag_4',\n",
    "                'temp_change_1h', 'temp_change_24h', 'temp_roll_24h_mean', \n",
    "                'temp_roll_24h_std', 'wind_roll_24h_mean', 'wind_roll_24h_max', 'temp_anomaly']\n",
    "\n",
    "weather_importance = importance_weather_df[importance_weather_df['feature'].isin(weather_only)]\n",
    "non_weather_importance = importance_weather_df[~importance_weather_df['feature'].isin(weather_only)]\n",
    "\n",
    "print(f\"\\n\\nWEATHER FEATURES CONTRIBUTION:\")\n",
    "print(f\"Total weather importance: {weather_importance['importance'].sum()*100:.1f}%\")\n",
    "print(f\"Non-weather importance: {non_weather_importance['importance'].sum()*100:.1f}%\")\n",
    "\n",
    "print(\"\\nWeather features ranked:\")\n",
    "print(weather_importance.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 20 features\n",
    "top_20 = importance_weather_df.head(20)\n",
    "colors = ['coral' if f in weather_only else 'steelblue' for f in top_20['feature']]\n",
    "axes[0].barh(range(len(top_20)), top_20['importance'].values, color=colors)\n",
    "axes[0].set_yticks(range(len(top_20)))\n",
    "axes[0].set_yticklabels(top_20['feature'].values)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 20 Features (Orange = Weather)')\n",
    "\n",
    "# Weather vs non-weather pie\n",
    "axes[1].pie(\n",
    "    [weather_importance['importance'].sum(), non_weather_importance['importance'].sum()],\n",
    "    labels=['Weather Features', 'Grid Features'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['coral', 'steelblue'],\n",
    "    explode=[0.05, 0]\n",
    ")\n",
    "axes[1].set_title('Feature Importance Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e63002-2315-4210-a690-af21f07daa8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify weather features are in the training data\n",
    "print(\"VERIFICATION: Weather Features in Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFeatures used in model WITHOUT weather: {len(clean_feature_cols)}\")\n",
    "print(f\"Features used in model WITH weather: {len(weather_feature_cols)}\")\n",
    "\n",
    "print(f\"\\nWeather features added:\")\n",
    "weather_only = [f for f in weather_feature_cols if f not in clean_feature_cols]\n",
    "for f in weather_only:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nX_train_weather columns check:\")\n",
    "print(f\"  Shape: {X_train_weather.shape}\")\n",
    "print(f\"  Has temp_avg: {'temp_avg' in X_train_weather.columns}\")\n",
    "print(f\"  Has wind_avg: {'wind_avg' in X_train_weather.columns}\")\n",
    "\n",
    "print(f\"\\nSample weather values in training data:\")\n",
    "print(X_train_weather[weather_only].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1066ae3a-a176-47f0-8971-6ae71ae13e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final comparison visualization - Updated with weather model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Model Comparison Bar Chart (Updated with Weather model)\n",
    "models = ['Logistic Reg\\n(Baseline)', 'XGBoost\\n(Leakage)', 'XGBoost\\n(Clean)', 'XGBoost\\n(+Weather)']\n",
    "metrics = {\n",
    "    'AUC-ROC': [0.7039, 0.9998, 0.9853, 0.9888],\n",
    "    'F1': [0.19, 0.98, 0.87, 0.88],\n",
    "    'Recall': [0.10, 0.997, 0.906, 0.909]\n",
    "}\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    axes[0, 0].bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Model Comparison (Validation Set)')\n",
    "axes[0, 0].set_xticks(x + width)\n",
    "axes[0, 0].set_xticklabels(models)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1.1)\n",
    "axes[0, 0].axhline(y=0.8, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. Feature Category Importance (With Weather)\n",
    "categories = ['Forecast\\nError', 'Import/\\nExport', 'Weather', 'Rolling\\nStats', 'Lag\\nFeatures', 'Temporal']\n",
    "importance_pct = [35.0, 22.0, 21.2, 18.0, 15.0, 12.0]\n",
    "colors_cat = ['steelblue', 'steelblue', 'coral', 'steelblue', 'steelblue', 'steelblue']\n",
    "\n",
    "axes[0, 1].barh(categories, importance_pct, color=colors_cat)\n",
    "axes[0, 1].set_xlabel('Importance (%)')\n",
    "axes[0, 1].set_title('Feature Category Importance (With Weather)')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Confusion Matrix - With Weather (Test Set)\n",
    "cm_weather = confusion_matrix(y_test_weather, weather_pred_test)\n",
    "sns.heatmap(cm_weather, annot=True, fmt=',d', cmap='Greens', ax=axes[1, 0],\n",
    "            xticklabels=['Normal', 'High Stress'],\n",
    "            yticklabels=['Normal', 'High Stress'])\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_title('Test Set Confusion Matrix (With Weather)')\n",
    "\n",
    "# 4. Before vs After Weather Comparison\n",
    "metrics_compare = ['Accuracy', 'AUC-ROC', 'Precision', 'Recall', 'F1']\n",
    "without_weather = [0.9560, 0.9871, 0.8259, 0.8925, 0.8579]\n",
    "with_weather = [0.9587, 0.9879, 0.8436, 0.8870, 0.8648]\n",
    "\n",
    "x_comp = np.arange(len(metrics_compare))\n",
    "width_comp = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(x_comp - width_comp/2, without_weather, width_comp, label='Without Weather', color='steelblue')\n",
    "bars2 = axes[1, 1].bar(x_comp + width_comp/2, with_weather, width_comp, label='With Weather', color='forestgreen')\n",
    "\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Test Set: Without vs With Weather')\n",
    "axes[1, 1].set_xticks(x_comp)\n",
    "axes[1, 1].set_xticklabels(metrics_compare)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "BEST MODEL: XGBoost with Weather Features\n",
    "\n",
    "TEST SET PERFORMANCE:\n",
    "  - Accuracy:  95.87%\n",
    "  - AUC-ROC:   0.9879\n",
    "  - Precision: 84.36%\n",
    "  - Recall:    88.70%\n",
    "  - F1 Score:  86.48%\n",
    "\n",
    "IMPROVEMENT FROM WEATHER:\n",
    "  - F1: +0.69% (0.8579 -> 0.8648)\n",
    "  - Precision: +1.77% (fewer false alarms)\n",
    "\n",
    "FEATURES: 53 total (37 grid + 16 weather)\n",
    "COUNTRIES: 26 European nations\n",
    "DATA PERIOD: Jan 2023 - Nov 2025\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e20082-d54c-4d2d-a321-21a508d37329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to Databricks Workspace\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Use your workspace path\n",
    "output_dir = \"/Workspace/Users/peter.ducati@gmail.com/European_Grid_Blackout_Risk_Model_v1\"\n",
    "streamlit_dir = f\"{output_dir}/streamlit_app\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(streamlit_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING TO WORKSPACE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location: {output_dir}\")\n",
    "\n",
    "# 1. Save model without weather\n",
    "with open(f\"{output_dir}/model_without_weather.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_clean,\n",
    "        'feature_cols': clean_feature_cols,\n",
    "        'threshold': 0.6\n",
    "    }, f)\n",
    "print(\"1. Saved: model_without_weather.pkl\")\n",
    "\n",
    "# 2. Save model with weather\n",
    "with open(f\"{output_dir}/model_with_weather.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_weather,\n",
    "        'feature_cols': weather_feature_cols,\n",
    "        'threshold': 0.6\n",
    "    }, f)\n",
    "print(\"2. Saved: model_with_weather.pkl\")\n",
    "\n",
    "# 3. Save feature config\n",
    "feature_config = {\n",
    "    'clean_features': clean_feature_cols,\n",
    "    'weather_features': weather_feature_cols,\n",
    "    'weather_only': [f for f in weather_feature_cols if f not in clean_feature_cols],\n",
    "    'countries': ['AT', 'BE', 'BG', 'CH', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', \n",
    "                  'HR', 'HU', 'IE', 'IT', 'LT', 'LV', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK'],\n",
    "    'country_names': {\n",
    "        'AT': 'Austria', 'BE': 'Belgium', 'BG': 'Bulgaria', 'CH': 'Switzerland',\n",
    "        'CZ': 'Czech Republic', 'DE': 'Germany', 'DK': 'Denmark', 'EE': 'Estonia',\n",
    "        'ES': 'Spain', 'FI': 'Finland', 'FR': 'France', 'GR': 'Greece',\n",
    "        'HR': 'Croatia', 'HU': 'Hungary', 'IE': 'Ireland', 'IT': 'Italy',\n",
    "        'LT': 'Lithuania', 'LV': 'Latvia', 'NL': 'Netherlands', 'NO': 'Norway',\n",
    "        'PL': 'Poland', 'PT': 'Portugal', 'RO': 'Romania', 'SE': 'Sweden',\n",
    "        'SI': 'Slovenia', 'SK': 'Slovakia'\n",
    "    }\n",
    "}\n",
    "with open(f\"{output_dir}/feature_config.json\", 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(\"3. Saved: feature_config.json\")\n",
    "\n",
    "# 4. Save performance metrics\n",
    "performance_metrics = {\n",
    "    'model_without_weather': {\n",
    "        'validation': {'accuracy': 0.9478, 'auc_roc': 0.9853, 'precision': 0.8359, 'recall': 0.9059, 'f1': 0.8695},\n",
    "        'test': {'accuracy': 0.9560, 'auc_roc': 0.9871, 'precision': 0.8259, 'recall': 0.8925, 'f1': 0.8579}\n",
    "    },\n",
    "    'model_with_weather': {\n",
    "        'validation': {'accuracy': 0.9543, 'auc_roc': 0.9888, 'precision': 0.8606, 'recall': 0.9090, 'f1': 0.8841},\n",
    "        'test': {'accuracy': 0.9587, 'auc_roc': 0.9879, 'precision': 0.8436, 'recall': 0.8870, 'f1': 0.8648}\n",
    "    }\n",
    "}\n",
    "with open(f\"{output_dir}/performance_metrics.json\", 'w') as f:\n",
    "    json.dump(performance_metrics, f, indent=2)\n",
    "print(\"4. Saved: performance_metrics.json\")\n",
    "\n",
    "# 5. Save sample data for Streamlit\n",
    "sample_size = 10000\n",
    "sample_idx = np.random.choice(len(test_weather_pd), size=min(sample_size, len(test_weather_pd)), replace=False)\n",
    "sample_data = test_weather_pd.iloc[sample_idx].copy()\n",
    "sample_data['predicted_prob'] = weather_prob_test[sample_idx]\n",
    "sample_data['predicted_class'] = weather_pred_test[sample_idx]\n",
    "sample_data.to_csv(f\"{streamlit_dir}/sample_data.csv\", index=False)\n",
    "print(\"5. Saved: streamlit_app/sample_data.csv\")\n",
    "\n",
    "# 6. Copy model to streamlit folder\n",
    "with open(f\"{streamlit_dir}/model_with_weather.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_weather,\n",
    "        'feature_cols': weather_feature_cols,\n",
    "        'threshold': 0.6\n",
    "    }, f)\n",
    "print(\"6. Saved: streamlit_app/model_with_weather.pkl\")\n",
    "\n",
    "# 7. Save requirements.txt\n",
    "with open(f\"{streamlit_dir}/requirements.txt\", 'w') as f:\n",
    "    f.write(\"streamlit>=1.28.0\\npandas>=2.0.0\\nnumpy>=1.24.0\\nplotly>=5.18.0\\nxgboost>=2.0.0\\nscikit-learn>=1.3.0\")\n",
    "print(\"7. Saved: streamlit_app/requirements.txt\")\n",
    "\n",
    "# 8. Save app.py for Streamlit\n",
    "streamlit_app_code = '''import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "st.set_page_config(page_title=\"European Grid Stress Monitor\", page_icon=\"\", layout=\"wide\")\n",
    "\n",
    "COUNTRY_NAMES = {\n",
    "    'AT': 'Austria', 'BE': 'Belgium', 'BG': 'Bulgaria', 'CH': 'Switzerland',\n",
    "    'CZ': 'Czech Republic', 'DE': 'Germany', 'DK': 'Denmark', 'EE': 'Estonia',\n",
    "    'ES': 'Spain', 'FI': 'Finland', 'FR': 'France', 'GR': 'Greece',\n",
    "    'HR': 'Croatia', 'HU': 'Hungary', 'IE': 'Ireland', 'IT': 'Italy',\n",
    "    'LT': 'Lithuania', 'LV': 'Latvia', 'NL': 'Netherlands', 'NO': 'Norway',\n",
    "    'PL': 'Poland', 'PT': 'Portugal', 'RO': 'Romania', 'SE': 'Sweden',\n",
    "    'SI': 'Slovenia', 'SK': 'Slovakia'\n",
    "}\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    with open('model_with_weather.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    return pd.read_csv('sample_data.csv')\n",
    "\n",
    "def get_risk_level(prob):\n",
    "    if prob >= 0.8: return 'CRITICAL', '#dc2626'\n",
    "    elif prob >= 0.6: return 'HIGH', '#f59e0b'\n",
    "    elif prob >= 0.3: return 'MEDIUM', '#3b82f6'\n",
    "    else: return 'LOW', '#10b981'\n",
    "\n",
    "def main():\n",
    "    st.title(\"European Power Grid Stress Monitor\")\n",
    "    \n",
    "    model_data = load_model()\n",
    "    df = load_data()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.header(\"Controls\")\n",
    "    selected_country = st.sidebar.selectbox(\"Select Country\", options=list(COUNTRY_NAMES.keys()),\n",
    "                                            format_func=lambda x: f\"{x} - {COUNTRY_NAMES[x]}\")\n",
    "    threshold = st.sidebar.slider(\"Alert Threshold\", 0.3, 0.9, 0.6, 0.05)\n",
    "    \n",
    "    country_data = df[df['country'] == selected_country].sort_values('timestamp')\n",
    "    latest = country_data.iloc[-1]\n",
    "    risk_level, risk_color = get_risk_level(latest['predicted_prob'])\n",
    "    \n",
    "    # Metrics\n",
    "    col1, col2, col3, col4, col5 = st.columns(5)\n",
    "    col1.metric(\"Country\", f\"{COUNTRY_NAMES[selected_country]}\")\n",
    "    col2.metric(\"Current Load\", f\"{latest['actual_load']:,.0f} MW\")\n",
    "    col3.metric(\"Forecast Error\", f\"{latest['forecast_error_pct']:.1f}%\")\n",
    "    col4.metric(\"Risk Probability\", f\"{latest['predicted_prob']*100:.1f}%\")\n",
    "    col5.markdown(f\"<div style='background-color:{risk_color};padding:20px;border-radius:10px;text-align:center;'>\"\n",
    "                  f\"<b style='color:white;font-size:20px;'>{risk_level}</b></div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Charts\n",
    "    col_left, col_right = st.columns(2)\n",
    "    \n",
    "    with col_left:\n",
    "        fig_prob = go.Figure()\n",
    "        fig_prob.add_trace(go.Scatter(x=country_data['timestamp'], y=country_data['predicted_prob']*100,\n",
    "                                       mode='lines', name='Risk Probability', line=dict(color='#3b82f6', width=2)))\n",
    "        fig_prob.add_hline(y=threshold*100, line_dash=\"dash\", line_color=\"red\")\n",
    "        fig_prob.update_layout(title=\"Risk Probability Over Time\", xaxis_title=\"Time\", yaxis_title=\"Probability (%)\",\n",
    "                               template=\"plotly_dark\", height=400)\n",
    "        st.plotly_chart(fig_prob, use_container_width=True)\n",
    "    \n",
    "    with col_right:\n",
    "        fig_load = go.Figure()\n",
    "        fig_load.add_trace(go.Scatter(x=country_data['timestamp'], y=country_data['actual_load'],\n",
    "                                       mode='lines', name='Actual', line=dict(color='#10b981', width=2)))\n",
    "        fig_load.add_trace(go.Scatter(x=country_data['timestamp'], y=country_data['forecast_load'],\n",
    "                                       mode='lines', name='Forecast', line=dict(color='#f59e0b', width=2, dash='dot')))\n",
    "        fig_load.update_layout(title=\"Actual vs Forecast Load\", xaxis_title=\"Time\", yaxis_title=\"Load (MW)\",\n",
    "                               template=\"plotly_dark\", height=400)\n",
    "        st.plotly_chart(fig_load, use_container_width=True)\n",
    "    \n",
    "    # Alerts\n",
    "    st.subheader(\"Recent Alerts\")\n",
    "    alerts = country_data[country_data['predicted_prob'] >= threshold].tail(20)\n",
    "    if len(alerts) > 0:\n",
    "        st.dataframe(alerts[['timestamp', 'actual_load', 'forecast_error_pct', 'predicted_prob', 'temp_avg']].round(2))\n",
    "    else:\n",
    "        st.success(\"No alerts in selected period\")\n",
    "    \n",
    "    with st.expander(\"Model Information\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **Model**: XGBoost with Weather Features (53 features)\n",
    "        \n",
    "        **Test Performance**: Accuracy 95.87%, AUC-ROC 0.9879, F1 0.8648\n",
    "        \n",
    "        **Validated**: April 28, 2025 Spain/Portugal Blackout - 100% detection\n",
    "        \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "with open(f\"{streamlit_dir}/app.py\", 'w') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "print(\"8. Saved: streamlit_app/app.py\")\n",
    "\n",
    "# 9. Save README\n",
    "readme = \"\"\"# European Power Grid Stress Monitor\n",
    "\n",
    "## Model Performance (Test Set)\n",
    "| Model | Accuracy | AUC-ROC | F1 Score |\n",
    "|-------|----------|---------|----------|\n",
    "| Without Weather | 95.60% | 0.9871 | 0.8579 |\n",
    "| With Weather | 95.87% | 0.9879 | 0.8648 |\n",
    "\n",
    "## Files\n",
    "- model_without_weather.pkl - XGBoost (37 features)\n",
    "- model_with_weather.pkl - XGBoost + Weather (53 features)\n",
    "- feature_config.json - Feature lists\n",
    "- performance_metrics.json - Detailed metrics\n",
    "- streamlit_app/ - Dashboard application\n",
    "\n",
    "## Run Streamlit\n",
    "```bash\n",
    "cd streamlit_app\n",
    "pip install -r requirements.txt\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "## Validated Against\n",
    "April 28, 2025 Spain/Portugal Blackout - 100% detection\n",
    "\"\"\"\n",
    "with open(f\"{output_dir}/README.md\", 'w') as f:\n",
    "    f.write(readme)\n",
    "print(\"9. Saved: README.md\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLocation: {output_dir}\")\n",
    "print(\"\\nFiles:\")\n",
    "print(\"  - model_without_weather.pkl\")\n",
    "print(\"  - model_with_weather.pkl\")\n",
    "print(\"  - feature_config.json\")\n",
    "print(\"  - performance_metrics.json\")\n",
    "print(\"  - README.md\")\n",
    "print(\"  - streamlit_app/\")\n",
    "print(\"      - app.py\")\n",
    "print(\"      - model_with_weather.pkl\")\n",
    "print(\"      - sample_data.csv\")\n",
    "print(\"      - requirements.txt\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "European_Grid_Blackout_Risk_Model_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
