{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294503bb-14ea-4870-981c-295274b701b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_dataset(country_code, dataset_name, df):\n",
    "    \"\"\"\n",
    "    Write a single dataset to a Delta table.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return\n",
    "\n",
    "    # Replace spaces and invalid characters in column names for pandas DataFrame\n",
    "    df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "    table_name = f\"{country_code.lower()}__{dataset_name}\"\n",
    "    full_name = f\"{Config.DATABASE}.{table_name}\"\n",
    "\n",
    "    spark_df = spark.createDataFrame(df.reset_index())\n",
    "\n",
    "    (spark_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")        # A single dataset → safe overwrite\n",
    "        .saveAsTable(full_name))\n",
    "\n",
    "    print(f\"  → Saved to Delta: {full_name} ({spark_df.count()} rows)\")\n",
    "\n",
    "API_KEY = '7b785108-53d7-42f8-931e-3d28c4323c68'\n",
    "\n",
    "COUNTRIES = {\n",
    "    'ES': 'Spain', 'PT': 'Portugal', 'FR': 'France', 'DE': 'Germany',\n",
    "    'IT': 'Italy', 'GB': 'Great Britain', 'NL': 'Netherlands',\n",
    "    'BE': 'Belgium', 'AT': 'Austria', 'CH': 'Switzerland', 'PL': 'Poland',\n",
    "    'CZ': 'Czechia', 'DK': 'Denmark', 'SE': 'Sweden', 'NO': 'Norway',\n",
    "    'FI': 'Finland', 'GR': 'Greece', 'IE': 'Ireland', 'RO': 'Romania',\n",
    "    'BG': 'Bulgaria', 'HU': 'Hungary', 'SK': 'Slovakia', 'SI': 'Slovenia',\n",
    "    'HR': 'Croatia', 'EE': 'Estonia', 'LT': 'Lithuania', 'LV': 'Latvia'\n",
    "}\n",
    "\n",
    "# YOU REQUESTED THIS EXACT BLOCK KEPT UNCHANGED\n",
    "VALID_BORDERS = {\n",
    "    ('ES', 'PT'), ('ES', 'FR'),\n",
    "    ('FR', 'BE'), ('FR', 'CH'), ('FR', 'DE'), ('FR', 'IT'),\n",
    "    ('BE', 'NL'), ('BE', 'DE'),\n",
    "    ('NL', 'DE'), ('NL', 'GB'),\n",
    "    ('GB', 'NL'), ('GB', 'FR'), ('GB', 'IE'),\n",
    "    ('DE', 'CZ'), ('DE', 'PL'), ('DE', 'CH'), ('DE', 'DK'), ('DE', 'AT'),\n",
    "    ('DK', 'DE'), ('DK', 'NO'), ('DK', 'SE'),\n",
    "    ('SE', 'NO'), ('SE', 'FI'), ('SE', 'DK'),\n",
    "    ('NO', 'NL'), ('NO', 'GB'), ('NO', 'SE'), ('NO', 'DK'),\n",
    "    ('FI', 'EE'), ('FI', 'SE'),\n",
    "    ('EE', 'LV'),\n",
    "    ('LV', 'LT'),\n",
    "    ('LT', 'PL'),\n",
    "    ('PL', 'SK'), ('PL', 'CZ'),\n",
    "    ('CZ', 'AT'), ('CZ', 'SK'),\n",
    "    ('AT', 'SI'), ('AT', 'IT'), ('AT', 'CH'), ('AT', 'CZ'), ('AT', 'DE'),\n",
    "    ('SI', 'HR'), ('SI', 'IT'), ('SI', 'AT'),\n",
    "    ('HR', 'HU'), ('HR', 'SI'),\n",
    "    ('HU', 'SK'), ('HU', 'RO'), ('HU', 'HR'), ('HU', 'AT'),\n",
    "    ('SK', 'HU'), ('SK', 'CZ'), ('SK', 'PL'),\n",
    "    ('RO', 'BG'), ('RO', 'HU'),\n",
    "    ('BG', 'GR'), ('BG', 'RO'),\n",
    "    ('GR', 'BG')\n",
    "}\n",
    "\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE   = '2025-10-31'\n",
    "\n",
    "DATABASE = \"european_grid_raw\"\n",
    "\n",
    "# ============================================================================\n",
    "# DELTA WRITER\n",
    "# ============================================================================\n",
    "\n",
    "def write_dataset(country_code, dataset_name, df):\n",
    "    \"\"\"\n",
    "    Write a single dataset to a Delta table.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return\n",
    "\n",
    "    # Replace spaces and invalid characters in column names for pandas DataFrame\n",
    "    df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "    table_name = f\"{country_code.lower()}__{dataset_name}\"\n",
    "    full_name = f\"{Config.DATABASE}.{table_name}\"\n",
    "\n",
    "    spark_df = spark.createDataFrame(df.reset_index())\n",
    "\n",
    "    (spark_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")        # A single dataset → safe overwrite\n",
    "        .saveAsTable(full_name))\n",
    "\n",
    "    print(f\"  → Saved to Delta: {full_name} ({spark_df.count()} rows)\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLECTOR\n",
    "# ============================================================================\n",
    "\n",
    "class EuropeanGridDataCollector:\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        self.client = EntsoePandasClient(api_key=api_key)\n",
    "        self.countries = Config.COUNTRIES\n",
    "\n",
    "        self.start = pd.Timestamp(Config.START_DATE, tz=\"UTC\")\n",
    "        self.end   = pd.Timestamp(Config.END_DATE,   tz=\"UTC\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # SINGLE COUNTRY DATA\n",
    "    # -------------------------------\n",
    "    def collect_country_data(self, country_code):\n",
    "        c = country_code\n",
    "        print(f\"\\n==== Collecting for {c} ({self.countries[c]}) ====\")\n",
    "\n",
    "        # EXACT same logic and ordering preserved\n",
    "        try:\n",
    "            print(f\"    → load_actual...\")\n",
    "            df = self.client.query_load(c, start=self.start, end=self.end)\n",
    "            write_dataset(c, \"load_actual\", df)\n",
    "        except Exception as e: print(f\"    ✗ load_actual: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → load_forecast...\")\n",
    "            df = self.client.query_load_forecast(c, start=self.start, end=self.end)\n",
    "            write_dataset(c, \"load_forecast\", df)\n",
    "        except Exception as e: print(f\"    ✗ load_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → generation...\")\n",
    "            df = self.client.query_generation(c, start=self.start, end=self.end)\n",
    "            write_dataset(c, \"generation\", df)\n",
    "        except Exception as e: print(f\"    ✗ generation: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → wind_forecast...\")\n",
    "            df = self.client.query_wind_and_solar_forecast(c, start=self.start, end=self.end, psr_type='B19')\n",
    "            write_dataset(c, \"wind_forecast\", df)\n",
    "        except Exception as e: print(f\"    ✗ wind_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → solar_forecast...\")\n",
    "            df = self.client.query_wind_and_solar_forecast(c, start=self.start, end=self.end, psr_type='B16')\n",
    "            write_dataset(c, \"solar_forecast\", df)\n",
    "        except Exception as e: print(f\"    ✗ solar_forecast: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            print(f\"    → installed_capacity...\")\n",
    "            df = self.client.query_installed_generation_capacity(c, start=self.start, end=self.end)\n",
    "            write_dataset(c, \"installed_capacity\", df)\n",
    "        except Exception as e: print(f\"    ✗ installed_capacity: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # CROSS-BORDER FLOWS\n",
    "    # -------------------------------\n",
    "    def collect_crossborder_flows(self):\n",
    "        print(\"\\n=== Collecting Cross-Border Flows ===\")\n",
    "\n",
    "        flows_list = []\n",
    "\n",
    "        for from_c, to_c in Config.VALID_BORDERS:\n",
    "            print(f\"  → {from_c} ↔ {to_c}...\", end=\"\")\n",
    "\n",
    "            try:\n",
    "                flow = self.client.query_crossborder_flows(\n",
    "                    from_c, to_c, start=self.start, end=self.end\n",
    "                )\n",
    "                if flow is not None and len(flow) > 0:\n",
    "                    df = pd.DataFrame(flow)\n",
    "                    df[\"from_country\"] = from_c\n",
    "                    df[\"to_country\"]   = to_c\n",
    "                    flows_list.append(df)\n",
    "                    print(\" ✓\")\n",
    "                else:\n",
    "                    print(\" ✗ No data\")\n",
    "            except:\n",
    "                print(\" ✗ Failed\")\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if flows_list:\n",
    "            df = pd.concat(flows_list, ignore_index=True)\n",
    "            spark.table(Config.DATABASE + \".crossborder_flows\") \\\n",
    "                if spark._jsparkSession.catalog().tableExists(Config.DATABASE + \".crossborder_flows\") \\\n",
    "                else None\n",
    "\n",
    "            spark_df = spark.createDataFrame(df)\n",
    "            (spark_df.write.format(\"delta\").mode(\"overwrite\")\n",
    "                .saveAsTable(f\"{Config.DATABASE}.crossborder_flows\"))\n",
    "\n",
    "            print(\"  → Saved cross-border flows table\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # MAIN COLLECTOR\n",
    "    # -------------------------------\n",
    "    def collect_all(self):\n",
    "        for c in self.countries.keys():\n",
    "            self.collect_country_data(c)\n",
    "\n",
    "        self.collect_crossborder_flows()\n",
    "\n",
    "# ============================================================================\n",
    "# RUN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "collector = EuropeanGridDataCollector(api_key=API_KEY)\n",
    "collector.collect_all()\n",
    "print(\"\\nCOMPLETE.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/chavely.albert@gmail.com/energy-grid-load-prediction/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_grid_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
