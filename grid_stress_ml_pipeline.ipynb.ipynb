{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c905f3-04da-445c-b213-96cd7a3e935b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EUROPEAN GRID STRESS PREDICTION - MACHINE LEARNING PIPELINE\n",
    "# Author: Pedro Miguel\n",
    "# Description: Complete ML pipeline for predicting grid stress scores and \n",
    "#              blackout risk classification across 26 European countries\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\", \"--quiet\"])\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 1: DATA LOADING & CLEANING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1.1: DEFINE COLUMNS TO EXCLUDE\n",
    "# ============================================================================\n",
    "\n",
    "# Columns explicitly requested to exclude (contain leakage or are derived)\n",
    "EXCLUDE_COLS_EXPLICIT = [\n",
    "    'reserve_margin_ml',      # ML-derived feature (potential leakage)\n",
    "    'forecast_load_error',    # Derived from target components\n",
    "    'load_rel_error'          # Derived from target components\n",
    "]\n",
    "\n",
    "# Generation columns with 100% NaN (identified in exploration)\n",
    "EXCLUDE_COLS_NAN = [\n",
    "    'Fossil_Peat__Actual_Consumption',\n",
    "    'Marine__Actual_Aggregated',\n",
    "    'Energy_storage__Actual_Consumption',\n",
    "    'Fossil_Brown_coal_Lignite__Actual_Aggregated',\n",
    "    'Fossil_Coal_derived_gas__Actual_Aggregated',\n",
    "    'Wind_Offshore__Actual_Aggregated',\n",
    "    'Nuclear__Actual_Consumption',\n",
    "    'Fossil_Oil_shale__Actual_Aggregated',\n",
    "    'Fossil_Peat__Actual_Aggregated',\n",
    "    'Energy_storage__Actual_Aggregated',\n",
    "    'Wind_Offshore__Actual_Consumption',\n",
    "    'Nuclear__Actual_Aggregated'\n",
    "]\n",
    "\n",
    "# Combine all columns to exclude\n",
    "EXCLUDE_COLS_ALL = EXCLUDE_COLS_EXPLICIT + EXCLUDE_COLS_NAN\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COLUMNS TO EXCLUDE:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Explicit exclusions: {len(EXCLUDE_COLS_EXPLICIT)}\")\n",
    "print(f\"  NaN columns (100%):  {len(EXCLUDE_COLS_NAN)}\")\n",
    "print(f\"  Total to exclude:    {len(EXCLUDE_COLS_ALL)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1.2: LOAD DATASETS FROM EXISTING SPLITS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"LOADING EXISTING DATA SPLITS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load the pre-split datasets\n",
    "train_spark = spark.table(\"default.train_set\")\n",
    "val_spark = spark.table(\"default.validation_set\")\n",
    "test_spark = spark.table(\"default.test_set\")\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "# Note: This loads all data into memory - ensure sufficient RAM\n",
    "print(\"\\n  Converting to Pandas DataFrames...\")\n",
    "train_df = train_spark.toPandas()\n",
    "val_df = val_spark.toPandas()\n",
    "test_df = test_spark.toPandas()\n",
    "\n",
    "print(f\"\\n  ✓ Train set loaded:      {train_df.shape[0]:,} rows × {train_df.shape[1]} columns\")\n",
    "print(f\"  ✓ Validation set loaded: {val_df.shape[0]:,} rows × {val_df.shape[1]} columns\")\n",
    "print(f\"  ✓ Test set loaded:       {test_df.shape[0]:,} rows × {test_df.shape[1]} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1.3: DATA CLEANING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def clean_dataset(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Clean dataset by removing excluded columns and handling missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to clean\n",
    "    name : str\n",
    "        Name of the dataset (for logging purposes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe\n",
    "    dict\n",
    "        Cleaning report with statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Processing {name}...\")\n",
    "    \n",
    "    # Store initial shape\n",
    "    initial_shape = df.shape\n",
    "    \n",
    "    # Remove excluded columns\n",
    "    cols_to_drop = [col for col in EXCLUDE_COLS_ALL if col in df.columns]\n",
    "    df_cleaned = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Check for remaining NaN values in non-excluded columns\n",
    "    nan_counts = df_cleaned.isnull().sum()\n",
    "    cols_with_nan = nan_counts[nan_counts > 0]\n",
    "    \n",
    "    # Create cleaning report\n",
    "    report = {\n",
    "        'initial_shape': initial_shape,\n",
    "        'final_shape': df_cleaned.shape,\n",
    "        'columns_removed': len(cols_to_drop),\n",
    "        'remaining_nan_cols': len(cols_with_nan),\n",
    "        'nan_details': cols_with_nan.to_dict() if len(cols_with_nan) > 0 else {}\n",
    "    }\n",
    "    \n",
    "    print(f\"    Initial: {initial_shape[0]:,} rows × {initial_shape[1]} cols\")\n",
    "    print(f\"    Final:   {df_cleaned.shape[0]:,} rows × {df_cleaned.shape[1]} cols\")\n",
    "    print(f\"    Removed: {len(cols_to_drop)} columns\")\n",
    "    \n",
    "    return df_cleaned, report\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1.4: APPLY CLEANING TO ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLEANING DATASETS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "train_clean, train_report = clean_dataset(train_df, \"Train Set\")\n",
    "val_clean, val_report = clean_dataset(val_df, \"Validation Set\")\n",
    "test_clean, test_report = clean_dataset(test_df, \"Test Set\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1.5: VERIFY DATA INTEGRITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DATA INTEGRITY CHECK:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if target variable exists and has no missing values\n",
    "target_col = 'grid_stress_score'\n",
    "\n",
    "for name, df in [(\"Train\", train_clean), (\"Validation\", val_clean), (\"Test\", test_clean)]:\n",
    "    if target_col in df.columns:\n",
    "        missing = df[target_col].isnull().sum()\n",
    "        print(f\"\\n  {name} - {target_col}:\")\n",
    "        print(f\"    Present: ✓\")\n",
    "        print(f\"    Missing: {missing} ({missing/len(df)*100:.2f}%)\")\n",
    "        print(f\"    Range:   [{df[target_col].min():.1f}, {df[target_col].max():.1f}]\")\n",
    "    else:\n",
    "        print(f\"\\n  {name} - {target_col}: ✗ NOT FOUND\")\n",
    "\n",
    "# Check for any remaining columns with high NaN percentage\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"REMAINING NaN ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "nan_threshold = 50  # Flag columns with >50% NaN\n",
    "for name, df in [(\"Train\", train_clean), (\"Validation\", val_clean), (\"Test\", test_clean)]:\n",
    "    nan_pct = (df.isnull().sum() / len(df) * 100)\n",
    "    high_nan_cols = nan_pct[nan_pct > nan_threshold]\n",
    "    \n",
    "    if len(high_nan_cols) > 0:\n",
    "        print(f\"\\n  {name} - Columns with >{nan_threshold}% NaN:\")\n",
    "        for col, pct in high_nan_cols.items():\n",
    "            print(f\"    {col}: {pct:.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\n  {name}: ✓ No columns with >{nan_threshold}% NaN\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 1 COMPLETE: Data loaded and cleaned successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a25ca0-0b1f-4363-a885-6b129c763083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 2: ADVANCED CLEANING - COMPLETE DATA ONLY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 2: ADVANCED CLEANING - COMPLETE DATA ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.1: KEEP ONLY COLUMNS WITH ZERO NaN (COMPLETE DATA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FILTERING FOR COMPLETE COLUMNS (0% NaN):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def keep_complete_columns_only(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Keep only columns that have absolutely no missing values.\n",
    "    Protected columns (index, country, target) are always kept.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    name : str\n",
    "        Dataset name for logging\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataframe with only complete columns\n",
    "    list\n",
    "        List of removed column names\n",
    "    list\n",
    "        List of kept column names\n",
    "    \"\"\"\n",
    "    # Protected columns that must be kept regardless\n",
    "    protected_cols = ['index', 'country', 'grid_stress_score']\n",
    "    \n",
    "    # Calculate NaN count for each column\n",
    "    nan_counts = df.isnull().sum()\n",
    "    \n",
    "    # Identify complete columns (0 NaN)\n",
    "    complete_cols = nan_counts[nan_counts == 0].index.tolist()\n",
    "    \n",
    "    # Identify incomplete columns (excluding protected ones)\n",
    "    incomplete_cols = []\n",
    "    for col in df.columns:\n",
    "        if col not in protected_cols and col not in complete_cols:\n",
    "            incomplete_cols.append(col)\n",
    "    \n",
    "    # Keep only complete columns + protected columns\n",
    "    cols_to_keep = list(set(complete_cols + protected_cols))\n",
    "    df_complete = df[cols_to_keep]\n",
    "    \n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Initial columns: {len(df.columns)}\")\n",
    "    print(f\"    Complete columns: {len(complete_cols)}\")\n",
    "    print(f\"    Incomplete columns removed: {len(incomplete_cols)}\")\n",
    "    print(f\"    Final columns: {len(df_complete.columns)}\")\n",
    "    print(f\"    Shape: {df_complete.shape}\")\n",
    "    \n",
    "    return df_complete, incomplete_cols, cols_to_keep\n",
    "\n",
    "# Apply to all datasets\n",
    "train_complete, train_removed, train_kept = keep_complete_columns_only(train_clean, \"Train\")\n",
    "val_complete, val_removed, val_kept = keep_complete_columns_only(val_clean, \"Validation\")\n",
    "test_complete, test_removed, test_kept = keep_complete_columns_only(test_clean, \"Test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.2: SHOW REMOVED COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COLUMNS REMOVED (had NaN values):\")\n",
    "print(\"-\" * 80)\n",
    "for i, col in enumerate(sorted(train_removed), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.3: SHOW KEPT COLUMNS (FEATURES)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COLUMNS KEPT (complete data):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Separate into categories\n",
    "feature_cols = [col for col in train_kept if col not in ['index', 'country', 'grid_stress_score']]\n",
    "\n",
    "print(f\"\\nProtected columns (3):\")\n",
    "print(f\"  - index\")\n",
    "print(f\"  - country\")\n",
    "print(f\"  - grid_stress_score (target)\")\n",
    "\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(sorted(feature_cols), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.4: VERIFY DATA INTEGRITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DATA INTEGRITY VERIFICATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df in [(\"Train\", train_complete), (\"Validation\", val_complete), (\"Test\", test_complete)]:\n",
    "    total_nans = df.isnull().sum().sum()\n",
    "    print(f\"\\n  {name} Set:\")\n",
    "    print(f\"    Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"    Features: {df.shape[1] - 3}\")\n",
    "    print(f\"    Total NaN values: {total_nans}\")\n",
    "    \n",
    "    if total_nans == 0:\n",
    "        print(f\"    Status: ✓ COMPLETE DATA (0% missing)\")\n",
    "    else:\n",
    "        print(f\"    Status: ✗ WARNING - Still has NaN values!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.5: FINAL DATASET SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FINAL CLEANED DATASETS SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df in [(\"Train\", train_complete), (\"Validation\", val_complete), (\"Test\", test_complete)]:\n",
    "    print(f\"\\n  {name} Set:\")\n",
    "    print(f\"    Rows: {df.shape[0]:,}\")\n",
    "    print(f\"    Features: {df.shape[1] - 3} (excluding index, country, target)\")\n",
    "    print(f\"    Date range: {df['index'].min()} to {df['index'].max()}\")\n",
    "    print(f\"    Countries: {df['country'].nunique()}\")\n",
    "    \n",
    "    # Target statistics\n",
    "    print(f\"    Target (grid_stress_score):\")\n",
    "    print(f\"      Mean: {df['grid_stress_score'].mean():.2f}\")\n",
    "    print(f\"      Std:  {df['grid_stress_score'].std():.2f}\")\n",
    "    print(f\"      Range: [{df['grid_stress_score'].min():.0f}, {df['grid_stress_score'].max():.0f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 2 COMPLETE: Only complete data retained!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNext: Module 3 will prepare features for modeling\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4446ef-7653-4564-ae90-5d6c9be6851a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 2.5: REMOVE TARGET LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 2.5: REMOVING TARGET LEAKAGE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.5.1: IDENTIFY AND REMOVE SCORE COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"REMOVING SCORE COMPONENTS (TARGET LEAKAGE):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# These columns are components of grid_stress_score and create data leakage\n",
    "LEAKAGE_COLS = [\n",
    "    'score_T7',\n",
    "    'score_T8',\n",
    "    'score_load_error',\n",
    "    'score_reserve_margin'\n",
    "]\n",
    "\n",
    "print(\"\\nColumns to remove (components of target variable):\")\n",
    "for i, col in enumerate(LEAKAGE_COLS, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Remove from all datasets\n",
    "train_final = train_complete.drop(columns=LEAKAGE_COLS)\n",
    "val_final = val_complete.drop(columns=LEAKAGE_COLS)\n",
    "test_final = test_complete.drop(columns=LEAKAGE_COLS)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df_before, df_after in [\n",
    "    (\"Train\", train_complete, train_final),\n",
    "    (\"Validation\", val_complete, val_final),\n",
    "    (\"Test\", test_complete, test_final)\n",
    "]:\n",
    "    print(f\"\\n  {name} Set:\")\n",
    "    print(f\"    Before: {df_before.shape[0]:,} rows × {df_before.shape[1]} columns\")\n",
    "    print(f\"    After:  {df_after.shape[0]:,} rows × {df_after.shape[1]} columns\")\n",
    "    print(f\"    Features: {df_after.shape[1] - 3} (excluding index, country, target)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.5.2: FINAL FEATURE LIST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FINAL FEATURE SET (NO LEAKAGE):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get final feature columns\n",
    "final_features = [col for col in train_final.columns if col not in ['index', 'country', 'grid_stress_score']]\n",
    "\n",
    "print(f\"\\nTotal features: {len(final_features)}\")\n",
    "print(\"\\nFeature list:\")\n",
    "for i, col in enumerate(sorted(final_features), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Categorize features for clarity\n",
    "load_features = [col for col in final_features if 'Load' in col]\n",
    "forecast_features = [col for col in final_features if 'forecast' in col]\n",
    "weather_features = [col for col in final_features if 'mean_' in col or 'ssrd' in col]\n",
    "network_features = [col for col in final_features if 'import' in col or 'export' in col or 'P10' in col or 'P90' in col]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FEATURE CATEGORIES:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Load-related:     {len(load_features)} features\")\n",
    "for f in load_features:\n",
    "    print(f\"    - {f}\")\n",
    "    \n",
    "print(f\"\\n  Forecast-related: {len(forecast_features)} features\")\n",
    "for f in forecast_features:\n",
    "    print(f\"    - {f}\")\n",
    "    \n",
    "print(f\"\\n  Weather-related:  {len(weather_features)} features\")\n",
    "for f in weather_features:\n",
    "    print(f\"    - {f}\")\n",
    "    \n",
    "print(f\"\\n  Network-related:  {len(network_features)} features\")\n",
    "for f in network_features:\n",
    "    print(f\"    - {f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2.5.3: VERIFY NO LEAKAGE & DATA QUALITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DATA QUALITY VERIFICATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df in [(\"Train\", train_final), (\"Validation\", val_final), (\"Test\", test_final)]:\n",
    "    print(f\"\\n  {name} Set:\")\n",
    "    print(f\"    ✓ Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"    ✓ Features: {df.shape[1] - 3}\")\n",
    "    print(f\"    ✓ NaN values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"    ✓ Target present: {'grid_stress_score' in df.columns}\")\n",
    "    print(f\"    ✓ No leakage: {all(col not in df.columns for col in LEAKAGE_COLS)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 2 COMPLETE: Clean data with NO leakage!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nReady for Module 3: Feature Engineering & Preprocessing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543d8e4a-e91f-4739-8f4d-a5bc34632d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 3: FEATURE ENGINEERING & PREPROCESSING PIPELINE (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 3: FEATURE ENGINEERING & PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.1: SEPARATE FEATURES AND TARGET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SEPARATING FEATURES AND TARGET:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def separate_features_target(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Separate features, target, and metadata from dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    name : str\n",
    "        Dataset name for logging\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (X, y, metadata)\n",
    "        X: Feature dataframe\n",
    "        y: Target series\n",
    "        metadata: DataFrame with index, country\n",
    "    \"\"\"\n",
    "    # Metadata columns\n",
    "    metadata = df[['index', 'country']].copy()\n",
    "    \n",
    "    # Target variable\n",
    "    y = df['grid_stress_score'].copy()\n",
    "    \n",
    "    # Features (all numeric columns except metadata and target)\n",
    "    feature_cols = [col for col in df.columns if col not in ['index', 'country', 'grid_stress_score']]\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Features (X): {X.shape}\")\n",
    "    print(f\"    Target (y):   {y.shape}\")\n",
    "    print(f\"    Metadata:     {metadata.shape}\")\n",
    "    \n",
    "    return X, y, metadata\n",
    "\n",
    "# Separate all datasets\n",
    "X_train, y_train, meta_train = separate_features_target(train_final, \"Train\")\n",
    "X_val, y_val, meta_val = separate_features_target(val_final, \"Validation\")\n",
    "X_test, y_test, meta_test = separate_features_target(test_final, \"Test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.2: CHECK FOR PROBLEMATIC VALUES BEFORE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CHECKING FOR PROBLEMATIC VALUES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def check_data_issues(X, name=\"Dataset\"):\n",
    "    \"\"\"Check for inf, nan, and zero values that could cause issues\"\"\"\n",
    "    print(f\"\\n  {name}:\")\n",
    "    \n",
    "    # Check for inf\n",
    "    inf_cols = X.columns[np.isinf(X).any()].tolist()\n",
    "    if inf_cols:\n",
    "        print(f\"    ✗ Infinity values in: {inf_cols}\")\n",
    "    else:\n",
    "        print(f\"    ✓ No infinity values\")\n",
    "    \n",
    "    # Check for zeros in denominator columns\n",
    "    zero_checks = {\n",
    "        'Forecasted_Load': (X['Forecasted_Load'] == 0).sum(),\n",
    "        'Actual_Load': (X['Actual_Load'] == 0).sum()\n",
    "    }\n",
    "    \n",
    "    for col, zero_count in zero_checks.items():\n",
    "        if zero_count > 0:\n",
    "            print(f\"    ⚠ {col} has {zero_count} zero values (potential division issue)\")\n",
    "        else:\n",
    "            print(f\"    ✓ {col} has no zeros\")\n",
    "    \n",
    "    return inf_cols, zero_checks\n",
    "\n",
    "inf_train, zeros_train = check_data_issues(X_train, \"Train\")\n",
    "inf_val, zeros_val = check_data_issues(X_val, \"Validation\")\n",
    "inf_test, zeros_test = check_data_issues(X_test, \"Test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.3: CREATE DERIVED FEATURES (WITH SAFETY CHECKS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ENGINEERING DERIVED FEATURES (WITH SAFETY):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def engineer_features_safe(X, meta, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Create derived features with safety checks to avoid inf/nan.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature dataframe\n",
    "    meta : pd.DataFrame\n",
    "        Metadata with datetime index\n",
    "    name : str\n",
    "        Dataset name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Enhanced feature dataframe with new features\n",
    "    \"\"\"\n",
    "    X_eng = X.copy()\n",
    "    \n",
    "    print(f\"\\n  {name}: Creating derived features...\")\n",
    "    \n",
    "    # Safety constant to avoid division by zero\n",
    "    EPSILON = 1e-6\n",
    "    \n",
    "    # 1. Load forecast error (actual vs forecasted)\n",
    "    X_eng['load_forecast_error'] = X_eng['Actual_Load'] - X_eng['Forecasted_Load']\n",
    "    \n",
    "    # Use safe division with epsilon and clip extreme values\n",
    "    X_eng['load_forecast_error_pct'] = (X_eng['load_forecast_error'] / \n",
    "                                        (X_eng['Forecasted_Load'].abs() + EPSILON)) * 100\n",
    "    X_eng['load_forecast_error_pct'] = X_eng['load_forecast_error_pct'].clip(-1000, 1000)\n",
    "    \n",
    "    # 2. Net position metrics (P10 and P90 spread)\n",
    "    X_eng['net_position_spread'] = X_eng['P90_net'] - X_eng['P10_net']\n",
    "    X_eng['net_position_avg'] = (X_eng['P90_net'] + X_eng['P10_net']) / 2\n",
    "    \n",
    "    # 3. Import/Export balance\n",
    "    X_eng['trade_balance_flag'] = (X_eng['T7_high_exports'].astype(int) - \n",
    "                                   X_eng['T8_high_imports'].astype(int))\n",
    "    \n",
    "    # 4. Renewable energy potential (solar + wind forecasts)\n",
    "    X_eng['renewable_forecast_total'] = X_eng['solar_forecast'] + X_eng['wind_forecast']\n",
    "    \n",
    "    # Safe ratio calculation\n",
    "    X_eng['renewable_to_load_ratio'] = (X_eng['renewable_forecast_total'] / \n",
    "                                        (X_eng['Forecasted_Load'].abs() + EPSILON))\n",
    "    X_eng['renewable_to_load_ratio'] = X_eng['renewable_to_load_ratio'].clip(0, 10)\n",
    "    \n",
    "    # 5. Temporal features from datetime index\n",
    "    datetime_series = pd.to_datetime(meta['index'])\n",
    "    X_eng['hour'] = datetime_series.dt.hour\n",
    "    X_eng['day_of_week'] = datetime_series.dt.dayofweek\n",
    "    X_eng['month'] = datetime_series.dt.month\n",
    "    X_eng['is_weekend'] = (datetime_series.dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    # 6. Cyclical encoding for hour (24-hour cycle)\n",
    "    X_eng['hour_sin'] = np.sin(2 * np.pi * X_eng['hour'] / 24)\n",
    "    X_eng['hour_cos'] = np.cos(2 * np.pi * X_eng['hour'] / 24)\n",
    "    \n",
    "    # 7. Cyclical encoding for month (12-month cycle)\n",
    "    X_eng['month_sin'] = np.sin(2 * np.pi * X_eng['month'] / 12)\n",
    "    X_eng['month_cos'] = np.cos(2 * np.pi * X_eng['month'] / 12)\n",
    "    \n",
    "    new_features = X_eng.shape[1] - X.shape[1]\n",
    "    print(f\"    Original features: {X.shape[1]}\")\n",
    "    print(f\"    New features added: {new_features}\")\n",
    "    print(f\"    Total features: {X_eng.shape[1]}\")\n",
    "    \n",
    "    # Check for inf/nan after engineering\n",
    "    inf_count = np.isinf(X_eng).sum().sum()\n",
    "    nan_count = X_eng.isna().sum().sum()\n",
    "    \n",
    "    if inf_count > 0:\n",
    "        print(f\"    ✗ WARNING: {inf_count} infinity values detected!\")\n",
    "    else:\n",
    "        print(f\"    ✓ No infinity values\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"    ✗ WARNING: {nan_count} NaN values detected!\")\n",
    "    else:\n",
    "        print(f\"    ✓ No NaN values\")\n",
    "    \n",
    "    return X_eng\n",
    "\n",
    "# Apply feature engineering to all datasets\n",
    "X_train_eng = engineer_features_safe(X_train, meta_train, \"Train\")\n",
    "X_val_eng = engineer_features_safe(X_val, meta_val, \"Validation\")\n",
    "X_test_eng = engineer_features_safe(X_test, meta_test, \"Test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.4: ENCODE COUNTRY VARIABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ENCODING COUNTRY VARIABLE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Initialize and fit label encoder on training countries\n",
    "country_encoder = LabelEncoder()\n",
    "country_encoder.fit(meta_train['country'])\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_eng['country_encoded'] = country_encoder.transform(meta_train['country'])\n",
    "X_val_eng['country_encoded'] = country_encoder.transform(meta_val['country'])\n",
    "X_test_eng['country_encoded'] = country_encoder.transform(meta_test['country'])\n",
    "\n",
    "print(f\"\\n  Countries encoded: {len(country_encoder.classes_)}\")\n",
    "print(f\"  Sample countries: {list(country_encoder.classes_[:5])}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.5: FINAL CHECK BEFORE SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FINAL CHECK BEFORE SCALING:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, X in [(\"Train\", X_train_eng), (\"Validation\", X_val_eng), (\"Test\", X_test_eng)]:\n",
    "    inf_count = np.isinf(X).sum().sum()\n",
    "    nan_count = X.isna().sum().sum()\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Shape: {X.shape}\")\n",
    "    print(f\"    Infinity values: {inf_count}\")\n",
    "    print(f\"    NaN values: {nan_count}\")\n",
    "    if inf_count == 0 and nan_count == 0:\n",
    "        print(f\"    Status: ✓ READY FOR SCALING\")\n",
    "    else:\n",
    "        print(f\"    Status: ✗ NEEDS CLEANING\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3.6: FEATURE SCALING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SCALING FEATURES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only (avoid data leakage)\n",
    "X_train_scaled = scaler.fit_transform(X_train_eng)\n",
    "X_val_scaled = scaler.transform(X_val_eng)\n",
    "X_test_scaled = scaler.transform(X_test_eng)\n",
    "\n",
    "# Convert back to DataFrame with column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_eng.columns, index=X_train_eng.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val_eng.columns, index=X_val_eng.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_eng.columns, index=X_test_eng.index)\n",
    "\n",
    "print(f\"\\n  ✓ Scaler fitted on train set\")\n",
    "print(f\"  ✓ Train scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  ✓ Validation scaled: {X_val_scaled.shape}\")\n",
    "print(f\"  ✓ Test scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 3 COMPLETE: Features engineered and preprocessed safely!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nReady for Module 4: Model Training\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260ef0d4-ac71-4560-aa40-2a7bd4d1f1b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 4: MODEL TRAINING PIPELINE (REGRESSION + CLASSIFICATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 4: MODEL TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.1: CREATE CLASSIFICATION TARGET (BLACKOUT RISK)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING CLASSIFICATION TARGET (BLACKOUT RISK):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def create_blackout_risk_target(y, threshold=50):\n",
    "    \"\"\"\n",
    "    Create binary classification target for blackout risk.\n",
    "    \n",
    "    High Risk (1): grid_stress_score >= threshold\n",
    "    Low Risk (0): grid_stress_score < threshold\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : pd.Series\n",
    "        Grid stress score\n",
    "    threshold : float\n",
    "        Threshold for high risk classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Binary classification target\n",
    "    \"\"\"\n",
    "    return (y >= threshold).astype(int)\n",
    "\n",
    "# Create classification targets\n",
    "y_train_class = create_blackout_risk_target(y_train, threshold=50)\n",
    "y_val_class = create_blackout_risk_target(y_val, threshold=50)\n",
    "y_test_class = create_blackout_risk_target(y_test, threshold=50)\n",
    "\n",
    "print(f\"\\n  Classification threshold: grid_stress_score >= 50\")\n",
    "print(f\"\\n  Train set class distribution:\")\n",
    "print(f\"    Low Risk (0):  {(y_train_class == 0).sum():,} ({(y_train_class == 0).sum()/len(y_train_class)*100:.2f}%)\")\n",
    "print(f\"    High Risk (1): {(y_train_class == 1).sum():,} ({(y_train_class == 1).sum()/len(y_train_class)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Validation set class distribution:\")\n",
    "print(f\"    Low Risk (0):  {(y_val_class == 0).sum():,} ({(y_val_class == 0).sum()/len(y_val_class)*100:.2f}%)\")\n",
    "print(f\"    High Risk (1): {(y_val_class == 1).sum():,} ({(y_val_class == 1).sum()/len(y_val_class)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Test set class distribution:\")\n",
    "print(f\"    Low Risk (0):  {(y_test_class == 0).sum():,} ({(y_test_class == 0).sum()/len(y_test_class)*100:.2f}%)\")\n",
    "print(f\"    High Risk (1): {(y_test_class == 1).sum():,} ({(y_test_class == 1).sum()/len(y_test_class)*100:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.2: DEFINE MODEL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MODEL CONFIGURATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Regression models\n",
    "regression_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=20, \n",
    "                                          min_samples_split=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, \n",
    "                                                   learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                           random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Classification models\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=20, \n",
    "                                           min_samples_split=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, \n",
    "                                                    learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                            random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "print(f\"\\n  Regression models: {len(regression_models)}\")\n",
    "for model_name in regression_models.keys():\n",
    "    print(f\"    - {model_name}\")\n",
    "\n",
    "print(f\"\\n  Classification models: {len(classification_models)}\")\n",
    "for model_name in classification_models.keys():\n",
    "    print(f\"    - {model_name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.3: TRAIN REGRESSION MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING REGRESSION MODELS (GRID STRESS SCORE PREDICTION)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for model_name, model in regression_models.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on all sets\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_time': train_time,\n",
    "        'train_metrics': {\n",
    "            'r2': r2_score(y_train, y_train_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            'mae': mean_absolute_error(y_train, y_train_pred)\n",
    "        },\n",
    "        'val_metrics': {\n",
    "            'r2': r2_score(y_val, y_val_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "            'mae': mean_absolute_error(y_val, y_val_pred)\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'r2': r2_score(y_test, y_test_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'mae': mean_absolute_error(y_test, y_test_pred)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    regression_results[model_name] = results\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n  Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"\\n  Train Set:\")\n",
    "    print(f\"    R²:   {results['train_metrics']['r2']:.6f}\")\n",
    "    print(f\"    RMSE: {results['train_metrics']['rmse']:.4f}\")\n",
    "    print(f\"    MAE:  {results['train_metrics']['mae']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Validation Set:\")\n",
    "    print(f\"    R²:   {results['val_metrics']['r2']:.6f}\")\n",
    "    print(f\"    RMSE: {results['val_metrics']['rmse']:.4f}\")\n",
    "    print(f\"    MAE:  {results['val_metrics']['mae']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Test Set:\")\n",
    "    print(f\"    R²:   {results['test_metrics']['r2']:.6f}\")\n",
    "    print(f\"    RMSE: {results['test_metrics']['rmse']:.4f}\")\n",
    "    print(f\"    MAE:  {results['test_metrics']['mae']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4.4: REGRESSION MODELS COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REGRESSION MODELS COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, results in regression_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train R²': results['train_metrics']['r2'],\n",
    "        'Val R²': results['val_metrics']['r2'],\n",
    "        'Test R²': results['test_metrics']['r2'],\n",
    "        'Val RMSE': results['val_metrics']['rmse'],\n",
    "        'Test RMSE': results['test_metrics']['rmse'],\n",
    "        'Train Time (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Val R²', ascending=False)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_regression_model = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST REGRESSION MODEL: {best_regression_model}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 4A COMPLETE: Regression models trained!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ab6a4a-a8fd-4af5-b79f-e9bf59d0216d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 4B: CLASSIFICATION MODEL TRAINING (BLACKOUT RISK)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 4B: CLASSIFICATION MODEL TRAINING (BLACKOUT RISK)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "classification_results = {}\n",
    "\n",
    "for model_name, model in classification_models.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train_class)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✓ Training completed in {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on all sets\n",
    "    print(f\"  Making predictions...\")\n",
    "    y_train_pred_class = model.predict(X_train_scaled)\n",
    "    y_val_pred_class = model.predict(X_val_scaled)\n",
    "    y_test_pred_class = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_time': train_time,\n",
    "        'train_metrics': {\n",
    "            'accuracy': accuracy_score(y_train_class, y_train_pred_class),\n",
    "            'precision': precision_score(y_train_class, y_train_pred_class, zero_division=0),\n",
    "            'recall': recall_score(y_train_class, y_train_pred_class, zero_division=0),\n",
    "            'f1': f1_score(y_train_class, y_train_pred_class, zero_division=0)\n",
    "        },\n",
    "        'val_metrics': {\n",
    "            'accuracy': accuracy_score(y_val_class, y_val_pred_class),\n",
    "            'precision': precision_score(y_val_class, y_val_pred_class, zero_division=0),\n",
    "            'recall': recall_score(y_val_class, y_val_pred_class, zero_division=0),\n",
    "            'f1': f1_score(y_val_class, y_val_pred_class, zero_division=0)\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'accuracy': accuracy_score(y_test_class, y_test_pred_class),\n",
    "            'precision': precision_score(y_test_class, y_test_pred_class, zero_division=0),\n",
    "            'recall': recall_score(y_test_class, y_test_pred_class, zero_division=0),\n",
    "            'f1': f1_score(y_test_class, y_test_pred_class, zero_division=0)\n",
    "        },\n",
    "        'confusion_matrix': {\n",
    "            'train': confusion_matrix(y_train_class, y_train_pred_class),\n",
    "            'val': confusion_matrix(y_val_class, y_val_pred_class),\n",
    "            'test': confusion_matrix(y_test_class, y_test_pred_class)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    classification_results[model_name] = results\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n  Train Set:\")\n",
    "    print(f\"    Accuracy:  {results['train_metrics']['accuracy']:.6f}\")\n",
    "    print(f\"    Precision: {results['train_metrics']['precision']:.6f}\")\n",
    "    print(f\"    Recall:    {results['train_metrics']['recall']:.6f}\")\n",
    "    print(f\"    F1-Score:  {results['train_metrics']['f1']:.6f}\")\n",
    "    \n",
    "    print(f\"\\n  Validation Set:\")\n",
    "    print(f\"    Accuracy:  {results['val_metrics']['accuracy']:.6f}\")\n",
    "    print(f\"    Precision: {results['val_metrics']['precision']:.6f}\")\n",
    "    print(f\"    Recall:    {results['val_metrics']['recall']:.6f}\")\n",
    "    print(f\"    F1-Score:  {results['val_metrics']['f1']:.6f}\")\n",
    "    \n",
    "    print(f\"\\n  Test Set:\")\n",
    "    print(f\"    Accuracy:  {results['test_metrics']['accuracy']:.6f}\")\n",
    "    print(f\"    Precision: {results['test_metrics']['precision']:.6f}\")\n",
    "    print(f\"    Recall:    {results['test_metrics']['recall']:.6f}\")\n",
    "    print(f\"    F1-Score:  {results['test_metrics']['f1']:.6f}\")\n",
    "    \n",
    "    # Show confusion matrix for test set\n",
    "    print(f\"\\n  Test Confusion Matrix:\")\n",
    "    cm = results['confusion_matrix']['test']\n",
    "    print(f\"    True Negatives:  {cm[0, 0]:,}\")\n",
    "    print(f\"    False Positives: {cm[0, 1]:,}\")\n",
    "    print(f\"    False Negatives: {cm[1, 0]:,}\")\n",
    "    print(f\"    True Positives:  {cm[1, 1]:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSIFICATION MODELS COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLASSIFICATION MODELS COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "comparison_class_data = []\n",
    "for model_name, results in classification_results.items():\n",
    "    comparison_class_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train Acc': results['train_metrics']['accuracy'],\n",
    "        'Val Acc': results['val_metrics']['accuracy'],\n",
    "        'Test Acc': results['test_metrics']['accuracy'],\n",
    "        'Test Precision': results['test_metrics']['precision'],\n",
    "        'Test Recall': results['test_metrics']['recall'],\n",
    "        'Test F1': results['test_metrics']['f1'],\n",
    "        'Train Time (s)': results['train_time']\n",
    "    })\n",
    "\n",
    "comparison_class_df = pd.DataFrame(comparison_class_data)\n",
    "comparison_class_df = comparison_class_df.sort_values('Test F1', ascending=False)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_class_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_classification_model = comparison_class_df.iloc[0]['Model']\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST CLASSIFICATION MODEL: {best_classification_model}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 4B COMPLETE: Classification models trained!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNext: Module 5 will create visualizations and final model selection\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fb2f1e-e926-41f2-9de6-8865ab7f376c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 5: MODEL EVALUATION & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 5: MODEL EVALUATION & VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5.1: REGRESSION - ACTUAL VS PREDICTED PLOTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING REGRESSION VISUALIZATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get best regression model predictions\n",
    "best_reg_model = regression_results['Random Forest']['model']\n",
    "y_test_pred_reg = best_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Regression Model Evaluation - Random Forest', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_test, y_test_pred_reg, alpha=0.3, s=10)\n",
    "ax1.plot([0, 75], [0, 75], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Grid Stress Score', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Grid Stress Score', fontsize=12)\n",
    "ax1.set_title('Actual vs Predicted (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add R² text\n",
    "r2_test = regression_results['Random Forest']['test_metrics']['r2']\n",
    "ax1.text(0.05, 0.95, f'R² = {r2_test:.4f}', transform=ax1.transAxes, \n",
    "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Residuals plot\n",
    "residuals = y_test - y_test_pred_reg\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_test_pred_reg, residuals, alpha=0.3, s=10)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Grid Stress Score', fontsize=12)\n",
    "ax2.set_ylabel('Residuals', fontsize=12)\n",
    "ax2.set_title('Residuals Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Residuals', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Residuals Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics\n",
    "mean_res = residuals.mean()\n",
    "std_res = residuals.std()\n",
    "ax3.text(0.05, 0.95, f'Mean: {mean_res:.4f}\\nStd: {std_res:.4f}', \n",
    "         transform=ax3.transAxes, fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 4. Model comparison\n",
    "ax4 = axes[1, 1]\n",
    "models = list(regression_results.keys())\n",
    "test_r2 = [regression_results[m]['test_metrics']['r2'] for m in models]\n",
    "colors = ['green' if r2 == max(test_r2) else 'skyblue' for r2 in test_r2]\n",
    "\n",
    "bars = ax4.barh(models, test_r2, color=colors, edgecolor='black')\n",
    "ax4.set_xlabel('R² Score', fontsize=12)\n",
    "ax4.set_title('Model Comparison (Test R²)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (bar, r2) in enumerate(zip(bars, test_r2)):\n",
    "    ax4.text(r2 + 0.01, i, f'{r2:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  ✓ Regression visualizations created\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5.2: CLASSIFICATION - CONFUSION MATRIX & METRICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING CLASSIFICATION VISUALIZATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get best classification model predictions\n",
    "best_class_model = classification_results['Random Forest']['model']\n",
    "y_test_pred_class = best_class_model.predict(X_test_scaled)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Classification Model Evaluation - Random Forest', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = axes[0, 0]\n",
    "cm = confusion_matrix(y_test_class, y_test_pred_class)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=False,\n",
    "            xticklabels=['Low Risk', 'High Risk'],\n",
    "            yticklabels=['Low Risk', 'High Risk'])\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('Actual', fontsize=12)\n",
    "ax1.set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Classification metrics comparison\n",
    "ax2 = axes[0, 1]\n",
    "models_class = list(classification_results.keys())\n",
    "test_f1 = [classification_results[m]['test_metrics']['f1'] for m in models_class]\n",
    "test_acc = [classification_results[m]['test_metrics']['accuracy'] for m in models_class]\n",
    "test_prec = [classification_results[m]['test_metrics']['precision'] for m in models_class]\n",
    "test_rec = [classification_results[m]['test_metrics']['recall'] for m in models_class]\n",
    "\n",
    "x = np.arange(len(models_class))\n",
    "width = 0.2\n",
    "\n",
    "ax2.bar(x - 1.5*width, test_acc, width, label='Accuracy', alpha=0.8)\n",
    "ax2.bar(x - 0.5*width, test_prec, width, label='Precision', alpha=0.8)\n",
    "ax2.bar(x + 0.5*width, test_rec, width, label='Recall', alpha=0.8)\n",
    "ax2.bar(x + 1.5*width, test_f1, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Models', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Classification Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_class, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Class distribution in predictions\n",
    "ax3 = axes[1, 0]\n",
    "actual_counts = [sum(y_test_class == 0), sum(y_test_class == 1)]\n",
    "pred_counts = [sum(y_test_pred_class == 0), sum(y_test_pred_class == 1)]\n",
    "\n",
    "x_pos = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x_pos - width/2, actual_counts, width, label='Actual', alpha=0.8)\n",
    "bars2 = ax3.bar(x_pos + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Class', fontsize=12)\n",
    "ax3.set_ylabel('Count', fontsize=12)\n",
    "ax3.set_title('Class Distribution: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(['Low Risk (0)', 'High Risk (1)'])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Model F1-Score comparison\n",
    "ax4 = axes[1, 1]\n",
    "colors_f1 = ['green' if f1 == max(test_f1) else 'coral' for f1 in test_f1]\n",
    "\n",
    "bars = ax4.barh(models_class, test_f1, color=colors_f1, edgecolor='black')\n",
    "ax4.set_xlabel('F1-Score', fontsize=12)\n",
    "ax4.set_title('Model Comparison (Test F1-Score)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (bar, f1) in enumerate(zip(bars, test_f1)):\n",
    "    ax4.text(f1 + 0.01, i, f'{f1:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  ✓ Classification visualizations created\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5.3: FEATURE IMPORTANCE (RANDOM FOREST)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get feature importance from best regression model\n",
    "feature_importance_reg = best_reg_model.feature_importances_\n",
    "feature_names = X_train_scaled.columns\n",
    "\n",
    "# Create DataFrame and sort\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance_reg\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['Importance'], \n",
    "               color='steelblue', edgecolor='black')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Feature Importance - Random Forest Regression', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (bar, imp) in enumerate(zip(bars, top_features['Importance'])):\n",
    "    ax.text(imp + 0.001, i, f'{imp:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n  Top 10 Most Important Features:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"    {i+1:2d}. {row['Feature']:40s} {row['Importance']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 5 COMPLETE: Visualizations and analysis finished!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067bd33f-2583-4db5-9682-cd9b2d6a36eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODULE 6: MODEL SAVING & PIPELINE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODULE 6: MODEL SAVING & PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.0: CREATE OUTPUT FOLDER USING RELATIVE PATH\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING OUTPUT FOLDER:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get current working directory and create relative path\n",
    "current_dir = os.getcwd()\n",
    "print(f\"  Current directory: {current_dir}\")\n",
    "\n",
    "# Create output folder name\n",
    "output_folder_name = 'pipeline_model_v2'\n",
    "\n",
    "# Try to create in current directory first\n",
    "output_folder = os.path.join(current_dir, output_folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"  ✓ Folder created: {output_folder_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error with current directory: {e}\")\n",
    "    # Fallback: use /tmp which always works\n",
    "    output_folder = f'/tmp/{output_folder_name}'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"  ✓ Using temp folder: {output_folder}\")\n",
    "\n",
    "print(f\"  ✓ Output folder ready\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.1: SAVE PREPROCESSING OBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAVING PREPROCESSING OBJECTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create a preprocessing pipeline dictionary\n",
    "preprocessing_pipeline = {\n",
    "    'scaler': scaler,\n",
    "    'country_encoder': country_encoder,\n",
    "    'feature_names': list(X_train_scaled.columns),\n",
    "    'target_name': 'grid_stress_score',\n",
    "    'classification_threshold': 50\n",
    "}\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "preprocessing_path = os.path.join(output_folder, 'grid_stress_preprocessing.pkl')\n",
    "with open(preprocessing_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_pipeline, f)\n",
    "\n",
    "print(f\"  ✓ grid_stress_preprocessing.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.2: SAVE BEST MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAVING BEST MODELS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save best regression model\n",
    "best_reg_model_name = 'Random Forest'\n",
    "best_reg_model = regression_results[best_reg_model_name]['model']\n",
    "\n",
    "regression_path = os.path.join(output_folder, 'grid_stress_regression_model.pkl')\n",
    "with open(regression_path, 'wb') as f:\n",
    "    pickle.dump(best_reg_model, f)\n",
    "\n",
    "print(f\"  ✓ grid_stress_regression_model.pkl\")\n",
    "\n",
    "# Save best classification model\n",
    "best_class_model_name = 'Random Forest'\n",
    "best_class_model = classification_results[best_class_model_name]['model']\n",
    "\n",
    "classification_path = os.path.join(output_folder, 'grid_stress_classification_model.pkl')\n",
    "with open(classification_path, 'wb') as f:\n",
    "    pickle.dump(best_class_model, f)\n",
    "\n",
    "print(f\"  ✓ grid_stress_classification_model.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.3: SAVE ALL MODEL RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAVING ALL MODEL RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save all regression results\n",
    "regression_results_summary = {}\n",
    "for model_name, results in regression_results.items():\n",
    "    regression_results_summary[model_name] = {\n",
    "        'train_time': results['train_time'],\n",
    "        'train_metrics': results['train_metrics'],\n",
    "        'val_metrics': results['val_metrics'],\n",
    "        'test_metrics': results['test_metrics']\n",
    "    }\n",
    "\n",
    "results_reg_path = os.path.join(output_folder, 'regression_results_all.pkl')\n",
    "with open(results_reg_path, 'wb') as f:\n",
    "    pickle.dump(regression_results_summary, f)\n",
    "\n",
    "print(f\"  ✓ regression_results_all.pkl\")\n",
    "\n",
    "# Save all classification results\n",
    "classification_results_summary = {}\n",
    "for model_name, results in classification_results.items():\n",
    "    classification_results_summary[model_name] = {\n",
    "        'train_time': results['train_time'],\n",
    "        'train_metrics': results['train_metrics'],\n",
    "        'val_metrics': results['val_metrics'],\n",
    "        'test_metrics': results['test_metrics'],\n",
    "        'confusion_matrix': results['confusion_matrix']\n",
    "    }\n",
    "\n",
    "results_class_path = os.path.join(output_folder, 'classification_results_all.pkl')\n",
    "with open(results_class_path, 'wb') as f:\n",
    "    pickle.dump(classification_results_summary, f)\n",
    "\n",
    "print(f\"  ✓ classification_results_all.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.4: CREATE MODEL METADATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING MODEL METADATA:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "metadata = {\n",
    "    'created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'author': 'Peter Leme',\n",
    "    'project': 'European Grid Stress Prediction',\n",
    "    'version': 'v2',\n",
    "    \n",
    "    # Data information\n",
    "    'data_info': {\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'total_samples': len(X_train) + len(X_val) + len(X_test),\n",
    "        'num_features': X_train_scaled.shape[1],\n",
    "        'num_countries': len(country_encoder.classes_),\n",
    "        'countries': list(country_encoder.classes_),\n",
    "        'date_range_train': str(meta_train['index'].min()) + \" to \" + str(meta_train['index'].max()),\n",
    "        'date_range_val': str(meta_val['index'].min()) + \" to \" + str(meta_val['index'].max()),\n",
    "        'date_range_test': str(meta_test['index'].min()) + \" to \" + str(meta_test['index'].max())\n",
    "    },\n",
    "    \n",
    "    # Regression model performance\n",
    "    'regression': {\n",
    "        'best_model': best_reg_model_name,\n",
    "        'test_r2': float(regression_results[best_reg_model_name]['test_metrics']['r2']),\n",
    "        'test_rmse': float(regression_results[best_reg_model_name]['test_metrics']['rmse']),\n",
    "        'test_mae': float(regression_results[best_reg_model_name]['test_metrics']['mae']),\n",
    "        'val_r2': float(regression_results[best_reg_model_name]['val_metrics']['r2']),\n",
    "        'train_time_seconds': float(regression_results[best_reg_model_name]['train_time'])\n",
    "    },\n",
    "    \n",
    "    # Classification model performance\n",
    "    'classification': {\n",
    "        'best_model': best_class_model_name,\n",
    "        'threshold': 50,\n",
    "        'test_accuracy': float(classification_results[best_class_model_name]['test_metrics']['accuracy']),\n",
    "        'test_precision': float(classification_results[best_class_model_name]['test_metrics']['precision']),\n",
    "        'test_recall': float(classification_results[best_class_model_name]['test_metrics']['recall']),\n",
    "        'test_f1': float(classification_results[best_class_model_name]['test_metrics']['f1']),\n",
    "        'val_accuracy': float(classification_results[best_class_model_name]['val_metrics']['accuracy']),\n",
    "        'val_f1': float(classification_results[best_class_model_name]['val_metrics']['f1']),\n",
    "        'train_time_seconds': float(classification_results[best_class_model_name]['train_time']),\n",
    "        'class_distribution': {\n",
    "            'train': {'low_risk': int((y_train_class == 0).sum()), 'high_risk': int((y_train_class == 1).sum())},\n",
    "            'val': {'low_risk': int((y_val_class == 0).sum()), 'high_risk': int((y_val_class == 1).sum())},\n",
    "            'test': {'low_risk': int((y_test_class == 0).sum()), 'high_risk': int((y_test_class == 1).sum())}\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Feature information\n",
    "    'features': {\n",
    "        'total': len(X_train_scaled.columns),\n",
    "        'original': 12,\n",
    "        'engineered': 15,\n",
    "        'encoded': 1,\n",
    "        'feature_list': list(X_train_scaled.columns)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as pickle\n",
    "metadata_path = os.path.join(output_folder, 'model_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"  ✓ model_metadata.pkl\")\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "metadata_json_path = os.path.join(output_folder, 'model_metadata.json')\n",
    "with open(metadata_json_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"  ✓ model_metadata.json\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.5: CREATE README\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CREATING README:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "readme_content = f\"\"\"European Grid Stress Prediction - ML Pipeline v2\n",
    "Author: Peter Leme\n",
    "Created: {metadata['created_date']}\n",
    "\n",
    "MODELS:\n",
    "- Regression (Random Forest): Test R2 = {metadata['regression']['test_r2']:.4f}\n",
    "- Classification (Random Forest): Test F1 = {metadata['classification']['test_f1']:.4f}\n",
    "\n",
    "DATA:\n",
    "- Total: {metadata['data_info']['total_samples']:,} samples\n",
    "- Features: {metadata['features']['total']}\n",
    "- Countries: {metadata['data_info']['num_countries']}\n",
    "\n",
    "FILES:\n",
    "1. grid_stress_preprocessing.pkl\n",
    "2. grid_stress_regression_model.pkl\n",
    "3. grid_stress_classification_model.pkl\n",
    "4. regression_results_all.pkl\n",
    "5. classification_results_all.pkl\n",
    "6. model_metadata.pkl\n",
    "7. model_metadata.json\n",
    "8. README.txt\n",
    "\"\"\"\n",
    "\n",
    "readme_path = os.path.join(output_folder, 'README.txt')\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"  ✓ README.txt\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6.6: VERIFY AND SUMMARIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAVED FILES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "saved_files = os.listdir(output_folder)\n",
    "for i, file in enumerate(sorted(saved_files), 1):\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"  {i}. {file:45s} ({file_size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\n  Total files: {len(saved_files)}\")\n",
    "print(f\"  Location: {output_folder}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎉 COMPLETE PIPELINE FINISHED! 🎉\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMODELS PERFORMANCE:\")\n",
    "print(f\"  Regression  - Test R²: {metadata['regression']['test_r2']:.4f}, RMSE: {metadata['regression']['test_rmse']:.2f}\")\n",
    "print(f\"  Classification - Test F1: {metadata['classification']['test_f1']:.4f}, Accuracy: {metadata['classification']['test_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nALL FILES SAVED TO: {output_folder}\")\n",
    "print(\"\\n✓ Ready for Streamlit deployment!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6690b4a1-0991-4d95-9f1e-5123ce9d9904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/path/to/file.csv\", \"file:/tmp/file.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "grid_stress_ml_pipeline.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
